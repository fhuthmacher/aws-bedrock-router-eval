{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Bedrock LLM Router Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro and Goal\n",
    "This Jupyter Notebook is designed to test an LLM (Large Language Model) routing system on a Text-to-SQL use case.\n",
    "\n",
    "The goal is to take a prompt, determine the level of complexity and then route the prompt either to a small or large LLM to generate the corresponding SQL query.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "1. Create a ground truth dataset comprised of questions and SQL queries for a given database (e.g. Northwind)\n",
    "2. Finetune LLM Classifier to predict level of complexity based on provided user prompt and database schema.\n",
    "3. Evaluate accuracy, cost, and latency of LLM classifier router approach compared to large LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a conda environment\n",
    "\n",
    "# !conda create -y --name bedrock-router-eval python=3.11.8\n",
    "# !conda init && activate bedrock-router-eval\n",
    "# !conda install -n bedrock-router-eval ipykernel --update-deps --force-reinstall -y\n",
    "# !conda install -c conda-forge ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/sagemaker-user/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.34.162)\n",
      "Requirement already satisfied: scipy in /home/sagemaker-user/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.14.0)\n",
      "Requirement already satisfied: numpy in /home/sagemaker-user/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: python-dotenv in /home/sagemaker-user/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (1.0.1)\n",
      "Collecting importlib_resources==6.4.0 (from -r requirements.txt (line 5))\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting datasets==2.20.0 (from -r requirements.txt (line 6))\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting filelock (from datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading pyarrow-17.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting requests>=2.32.2 (from datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading aiohttp-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /home/sagemaker-user/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages (from datasets==2.20.0->-r requirements.txt (line 6)) (24.1)\n",
      "Collecting pyyaml>=5.1 (from datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.162 in /home/sagemaker-user/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages (from boto3->-r requirements.txt (line 1)) (1.34.162)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/sagemaker-user/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages (from boto3->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/sagemaker-user/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages (from boto3->-r requirements.txt (line 1)) (0.10.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/sagemaker-user/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.162->boto3->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/sagemaker-user/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.162->boto3->-r requirements.txt (line 1)) (2.2.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading aiohappyeyeballs-2.3.6-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sagemaker-user/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets==2.20.0->-r requirements.txt (line 6)) (4.11.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==2.20.0->-r requirements.txt (line 6))\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/sagemaker-user/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.162->boto3->-r requirements.txt (line 1)) (1.16.0)\n",
      "Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Downloading aiohttp-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "Downloading pyarrow-17.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.3.6-py3-none-any.whl (12 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached multidict-6.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (328 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, pyyaml, pyarrow-hotfix, pyarrow, multidict, importlib_resources, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, aiohappyeyeballs, yarl, requests, pandas, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.3.6 aiohttp-3.10.3 aiosignal-1.3.1 attrs-24.2.0 certifi-2024.7.4 charset-normalizer-3.3.2 datasets-2.20.0 dill-0.3.8 filelock-3.15.4 frozenlist-1.4.1 fsspec-2024.5.0 huggingface-hub-0.24.5 idna-3.7 importlib_resources-6.4.0 multidict-6.0.5 multiprocess-0.70.16 pandas-2.2.2 pyarrow-17.0.0 pyarrow-hotfix-0.6 pytz-2024.1 pyyaml-6.0.2 requests-2.32.3 tqdm-4.66.5 tzdata-2024.1 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "# 2. Install dependencies\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Import necessary libraries and load environment variables\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import boto3\n",
    "import sqlite3\n",
    "from pandas.io import sql\n",
    "from botocore.config import Config\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'bedrock-router-eval.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "REGION = os.environ['REGION']\n",
    "HF_TOKEN = os.environ['HF_TOKEN']\n",
    "\n",
    "# Initialize Bedrock runtime\n",
    "config = Config(\n",
    "   retries = {\n",
    "      'max_attempts': 10,\n",
    "      'mode': 'standard'\n",
    "   }\n",
    ")\n",
    "bedrock_runtime_client = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        config=config,\n",
    "        region_name=REGION)\n",
    "\n",
    "bedrock_client = boto3.client(service_name='bedrock', region_name=REGION)\n",
    "athena_client = boto3.client('athena')\n",
    "glue_client = boto3.client('glue')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\" # \"anthropic.claude-3-5-sonnet-20240620-v1:0\" \"meta.llama3-1-70b-instruct-v1:0\"\n",
    "\n",
    "SQL_DATABASE = 'LOCAL' #GLUE\n",
    "SQL_DIALECT = 'SQlite' #awsathena\n",
    "\n",
    "if SQL_DATABASE == 'LOCAL':\n",
    "   %load_ext sql\n",
    "   %sql sqlite:///routedb.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Helper functions\n",
    "# TO DO: decide if we remove Bedrock Batch due to filesize restrictions\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import json\n",
    "from io import StringIO\n",
    "import sqlparse\n",
    "import sqlite3\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import typing as t\n",
    "import time\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "SCORE_PATTERN = r'<score>(.*?)</score>'\n",
    "REASONING_PATTERN = r'<thinking>(.*?)</thinking>'\n",
    "SQL_PATTERN = r'<SQL>(.*?)</SQL>'\n",
    "\n",
    "# Strip out the portion of the response with regex.\n",
    "def extract_with_regex(response, regex):\n",
    "    matches = re.search(regex, response, re.DOTALL)\n",
    "    # Extract the matched content, if any\n",
    "    return matches.group(1).strip() if matches else None\n",
    "\n",
    "def format_results(grade: str, chat_conversation: list[dict]) -> dict:\n",
    "    reasoning: str = extract_with_regex(grade, REASONING_PATTERN)\n",
    "    sqlquery: str =  extract_with_regex(grade, SQL_PATTERN)\n",
    "    score: str =  extract_with_regex(grade, SCORE_PATTERN)\n",
    "\n",
    "    return {\n",
    "        'chat_conversation': chat_conversation,\n",
    "        'reasoning': reasoning,\n",
    "        'score': score\n",
    "    }\n",
    "\n",
    "def balance_dataset(\n",
    "    dataset_df: pd.DataFrame, key: str, random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Balance the dataset by oversampling the minority class.\n",
    "    \"\"\"\n",
    "    # Determine the minority class\n",
    "    min_count = dataset_df[key].value_counts().min()\n",
    "\n",
    "    # Create a balanced DataFrame\n",
    "    sampled_dfs = []\n",
    "    for label in dataset_df[key].unique():\n",
    "        sampled = dataset_df[dataset_df[key] == label].sample(\n",
    "            n=min_count, random_state=random_state\n",
    "        )\n",
    "        sampled_dfs.append(sampled)\n",
    "\n",
    "    balanced_df = pd.concat(sampled_dfs).sample(frac=1, random_state=random_state)\n",
    "    return balanced_df\n",
    "    \n",
    "def visualize_distribution(df, key):\n",
    "    # Check if 'score' column exists in the DataFrame\n",
    "    if key not in df.columns:\n",
    "        raise ValueError(f\"The DataFrame does not contain a '{key}' column.\")\n",
    "    \n",
    "    # Count the frequency of each score\n",
    "    score_counts = df[key].value_counts().sort_index()\n",
    "    \n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(score_counts.index, score_counts.values)\n",
    "    \n",
    "    # Customize the chart\n",
    "    plt.title(f'Distribution of {key}')\n",
    "    plt.xlabel(f'{key}')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(range(int(score_counts.index.min()), int(score_counts.index.max()) + 1))\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for i, v in enumerate(score_counts.values):\n",
    "        plt.text(score_counts.index[i], v, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # Display the chart\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def execution_accuracy(generated_sql, labeled_sql):\n",
    "    \"\"\"\n",
    "    Calculate Execution Accuracy (EX)\n",
    "    \n",
    "    Args:\n",
    "    generated_sql (str): The SQL query generated by the model\n",
    "    labeled_sql (str): The labeled (ground truth) SQL query\n",
    "    \n",
    "    Returns:\n",
    "    float: 1.0 if the queries match, 0.0 otherwise\n",
    "    \"\"\"\n",
    "    # Normalize and compare the SQL queries\n",
    "    gen_normalized = sqlparse.format(generated_sql, strip_comments=True, reindent=True)\n",
    "    lab_normalized = sqlparse.format(labeled_sql, strip_comments=True, reindent=True)\n",
    "    \n",
    "    return 1.0 if gen_normalized == lab_normalized else 0.0\n",
    "\n",
    "def exact_set_match_accuracy(generated_sql, labeled_sql, db_connection):\n",
    "    \"\"\"\n",
    "    Calculate Exact Set Match Accuracy (EM)\n",
    "    \n",
    "    Args:\n",
    "    generated_sql (str): The SQL query generated by the model\n",
    "    labeled_sql (str): The labeled (ground truth) SQL query\n",
    "    db_connection: A database connection object\n",
    "    \n",
    "    Returns:\n",
    "    float: 1.0 if the result sets match, 0.0 otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Execute both queries\n",
    "        gen_result = pd.read_sql_query(generated_sql, db_connection)\n",
    "        lab_result = pd.read_sql_query(labeled_sql, db_connection)\n",
    "        \n",
    "        # Compare the result sets\n",
    "        return 1.0 if gen_result.equals(lab_result) else 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SQL: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def valid_efficiency_score(generated_sql, labeled_sql, db_connection):\n",
    "    \"\"\"\n",
    "    Calculate Valid Efficiency Score (VES)\n",
    "    \n",
    "    Args:\n",
    "    generated_sql (str): The SQL query generated by the model\n",
    "    labeled_sql (str): The labeled (ground truth) SQL query\n",
    "    db_connection: A database connection object\n",
    "    \n",
    "    Returns:\n",
    "    float: The VES score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Execute both queries and measure execution time\n",
    "        gen_start = time.time()\n",
    "        gen_result = pd.read_sql_query(generated_sql, db_connection)\n",
    "        gen_time = time.time() - gen_start\n",
    "        # print(f'generated_sql_execution_time: {gen_time}')\n",
    "        lab_start = time.time()\n",
    "        lab_result = pd.read_sql_query(labeled_sql, db_connection)\n",
    "        lab_time = time.time() - lab_start\n",
    "        # print(f'labeled_sql_execution_time: {lab_time}')\n",
    "        \n",
    "        # Check if results match\n",
    "        if not gen_result.equals(lab_result):\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate VES\n",
    "        ves = min(lab_time / gen_time, 1.0)\n",
    "        return ves\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SQL: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def dataframe_to_s3_jsonl(df, bucket_name, prefix, filename):\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame to JSONL format and upload it to S3.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to be converted and uploaded.\n",
    "    bucket_name (str): The name of the S3 bucket.\n",
    "    prefix (str): The S3 prefix (folder path) where the file will be uploaded.\n",
    "    filename (str): The name of the file to be created in S3.\n",
    "\n",
    "    Returns:\n",
    "    str: The S3 URI of the uploaded file.\n",
    "    \"\"\"\n",
    "    # Convert DataFrame to JSONL\n",
    "    jsonl_buffer = StringIO()\n",
    "    for _, row in df.iterrows():\n",
    "        json.dump(row.to_dict(), jsonl_buffer)\n",
    "        jsonl_buffer.write('\\n')\n",
    "    jsonl_buffer.seek(0)\n",
    "\n",
    "    # Upload the JSONL data to S3\n",
    "    s3_key = f\"{prefix.rstrip('/')}/{filename}\"\n",
    "    s3_client.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=s3_key,\n",
    "        Body=jsonl_buffer.getvalue(),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "\n",
    "    # Return the S3 URI of the uploaded file\n",
    "    return f\"s3://{bucket_name}/{s3_key}\"\n",
    "\n",
    "\n",
    "def download_and_parse_jsonl(bucket_name, object_key):\n",
    "    \"\"\"\n",
    "    Downloads a JSONL file from an Amazon S3 bucket and parses it into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str): The name of the S3 bucket where the JSONL file is stored.\n",
    "        object_key (str): The key (path) of the JSONL file in the S3 bucket.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the data from the JSONL file.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Download the JSONL file from S3\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    jsonl_data = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    # Parse the JSONL data into a list of dictionaries\n",
    "    data = [json.loads(line) for line in jsonl_data.strip().split('\\n')]\n",
    "\n",
    "    # Create a DataFrame from the list of dictionaries\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n",
    "def check_job_status_and_wait(job_arn):\n",
    "    # # check status\n",
    "    # bedrock.get_model_invocation_job(jobIdentifier=jobArn)['status']\n",
    "\n",
    "    # # list batch jobs\n",
    "    # bedrock.list_model_invocation_jobs(\n",
    "    #     maxResults=10,\n",
    "    #     statusEquals=\"Failed\",\n",
    "    #     sortOrder=\"Descending\"\n",
    "    # )\n",
    "\n",
    "    while True:\n",
    "        job_status = bedrock_client.get_model_invocation_job(jobIdentifier=job_arn)['status']\n",
    "        print(f\"Job status: {job_status}\")\n",
    "\n",
    "        if job_status == 'COMPLETED':\n",
    "            output_s3_uri = bedrock_client.get_model_invocation_job(jobIdentifier=job_arn)['outputDataConfig']['s3OutputDataConfig']['s3Uri']\n",
    "            output_file_key = output_s3_uri.replace(f\"s3://{output_bucket}/{output_prefix}\", \"\")\n",
    "            output_file_name = output_file_key.split(\"/\")[-1]\n",
    "            break\n",
    "        elif job_status == 'FAILED':\n",
    "            print(\"Job failed.\")\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(60)  # Wait for 1 minute before checking again\n",
    "    \n",
    "    return output_s3_uri\n",
    "\n",
    "def get_schema(database_name, table_names=None):\n",
    "    try:\n",
    "        \n",
    "        table_schema_list = []\n",
    "        response = glue_client.get_tables(DatabaseName=database_name)\n",
    "\n",
    "        all_table_names = [table['Name'] for table in response['TableList']]\n",
    "\n",
    "        if table_names:\n",
    "            table_names = [name for name in table_names if name in all_table_names]\n",
    "        else:\n",
    "            table_names = all_table_names\n",
    "\n",
    "        for table_name in table_names:\n",
    "            response = glue_client.get_table(DatabaseName=database_name, Name=table_name)\n",
    "            columns = response['Table']['StorageDescriptor']['Columns']\n",
    "            schema = {column['Name']: column['Type'] for column in columns}\n",
    "            table_schema_list.append({\"Table: {}\".format(table_name): 'Schema: {}'.format(schema)})\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "    return table_schema_list\n",
    "\n",
    "def execute_athena_query(database, query):\n",
    "    # Start query execution\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={\n",
    "            'Database': database\n",
    "        },\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': outputLocation\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get query execution ID\n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "    print(f\"Query Execution ID: {query_execution_id}\")\n",
    "\n",
    "    # Wait for the query to complete\n",
    "    response_wait = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "\n",
    "    while response_wait['QueryExecution']['Status']['State'] in ['QUEUED', 'RUNNING']:\n",
    "        print(\"Query is still running...\")\n",
    "        response_wait = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "\n",
    "    print(f'response_wait {response_wait}')\n",
    "\n",
    "    # Check if the query completed successfully\n",
    "    if response_wait['QueryExecution']['Status']['State'] == 'SUCCEEDED':\n",
    "        print(\"Query succeeded!\")\n",
    "\n",
    "        # Get query results\n",
    "        query_results = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "\n",
    "        # Extract and return the result data\n",
    "        code = 'SUCCEEDED'\n",
    "        return code, extract_result_data(query_results)\n",
    "\n",
    "    else:\n",
    "        print(\"Query failed!\")\n",
    "        code = response_wait['QueryExecution']['Status']['State']\n",
    "        message = response_wait['QueryExecution']['Status']['StateChangeReason']\n",
    "    \n",
    "        return code, message\n",
    "\n",
    "def extract_result_data(query_results):\n",
    "    #Return a cleaned response to the agent\n",
    "    result_data = []\n",
    "\n",
    "    # Extract column names\n",
    "    column_info = query_results['ResultSet']['ResultSetMetadata']['ColumnInfo']\n",
    "    column_names = [column['Name'] for column in column_info]\n",
    "\n",
    "    # Extract data rows\n",
    "    for row in query_results['ResultSet']['Rows']:\n",
    "        data = [item['VarCharValue'] for item in row['Data']]\n",
    "        result_data.append(dict(zip(column_names, data)))\n",
    "\n",
    "    return result_data\n",
    "\n",
    "# sql_dialect = awsathena or SQLite\n",
    "def build_sqlquerygen_prompt(user_question: str, sql_database_schema: str):\n",
    "    prompt = \"\"\"You will be provided with the original user question and a SQL database schema. \n",
    "                Only return the SQL query and nothing else.\n",
    "                Here is the original user question.\n",
    "                <user_question>\n",
    "                {user_question}\n",
    "                </user_question>\n",
    "\n",
    "                Here is the SQL database schema.\n",
    "                <sql_database_schema>\n",
    "                {sql_database_schema}\n",
    "                </sql_database_schema>\n",
    "                \n",
    "                Instructions:\n",
    "                Generate a SQL query that answers the original user question.\n",
    "                Use the schema, first create a syntactically correct {sql_dialect} query to answer the question. \n",
    "                Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
    "                Pay attention to use only the column names that you can see in the schema description. \n",
    "                Be careful to not query for columns that do not exist. \n",
    "                Pay attention to which column is in which table. \n",
    "                Also, qualify column names with the table name when needed.\n",
    "                If you cannot answer the user question with the help of the provided SQL database schema, \n",
    "                then output that this question question cannot be answered based of the information stored in the database.\n",
    "                You are required to use the following format, each taking one line:\n",
    "                Return the sql query inside the <SQL></SQL> tab.\n",
    "                \"\"\".format(\n",
    "                    user_question=user_question,\n",
    "                    sql_database_schema=sql_database_schema,\n",
    "                    sql_dialect=SQL_DIALECT\n",
    "                ) \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def build_prediction_prompt(user_question: str, sql_database_schema: str):\n",
    "    prompt = \"\"\"You will be provided with the original user question and a SQL database schema. \n",
    "                \n",
    "                Here is the original user question.\n",
    "                <user_question>\n",
    "                {user_question}\n",
    "                </user_question>\n",
    "\n",
    "                Here is the SQL database schema for the SQL dialect {sql_dialect}.\n",
    "                <sql_database_schema>\n",
    "                {sql_database_schema}\n",
    "                </sql_database_schema>\n",
    "                \n",
    "                Instructions:\n",
    "                Your prediction should infer the level of proficiency needed to create a SQL query effectively. \n",
    "                Use a scale from 1 to 5, where a higher score indicates a higher anticipated quality of response. \n",
    "                \n",
    "                Here is the rubric:\n",
    "                - High Rating (4-5): The AI assistant can produce a very efficient SQL query, showing deep understanding, detailed insight of the SQL database schema, and high relevance.\n",
    "                - Medium Rating (3): The AI assistant can provide an adequate SQL query with moderate detail, relevance, and factual accuracy.\n",
    "                - Low Rating (1-2): The AI assistant will struggle to produce a valid SQL query due to the question's difficulty, vagueness, and or the complexity of the SQL database schema and the assistant's limitations.\n",
    "                \n",
    "                Provide your prediction inside <score></score> tags.\n",
    "                \"\"\".format(\n",
    "                    user_question=user_question,\n",
    "                    sql_database_schema=sql_database_schema,\n",
    "                    sql_dialect=SQL_DIALECT\n",
    "                ) \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def build_grader_prompt(original_instruction: str, sql_query: str, sql_query_run_result: str, sql_query_run_error: str, groundtruth_sql_query: str,ex_score:str, em_score: str, ves_score: str):\n",
    "    prompt = \"\"\"You will be provided with the original user prompt, context, generated SQL query and SQL result, which is trying to answer the initial user prompt.\n",
    "                You also get the groundtruth SQL query, SQL evaluation metrics, and a rubric that instructs you on how to grade the generated SQL query.\n",
    "\n",
    "                Here are the original instructions for the SQL query.\n",
    "                <original_instruction>\n",
    "                {original_instruction}\n",
    "                </original_instruction>\n",
    "\n",
    "                Here is the generated SQL query based on these instructions.\n",
    "                <sql_query>\n",
    "                {sql_query}\n",
    "                </sql_query>\n",
    "\n",
    "                Here is the SQL result based on the generated SQL query.\n",
    "                <sql_query_run_result>\n",
    "                {sql_query_run_result}\n",
    "                </sql_query_run_result>\n",
    "\n",
    "                Any SQL errors that might have occured based on the generated SQL query.\n",
    "                <sql_query_run_error>\n",
    "                {sql_query_run_error}\n",
    "                </sql_query_run_error>\n",
    "\n",
    "                Here is the groundtruth SQL query for comparison with the generated SQL query.\n",
    "                <groundtruth_sql_query>\n",
    "                {groundtruth_sql_query}\n",
    "                </groundtruth_sql_query>\n",
    "                \n",
    "                And here are the corresponding Text-to-SQL metrics:\n",
    "                - Execution Accuracy, which compares the generated SQL query to the labeled SQL query to determine if its a match or not: {ex_score} ;\n",
    "                - Exact Set Match Accuracy (EM), which evaluates if the returned result set actually answer the question, regardless of how the query was written: {em_score} ;\n",
    "                - Valid Efficiency Score (VES), which compares the runtime of the SQL provided as groundtruth to the generated SQL query: {ves_score} ; \n",
    "\n",
    "                Here is the rubric on how to grade the generated SQL query:\n",
    "                - High Rating (4-5): Reserved for SQL queries that are very close to the quality of the groundtruth_sql_query (reference) or even better and are performant.\n",
    "                - Medium Rating (3): Reserved for SQL queries that have moderate quality compared to the groundtruth_sql_query (reference).\n",
    "                - Low Rating (1-2): Allocated to SQL queries that are much lower quality compared to the groundtruth_sql_query (reference) or completely wrong and produced an error.\n",
    "                \n",
    "                First, think through the SQL query rating based on the rubric inside <thinking></thinking> tags.\n",
    "                Use a scale from 1 to 5, where a higher score indicates a higher quality of the SQL query, provide the score inside <score></score> tags.\n",
    "                \n",
    "                \"\"\".format(\n",
    "                    sql_query= sql_query,\n",
    "                    sql_query_run_error= sql_query_run_error,\n",
    "                    original_instruction= original_instruction,\n",
    "                    sql_query_run_result= sql_query_run_result,\n",
    "                    groundtruth_sql_query= groundtruth_sql_query,\n",
    "                    ex_score=ex_score,\n",
    "                    em_score=em_score,\n",
    "                    ves_score=ves_score,\n",
    "                ) \n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "class Util():\n",
    "    def __init__(self,\n",
    "        debug: bool = False\n",
    "\n",
    "    ):\n",
    "\n",
    "        self.debug = debug\n",
    "\n",
    "    def calculate_cost(self, usage, model_id):\n",
    "        '''\n",
    "        Takes the usage tokens returned by Bedrock in input and output, and coverts to cost in dollars.\n",
    "        '''\n",
    "        \n",
    "        input_token_haiku = 0.25/1000000\n",
    "        output_token_haiku = 1.25/1000000\n",
    "        input_token_sonnet = 3.00/1000000\n",
    "        output_token_sonnet = 15.00/1000000\n",
    "        input_token_opus = 15.00/1000000\n",
    "        output_token_opus = 75.00/1000000\n",
    "        \n",
    "        input_token_titan_embeddingv1 = 0.1/1000000\n",
    "        input_token_titan_embeddingv2 = 0.02/1000000\n",
    "        input_token_titan_embeddingmultimodal = 0.8/1000000\n",
    "        input_token_titan_premier = 0.5/1000000\n",
    "        output_token_titan_premier = 1.5/1000000\n",
    "        input_token_titan_lite = 0.15/1000000\n",
    "        output_token_titan_lite = 0.2/1000000\n",
    "        input_token_titan_express = 0.2/1000000\n",
    "        output_token_titan_express = 0.6/1000000\n",
    "       \n",
    "        input_token_cohere_command = 0.15/1000000\n",
    "        output_token_cohere_command = 2/1000000\n",
    "        input_token_cohere_commandlight = 0.3/1000000\n",
    "        output_token_cohere_commandlight = 0.6/1000000\n",
    "        input_token_cohere_commandrplus = 3/1000000\n",
    "        output_token_cohere_commandrplus = 15/1000000\n",
    "        input_token_cohere_commandr = 5/1000000\n",
    "        output_token_cohere_commandr = 1.5/1000000\n",
    "        input_token_cohere_embedenglish = 0.1/1000000\n",
    "        input_token_cohere_embedmultilang = 0.1/1000000\n",
    "\n",
    "        input_token_llama3_8b = 0.4/1000000\n",
    "        output_token_llama3_8b = 0.6/1000000\n",
    "        input_token_llama3_70b = 2.6/1000000\n",
    "        output_token_llama3_70b = 3.5/1000000\n",
    "\n",
    "        input_token_mistral_8b = 0.15/1000000\n",
    "        output_token_mistral_8b = 0.2/1000000\n",
    "        input_token_mistral_large = 4/1000000\n",
    "        output_token_mistral_large = 12/1000000\n",
    "\n",
    "        cost = 0\n",
    "\n",
    "        if 'haiku' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_haiku\n",
    "            cost+= usage['outputTokens']*output_token_haiku\n",
    "        if 'sonnet' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_sonnet\n",
    "            cost+= usage['outputTokens']*output_token_sonnet\n",
    "        if 'opus' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_opus\n",
    "            cost+= usage['outputTokens']*output_token_opus\n",
    "        if 'amazon.titan-embed-text-v1' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_embeddingv1\n",
    "        if 'amazon.titan-embed-text-v2' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_embeddingv2\n",
    "        if 'cohere.embed-multilingual' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_embedmultilang\n",
    "        if 'cohere.embed-english' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_embedenglish \n",
    "        if 'meta.llama3-8b-instruct' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_llama3_8b\n",
    "            cost+= usage['outputTokens']*output_token_llama3_8b\n",
    "        if 'meta.llama3-70b-instruct' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_llama3_70b\n",
    "            cost+= usage['outputTokens']*output_token_llama3_70b\n",
    "        if 'cohere.command-text' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_command\n",
    "            cost+= usage['outputTokens']*output_token_cohere_command\n",
    "        if 'cohere.command-light-text' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_commandlight\n",
    "            cost+= usage['outputTokens']*output_token_cohere_commandlight\n",
    "        if 'cohere.command-r-plus' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_commandrplus\n",
    "            cost+= usage['outputTokens']*output_token_cohere_commandrplus\n",
    "        if 'cohere.command-r' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_cohere_commandr\n",
    "            cost+= usage['outputTokens']*output_token_cohere_commandr\n",
    "        if 'amazon.titan-text-express' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_express\n",
    "            cost+= usage['outputTokens']*output_token_titan_express\n",
    "        if 'amazon.titan-text-lite' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_lite\n",
    "            cost+= usage['outputTokens']*output_token_titan_lite\n",
    "        if 'amazon.titan-text-premier' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_titan_premier\n",
    "            cost+= usage['outputTokens']*output_token_titan_premier\n",
    "        if 'mistral.mixtral-8x7b-instruct-v0:1' in model_id:\n",
    "            cost+= usage['inputTokens']*input_token_mistral_8b\n",
    "            cost+= usage['outputTokens']*output_token_mistral_8b\n",
    "\n",
    "        return cost\n",
    "\n",
    "class BedrockLLMWrapper():\n",
    "    def __init__(self,\n",
    "        model_id: str = 'anthropic.claude-3-haiku-20240307-v1:0', #'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "        top_k: int = 5,\n",
    "        top_p: int = 0.7,\n",
    "        temperature: float = 0.0,\n",
    "        max_token_count: int = 4000,\n",
    "        max_attempts: int = 3,\n",
    "        debug: bool = False\n",
    "\n",
    "    ):\n",
    "\n",
    "        self.model_id = model_id\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.temperature = temperature\n",
    "        self.max_token_count = max_token_count\n",
    "        self.max_attempts = max_attempts\n",
    "        self.debug = debug\n",
    "        self.bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", config=config, region_name=REGION)\n",
    "        \n",
    "    def generate(self,prompt):\n",
    "        if self.debug: \n",
    "            print('entered BedrockLLMWrapper generate')\n",
    "        attempt = 1\n",
    "\n",
    "        message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": prompt}]\n",
    "        }\n",
    "        messages = []\n",
    "        messages.append(message)\n",
    "        \n",
    "        # model specific inference parameters to use.\n",
    "        if \"anthropic\" in self.model_id.lower():\n",
    "            # system_prompts = [{\"text\": \"You are a helpful AI Assistant.\"}]\n",
    "            system_prompts = []\n",
    "            # Base inference parameters to use.\n",
    "            inference_config = {\n",
    "                                \"temperature\": self.temperature, \n",
    "                                \"maxTokens\": self.max_token_count,\n",
    "                                \"stopSequences\": [\"\\n\\nHuman:\"],\n",
    "                                \"topP\": self.top_p,\n",
    "                            }\n",
    "            additional_model_fields = {\"top_k\": self.top_k}\n",
    "        else:\n",
    "            system_prompts = []\n",
    "            # Base inference parameters to use.\n",
    "            inference_config = {\n",
    "                                \"temperature\": self.temperature, \n",
    "                                \"maxTokens\": self.max_token_count,\n",
    "                            }\n",
    "            additional_model_fields = {\"top_k\": self.top_k}\n",
    "\n",
    "        if self.debug: \n",
    "            print(\"Sending:\\nSystem:\\n\",system,\"\\nMessages:\\n\",str(messages))\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "\n",
    "                # Send the message.\n",
    "                response = self.bedrock_runtime.converse(\n",
    "                    modelId=self.model_id,\n",
    "                    messages=messages,\n",
    "                    system=system_prompts,\n",
    "                    inferenceConfig=inference_config,\n",
    "                    additionalModelRequestFields=additional_model_fields\n",
    "                )\n",
    "\n",
    "                # Log token usage.\n",
    "                text = response['output'].get('message').get('content')[0].get('text')\n",
    "                usage = response['usage']\n",
    "                latency = response['metrics'].get('latencyMs')\n",
    "\n",
    "                if self.debug: \n",
    "                    print(f'text: {text} ; and token usage: {usage} ; and query_time: {latency}')    \n",
    "                \n",
    "                break\n",
    "               \n",
    "            except Exception as e:\n",
    "                print(\"Error with calling Bedrock: \"+str(e))\n",
    "                attempt+=1\n",
    "                if attempt>self.max_attempts:\n",
    "                    print(\"Max attempts reached!\")\n",
    "                    result_text = str(e)\n",
    "                    break\n",
    "                else:#retry in 10 seconds\n",
    "                    print(\"retry\")\n",
    "                    time.sleep(10)\n",
    "\n",
    "        # return result_text\n",
    "        return [text,usage,latency]\n",
    "\n",
    "    # Threaded function for queue processing.\n",
    "    def thread_request(self, q, result):\n",
    "        while not q.empty():\n",
    "            work = q.get()    #fetch new work from the Queue\n",
    "            try:\n",
    "                data = self.generate(work[1])\n",
    "                result[work[0]] = data  #Store data back at correct index\n",
    "            except Exception as e:\n",
    "                print('Error with prompt!',str(e))\n",
    "                result[work[0]] = (str(e))\n",
    "            #signal to the queue that task has been processed\n",
    "            q.task_done()\n",
    "        return True\n",
    "\n",
    "    def generate_threaded(self,prompts):\n",
    "        '''\n",
    "        Call multi-threaded.\n",
    "        Returns a dict of the prompts and responses.\n",
    "        '''\n",
    "        system=\"\"\n",
    "        ignore_cache=False\n",
    "        q = Queue(maxsize=0)\n",
    "        num_theads = min(50, len(prompts))\n",
    "        #Populating Queue with tasks\n",
    "        results = [{} for x in prompts];\n",
    "        #load up the queue with the promts to fetch and the index for each job (as a tuple):\n",
    "        for i in range(len(prompts)):\n",
    "            #need the index and the url in each queue item.\n",
    "            q.put((i,prompts[i]))\n",
    "            \n",
    "        #Starting worker threads on queue processing\n",
    "        for i in range(num_theads):\n",
    "            if self.debug:\n",
    "                print('Starting thread ', i)\n",
    "            worker = Thread(target=self.thread_request, args=(q,results))\n",
    "            worker.daemon = True\n",
    "            worker.start()\n",
    "\n",
    "        #now we wait until the queue has been processed\n",
    "        q.join()\n",
    "        return results\n",
    "    \n",
    "\n",
    "def runBedrockBatchJob(modelid,df):\n",
    "    BEDROCK_BATCH_API = False\n",
    "    if BEDROCK_BATCH_API == True:\n",
    "        ## WIP\n",
    "        # upload df_results dataframe as jsonl to S3\n",
    "        prefix = 'routeeval/input'\n",
    "        filename = 'small_llm.jsonl'\n",
    "        bucket_name = 'felixh-demo'\n",
    "\n",
    "        s3_uri = dataframe_to_s3_jsonl(df, bucket_name, prefix, filename)\n",
    "        print(f\"File uploaded to: {s3_uri}\")\n",
    "\n",
    "        # generate SQL queries with Bedrock Batch API if filesize is >25mb otherwise use helper class for threaded calls\n",
    "\n",
    "        input_bucket = bucket_name\n",
    "        input_prefix = \"input/\"\n",
    "        output_bucket = bucket_name\n",
    "        output_prefix = \"output/mistral8binstruct\"\n",
    "        BATCH_ROLE_ARN = 'arn:aws:iam::026459568683:role/admin'\n",
    "\n",
    "        inputDataConfig=({\n",
    "            \"s3InputDataConfig\": {\n",
    "                \"s3Uri\": f\"s3://{input_bucket}/{input_prefix}\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "        outputDataConfig=({\n",
    "            \"s3OutputDataConfig\": {\n",
    "                \"s3Uri\": f\"s3://{output_bucket}/{output_prefix}\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "        batch_response=bedrock_client.create_model_invocation_job(\n",
    "            roleArn=BATCH_ROLE_ARN,\n",
    "            modelId=modelid,\n",
    "            jobName=\"small-llm-genSQL-job\",\n",
    "            inputDataConfig=inputDataConfig,\n",
    "            outputDataConfig=outputDataConfig\n",
    "        )\n",
    "\n",
    "        jobArn = batch_response.get('jobArn')\n",
    "\n",
    "\n",
    "        # wait for batch job to complete\n",
    "        output_s3_uri = check_job_status_and_wait(job_arn)\n",
    "        print(output_s3_uri)\n",
    "        output_file_name = output_s3_uri.split(\"/\")[-1]\n",
    "        print(output_file_name)\n",
    "\n",
    "        # download results from S3 into dataframe\n",
    "        df = download_and_parse_jsonl(bucket_name, object_key)\n",
    "\n",
    "    else:\n",
    "        # use helper class for threaded API calls\n",
    "        wrapper = BedrockLLMWrapper(debug=False, model_id=modelid)\n",
    "\n",
    "        prompts_list = []\n",
    "        for row in df.itertuples():\n",
    "            prompt = build_sqlquerygen_prompt(row.Question, row.Context)\n",
    "            prompts_list.append(prompt)\n",
    "        # [result_text,usage,query_time]\n",
    "        results = wrapper.generate_threaded(prompts_list)\n",
    "\n",
    "        # Create a list to store the generated SQL queries\n",
    "        generated_sql_queries = []\n",
    "\n",
    "        for result in results:\n",
    "            generated_sql_query = result[0]\n",
    "            generated_sql_queries.append(generated_sql_query)\n",
    "\n",
    "        # Add the new column 'Generated_SQL_Query' to df_results\n",
    "        df['Generated_SQL_Query'] = generated_sql_queries\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class RouteLLMWrapper():\n",
    "    def __init__(self,\n",
    "        small_llm_model_id: str = 'mistral.mixtral-8x7b-instruct-v0:1',\n",
    "        large_llm_model_id: str = 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    ):\n",
    "        self.small_llm_model_id = small_llm_model_id\n",
    "        self.large_llm_model_id = large_llm_model_id\n",
    "        self.small_llm = BedrockLLMWrapper(debug=False, model_id=self.small_llm_model_id, max_token_count=512)\n",
    "        self.large_llm = BedrockLLMWrapper(debug=False, model_id=self.large_llm_model_id, max_token_count=512)\n",
    "        \n",
    "        \n",
    "    def generate(self,user_question, sql_database_schema):\n",
    "        # 1. Classify prompt\n",
    "        classification_prompt = build_prediction_prompt(user_question, sql_database_schema)\n",
    "        classification_result = self.small_llm.generate(classification_prompt) #using small LLM as-is\n",
    "        classification = int(extract_with_regex(classification_result[0], SCORE_PATTERN))\n",
    "        classification_token = classification_result[1]\n",
    "        classification_query_time = classification_result[2]\n",
    "\n",
    "        # 2. Route to appropriate LLM and generate response\n",
    "        sql_prompt = build_sqlquerygen_prompt(user_question, sql_database_schema)\n",
    "        # print(f'classification: {classification}')\n",
    "        \n",
    "        if classification > 4:\n",
    "            # print('use small LLM')\n",
    "            result = self.small_llm.generate(sql_prompt) # invoke large LLM\n",
    "        else:\n",
    "            # print('use large LLM')\n",
    "            result = self.large_llm.generate(sql_prompt) # invoke large LLM\n",
    "\n",
    "        # 3. Return final response along with classification result         \n",
    "        return classification_result, result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: --\n",
      "-- PostgreSQL database dump\n",
      "--\n",
      "\n",
      "SET statement_timeout = 0\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET lock_timeout = 0\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET client_encoding = 'UTF8'\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET standard_conforming_strings = on\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET check_function_bodies = false\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET client_min_messages = warning\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET default_tablespace = ''\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET default_with_oids = false\n",
      "SQL execution completed.\n",
      "CREATE TABLE categories (\n",
      "    category_id smallint NOT NULL PRIMARY KEY,\n",
      "    category_name character varying(15) NOT NULL,\n",
      "    description text,\n",
      "    picture bytea\n",
      ");\n",
      "\n",
      "CREATE TABLE customer_demographics (\n",
      "    customer_type_id bpchar NOT NULL PRIMARY KEY,\n",
      "    customer_desc text\n",
      ");\n",
      "\n",
      "CREATE TABLE customers (\n",
      "    customer_id bpchar NOT NULL PRIMARY KEY,\n",
      "    company_name character varying(40) NOT NULL,\n",
      "    contact_name character varying(30),\n",
      "    contact_title character varying(30),\n",
      "    address character varying(60),\n",
      "    city character varying(15),\n",
      "    region character varying(15),\n",
      "    postal_code character varying(10),\n",
      "    country character varying(15),\n",
      "    phone character varying(24),\n",
      "    fax character varying(24)\n",
      ");\n",
      "\n",
      "CREATE TABLE customer_customer_demo (\n",
      "    customer_id bpchar NOT NULL,\n",
      "    customer_type_id bpchar NOT NULL,\n",
      "    PRIMARY KEY (customer_id, customer_type_id),\n",
      "    FOREIGN KEY (customer_type_id) REFERENCES customer_demographics,\n",
      "    FOREIGN KEY (customer_id) REFERENCES customers\n",
      ");\n",
      "\n",
      "CREATE TABLE employees (\n",
      "    employee_id smallint NOT NULL PRIMARY KEY,\n",
      "    last_name character varying(20) NOT NULL,\n",
      "    first_name character varying(10) NOT NULL,\n",
      "    title character varying(30),\n",
      "    title_of_courtesy character varying(25),\n",
      "    birth_date date,\n",
      "    hire_date date,\n",
      "    address character varying(60),\n",
      "    city character varying(15),\n",
      "    region character varying(15),\n",
      "    postal_code character varying(10),\n",
      "    country character varying(15),\n",
      "    home_phone character varying(24),\n",
      "    extension character varying(4),\n",
      "    photo bytea,\n",
      "    notes text,\n",
      "    reports_to smallint,\n",
      "    photo_path character varying(255),\n",
      "\tFOREIGN KEY (reports_to) REFERENCES employees\n",
      ");\n",
      "\n",
      "CREATE TABLE suppliers (\n",
      "    supplier_id smallint NOT NULL PRIMARY KEY,\n",
      "    company_name character varying(40) NOT NULL,\n",
      "    contact_name character varying(30),\n",
      "    contact_title character varying(30),\n",
      "    address character varying(60),\n",
      "    city character varying(15),\n",
      "    region character varying(15),\n",
      "    postal_code character varying(10),\n",
      "    country character varying(15),\n",
      "    phone character varying(24),\n",
      "    fax character varying(24),\n",
      "    homepage text\n",
      ");\n",
      "\n",
      "CREATE TABLE products (\n",
      "    product_id smallint NOT NULL PRIMARY KEY,\n",
      "    product_name character varying(40) NOT NULL,\n",
      "    supplier_id smallint,\n",
      "    category_id smallint,\n",
      "    quantity_per_unit character varying(20),\n",
      "    unit_price real,\n",
      "    units_in_stock smallint,\n",
      "    units_on_order smallint,\n",
      "    reorder_level smallint,\n",
      "    discontinued integer NOT NULL,\n",
      "\tFOREIGN KEY (category_id) REFERENCES categories,\n",
      "\tFOREIGN KEY (supplier_id) REFERENCES suppliers\n",
      ");\n",
      "\n",
      "CREATE TABLE region (\n",
      "    region_id smallint NOT NULL PRIMARY KEY,\n",
      "    region_description bpchar NOT NULL\n",
      ");\n",
      "\n",
      "CREATE TABLE shippers (\n",
      "    shipper_id smallint NOT NULL PRIMARY KEY,\n",
      "    company_name character varying(40) NOT NULL,\n",
      "    phone character varying(24)\n",
      ");\n",
      "\n",
      "CREATE TABLE orders (\n",
      "    order_id smallint NOT NULL PRIMARY KEY,\n",
      "    customer_id bpchar,\n",
      "    employee_id smallint,\n",
      "    order_date date,\n",
      "    required_date date,\n",
      "    shipped_date date,\n",
      "    ship_via smallint,\n",
      "    freight real,\n",
      "    ship_name character varying(40),\n",
      "    ship_address character varying(60),\n",
      "    ship_city character varying(15),\n",
      "    ship_region character varying(15),\n",
      "    ship_postal_code character varying(10),\n",
      "    ship_country character varying(15),\n",
      "    FOREIGN KEY (customer_id) REFERENCES customers,\n",
      "    FOREIGN KEY (employee_id) REFERENCES employees,\n",
      "    FOREIGN KEY (ship_via) REFERENCES shippers\n",
      ");\n",
      "\n",
      "CREATE TABLE territories (\n",
      "    territory_id character varying(20) NOT NULL PRIMARY KEY,\n",
      "    territory_description bpchar NOT NULL,\n",
      "    region_id smallint NOT NULL,\n",
      "\tFOREIGN KEY (region_id) REFERENCES region\n",
      ");\n",
      "\n",
      "CREATE TABLE employee_territories (\n",
      "    employee_id smallint NOT NULL,\n",
      "    territory_id character varying(20) NOT NULL,\n",
      "    PRIMARY KEY (employee_id, territory_id),\n",
      "    FOREIGN KEY (territory_id) REFERENCES territories,\n",
      "    FOREIGN KEY (employee_id) REFERENCES employees\n",
      ");\n",
      "\n",
      "CREATE TABLE order_details (\n",
      "    order_id smallint NOT NULL,\n",
      "    product_id smallint NOT NULL,\n",
      "    unit_price real NOT NULL,\n",
      "    quantity smallint NOT NULL,\n",
      "    discount real NOT NULL,\n",
      "    PRIMARY KEY (order_id, product_id),\n",
      "    FOREIGN KEY (product_id) REFERENCES products,\n",
      "    FOREIGN KEY (order_id) REFERENCES orders\n",
      ");\n",
      "\n",
      "CREATE TABLE us_states (\n",
      "    state_id smallint NOT NULL PRIMARY KEY,\n",
      "    state_name character varying(100),\n",
      "    state_abbr character varying(2),\n",
      "    state_region character varying(50)\n",
      ");\n",
      "\n",
      "\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: --\n",
      "-- PostgreSQL database dump\n",
      "--\n",
      "\n",
      "SET statement_timeout = 0\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET lock_timeout = 0\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET client_encoding = 'UTF8'\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET standard_conforming_strings = on\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET check_function_bodies = false\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET client_min_messages = warning\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET default_tablespace = ''\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET default_with_oids = false\n",
      "Error executing statement: near \"SET\": syntax error\n",
      "Statement: \n",
      "SET SESSION CHARACTERISTICS AS TRANSACTION ISOLATION LEVEL SERIALIZABLE\n",
      "SQL execution completed.\n",
      "CREATE TABLE categories (\n",
      "    category_id smallint NOT NULL PRIMARY KEY,\n",
      "    category_name character varying(15) NOT NULL,\n",
      "    description text,\n",
      "    picture bytea\n",
      ");\n",
      "\n",
      "CREATE TABLE customer_demographics (\n",
      "    customer_type_id bpchar NOT NULL PRIMARY KEY,\n",
      "    customer_desc text\n",
      ");\n",
      "\n",
      "CREATE TABLE customers (\n",
      "    customer_id bpchar NOT NULL PRIMARY KEY,\n",
      "    company_name character varying(40) NOT NULL,\n",
      "    contact_name character varying(30),\n",
      "    contact_title character varying(30),\n",
      "    address character varying(60),\n",
      "    city character varying(15),\n",
      "    region character varying(15),\n",
      "    postal_code character varying(10),\n",
      "    country character varying(15),\n",
      "    phone character varying(24),\n",
      "    fax character varying(24)\n",
      ");\n",
      "\n",
      "CREATE TABLE customer_customer_demo (\n",
      "    customer_id bpchar NOT NULL,\n",
      "    customer_type_id bpchar NOT NULL,\n",
      "    PRIMARY KEY (customer_id, customer_type_id),\n",
      "    FOREIGN KEY (customer_type_id) REFERENCES customer_demographics,\n",
      "    FOREIGN KEY (customer_id) REFERENCES customers\n",
      ");\n",
      "\n",
      "CREATE TABLE employees (\n",
      "    employee_id smallint NOT NULL PRIMARY KEY,\n",
      "    last_name character varying(20) NOT NULL,\n",
      "    first_name character varying(10) NOT NULL,\n",
      "    title character varying(30),\n",
      "    title_of_courtesy character varying(25),\n",
      "    birth_date date,\n",
      "    hire_date date,\n",
      "    address character varying(60),\n",
      "    city character varying(15),\n",
      "    region character varying(15),\n",
      "    postal_code character varying(10),\n",
      "    country character varying(15),\n",
      "    home_phone character varying(24),\n",
      "    extension character varying(4),\n",
      "    photo bytea,\n",
      "    notes text,\n",
      "    reports_to smallint,\n",
      "    photo_path character varying(255),\n",
      "\tFOREIGN KEY (reports_to) REFERENCES employees\n",
      ");\n",
      "\n",
      "CREATE TABLE suppliers (\n",
      "    supplier_id smallint NOT NULL PRIMARY KEY,\n",
      "    company_name character varying(40) NOT NULL,\n",
      "    contact_name character varying(30),\n",
      "    contact_title character varying(30),\n",
      "    address character varying(60),\n",
      "    city character varying(15),\n",
      "    region character varying(15),\n",
      "    postal_code character varying(10),\n",
      "    country character varying(15),\n",
      "    phone character varying(24),\n",
      "    fax character varying(24),\n",
      "    homepage text\n",
      ");\n",
      "\n",
      "CREATE TABLE products (\n",
      "    product_id smallint NOT NULL PRIMARY KEY,\n",
      "    product_name character varying(40) NOT NULL,\n",
      "    supplier_id smallint,\n",
      "    category_id smallint,\n",
      "    quantity_per_unit character varying(20),\n",
      "    unit_price real,\n",
      "    units_in_stock smallint,\n",
      "    units_on_order smallint,\n",
      "    reorder_level smallint,\n",
      "    discontinued integer NOT NULL,\n",
      "\tFOREIGN KEY (category_id) REFERENCES categories,\n",
      "\tFOREIGN KEY (supplier_id) REFERENCES suppliers\n",
      ");\n",
      "\n",
      "CREATE TABLE region (\n",
      "    region_id smallint NOT NULL PRIMARY KEY,\n",
      "    region_description bpchar NOT NULL\n",
      ");\n",
      "\n",
      "CREATE TABLE shippers (\n",
      "    shipper_id smallint NOT NULL PRIMARY KEY,\n",
      "    company_name character varying(40) NOT NULL,\n",
      "    phone character varying(24)\n",
      ");\n",
      "\n",
      "CREATE TABLE orders (\n",
      "    order_id smallint NOT NULL PRIMARY KEY,\n",
      "    customer_id bpchar,\n",
      "    employee_id smallint,\n",
      "    order_date date,\n",
      "    required_date date,\n",
      "    shipped_date date,\n",
      "    ship_via smallint,\n",
      "    freight real,\n",
      "    ship_name character varying(40),\n",
      "    ship_address character varying(60),\n",
      "    ship_city character varying(15),\n",
      "    ship_region character varying(15),\n",
      "    ship_postal_code character varying(10),\n",
      "    ship_country character varying(15),\n",
      "    FOREIGN KEY (customer_id) REFERENCES customers,\n",
      "    FOREIGN KEY (employee_id) REFERENCES employees,\n",
      "    FOREIGN KEY (ship_via) REFERENCES shippers\n",
      ");\n",
      "\n",
      "CREATE TABLE territories (\n",
      "    territory_id character varying(20) NOT NULL PRIMARY KEY,\n",
      "    territory_description bpchar NOT NULL,\n",
      "    region_id smallint NOT NULL,\n",
      "\tFOREIGN KEY (region_id) REFERENCES region\n",
      ");\n",
      "\n",
      "CREATE TABLE employee_territories (\n",
      "    employee_id smallint NOT NULL,\n",
      "    territory_id character varying(20) NOT NULL,\n",
      "    PRIMARY KEY (employee_id, territory_id),\n",
      "    FOREIGN KEY (territory_id) REFERENCES territories,\n",
      "    FOREIGN KEY (employee_id) REFERENCES employees\n",
      ");\n",
      "\n",
      "CREATE TABLE order_details (\n",
      "    order_id smallint NOT NULL,\n",
      "    product_id smallint NOT NULL,\n",
      "    unit_price real NOT NULL,\n",
      "    quantity smallint NOT NULL,\n",
      "    discount real NOT NULL,\n",
      "    PRIMARY KEY (order_id, product_id),\n",
      "    FOREIGN KEY (product_id) REFERENCES products,\n",
      "    FOREIGN KEY (order_id) REFERENCES orders\n",
      ");\n",
      "\n",
      "CREATE TABLE us_states (\n",
      "    state_id smallint NOT NULL PRIMARY KEY,\n",
      "    state_name character varying(100),\n",
      "    state_abbr character varying(2),\n",
      "    state_region character varying(50)\n",
      ");\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Get schema for all tables in database\n",
    "\n",
    "if SQL_DATABASE == 'LOCAL':\n",
    "    # create local db and import northwind database\n",
    "\n",
    "    import requests\n",
    "    import sqlite3\n",
    "    import re\n",
    "\n",
    "    # Download the SQL files\n",
    "    url1 = \"https://raw.githubusercontent.com/YugaByte/yugabyte-db/master/sample/northwind_ddl.sql\"\n",
    "    url2 = \"https://raw.githubusercontent.com/YugaByte/yugabyte-db/master/sample/northwind_data.sql\"\n",
    "\n",
    "    urls = [url1,url2]\n",
    "\n",
    "    for url in urls:\n",
    "        response = requests.get(url)\n",
    "        sql_content = response.text\n",
    "\n",
    "        # Create a SQLite database connection\n",
    "        conn = sqlite3.connect('routedb.db')\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Split the SQL content into individual statements\n",
    "        sql_statements = re.split(r';\\s*$', sql_content, flags=re.MULTILINE)\n",
    "\n",
    "        # Execute each SQL statement\n",
    "        for statement in sql_statements:\n",
    "            # Skip empty statements\n",
    "            if statement.strip():\n",
    "                # print(f'statement: {statement}')\n",
    "                # Replace PostgreSQL-specific syntax with SQLite equivalents\n",
    "                statement = statement.replace('SERIAL PRIMARY KEY', 'INTEGER PRIMARY KEY AUTOINCREMENT')\n",
    "                statement = statement.replace('::int', '')\n",
    "                statement = statement.replace('::varchar', '')\n",
    "                statement = statement.replace('::real', '')\n",
    "                statement = statement.replace('::date', '')\n",
    "                statement = statement.replace('::boolean', '')\n",
    "                statement = statement.replace('public.', '')\n",
    "                statement = re.sub(r'WITH \\(.*?\\)', '', statement)\n",
    "                \n",
    "                try:\n",
    "                    cursor.execute(statement)\n",
    "                except sqlite3.Error as e:\n",
    "                    print(f\"Error executing statement: {e}\")\n",
    "                    print(f\"Statement: {statement}\")\n",
    "\n",
    "        # Commit the changes and close the connection\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "        print(\"SQL execution completed.\")\n",
    "\n",
    "\n",
    "        def get_schema_as_string(db_path):\n",
    "            conn = sqlite3.connect(db_path)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Query to get all table names\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            tables = cursor.fetchall()\n",
    "\n",
    "            schema_string = \"\"\n",
    "\n",
    "            for table in tables:\n",
    "                table_name = table[0]\n",
    "                # Query to get the CREATE TABLE statement for each table\n",
    "                cursor.execute(f\"SELECT sql FROM sqlite_master WHERE type='table' AND name='{table_name}';\")\n",
    "                create_table_stmt = cursor.fetchone()[0]\n",
    "                \n",
    "                schema_string += f\"{create_table_stmt};\\n\\n\"\n",
    "\n",
    "            conn.close()\n",
    "            return schema_string\n",
    "        \n",
    "        schema = get_schema_as_string('routedb.db')\n",
    "        print(schema)\n",
    "\n",
    "else: \n",
    "    # use a Glue database\n",
    "    DATABASE = ''\n",
    "    schema = get_schema(DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing line: {\"question\":\"Which employees have processed orders with a total value higher than the company average?\",\"query\":\"WITH avg_order_value AS (SELECT AVG(order_value) as company_avg FROM (SELECT o.order_id, SUM(od.unit_price * od.quantity * (1 - od.discount)) as order_value FROM orders o JOIN order_details o\n",
      "Number of successfully parsed questions: 142\n",
      "                                            question  \\\n",
      "0             What is the total number of customers?   \n",
      "1      List all product names and their unit prices.   \n",
      "2        Who are the top 5 customers by order count?   \n",
      "3       What is the average freight cost for orders?   \n",
      "4  List all employees with their full names and t...   \n",
      "\n",
      "                                               query  \n",
      "0                    SELECT COUNT(*) FROM customers;  \n",
      "1     SELECT product_name, unit_price FROM products;  \n",
      "2  SELECT c.customer_id, c.company_name, COUNT(o....  \n",
      "3                   SELECT AVG(freight) FROM orders;  \n",
      "4  SELECT employee_id, first_name || ' ' || last_...  \n"
     ]
    }
   ],
   "source": [
    "# 6. Generate questions and SQL queries based on database schema with Sonnet 3.5\n",
    "## re-run cell to increase # of question query pairs which will function as groundtruth\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def generate_question_query_dataset(existing_questions):\n",
    "    prompt = \"\"\"Human: Review the provided database schema below. \n",
    "            Then create 100 questions in natural language along with corresponding SQL queries that would answer these questions based on this database schema.\n",
    "            \n",
    "            <database_schema>\n",
    "            {database_schema}\n",
    "            </database_schema>\n",
    "\n",
    "            Ensure that the generated question is not already part of the existing data below.\n",
    "\n",
    "            <existing_questions>\n",
    "            {existing_questions}\n",
    "            </existing_questions>\n",
    "            \n",
    "            Return the response in JSONL and return only the JSON and nothing else.      \n",
    "            Assistant: {{\"\"\".format(database_schema=schema, existing_questions=existing_questions)\n",
    "\n",
    "    MODEL_ID = 'anthropic.claude-3-5-sonnet-20240620-v1:0'\n",
    "    wrapper = BedrockLLMWrapper(debug=False, model_id=MODEL_ID, max_token_count=3000)\n",
    "    result = wrapper.generate(prompt)\n",
    "    \n",
    "    return result[0]\n",
    "     \n",
    "if os.path.exists('question_query.jsonl'):\n",
    "    with open('question_query.jsonl', 'r') as file:\n",
    "        existing_questions = file.read()\n",
    "else:\n",
    "    existing_questions = ''\n",
    "\n",
    "response_text = generate_question_query_dataset(existing_questions)\n",
    "\n",
    "# append response_text to existing_questions\n",
    "response_text = existing_questions + response_text\n",
    "\n",
    "# read jsonl_string into dataframe\n",
    "\n",
    "def parse_json_line(line):\n",
    "    try:\n",
    "        return json.loads(line)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error parsing line: {line}\")\n",
    "        return None\n",
    "\n",
    "if 'response_text' not in globals():\n",
    "    response_text = []\n",
    "    with open('question_query.jsonl', 'r') as file:\n",
    "        response_text = file.read()\n",
    "\n",
    "# Split the string into lines and parse each line\n",
    "data = [parse_json_line(line) for line in response_text.strip().split('\\n')]\n",
    "\n",
    "# Remove any None values (failed parses)\n",
    "data = [d for d in data if d is not None]\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_json('question_query.jsonl', orient='records', lines=True)\n",
    "\n",
    "print(f\"Number of successfully parsed questions: {len(df)}\")\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df already exists in memory.\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: no such column: INTERVAL\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Error executing statement: near \"FROM\": syntax error\n",
      "Number of successful queries: 123\n",
      "Number of unsuccessful queries: 19\n"
     ]
    }
   ],
   "source": [
    "# 7. Test generated SQL queries and remove those question query pairs that do not run successfully.\n",
    "results = []\n",
    "\n",
    "# Check if df exists in the current namespace\n",
    "if 'df' not in globals():\n",
    "    # If it doesn't exist, try to load it from a JSONL file\n",
    "    file_path = 'question_query.jsonl'\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # Load the dataframe from the JSONL file\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        print(\"df loaded from JSONL file.\")\n",
    "    else:\n",
    "        print(f\"Error: JSONL file not found at {file_path}\")\n",
    "else:\n",
    "    print(\"df already exists in memory.\")\n",
    "\n",
    "df.columns = df.columns.str.capitalize()\n",
    "if SQL_DATABASE == 'LOCAL':\n",
    "    # Create a SQLite database connection\n",
    "    conn = sqlite3.connect('routedb.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "for row in df.itertuples():\n",
    "    # print(row.query)\n",
    "    error = None\n",
    "    result = None\n",
    "    try:\n",
    "        \n",
    "        if SQL_DATABASE == 'LOCAL':\n",
    "            # Use local SQL lite\n",
    "            statement = row.Query\n",
    "            # Replace PostgreSQL-specific syntax with SQLite equivalents\n",
    "            \n",
    "            try:\n",
    "                cursor.execute(statement)\n",
    "                # Fetch all rows from the result\n",
    "                result = cursor.fetchall()\n",
    "\n",
    "            except sqlite3.Error as e:\n",
    "                print(f\"Error executing statement: {e}\")\n",
    "                error = e\n",
    "                # print(f\"Statement: {statement}\")\n",
    "\n",
    "        else:\n",
    "            # Use Athena if AWS Glue Schema is used\n",
    "            result = execute_athena_query(DATABASE, row.Query)\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error = e\n",
    "\n",
    "    results.append({'Question': row.Question,'Query': row.Query, 'Result': result, 'Error': error, 'Context': schema})\n",
    "\n",
    "if SQL_DATABASE == 'LOCAL':\n",
    "    # close the connection\n",
    "    conn.close()\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Use all generated prompts that resulted in valid SQL queries and filter out the rest\n",
    "\n",
    "df_good_results = df_results[df_results['Error'].isnull() | (df_results['Error'] == None)]\n",
    "print(f\"Number of successful queries: {len(df_good_results)}\")\n",
    "\n",
    "df_bad_results = df_results[df_results['Error'].notnull() | (df_results['Error'] == 'None')]\n",
    "print(f\"Number of unsuccessful queries: {len(df_bad_results)}\")\n",
    "\n",
    "# safe good queries as jsonl as our golden dataset\n",
    "df_good_results.to_json('question_query_good_results.jsonl', orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated_sql_query:  <SQL>\n",
      "SELECT COUNT(*)\n",
      "FROM customers;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT products.product_name, products.unit_price\n",
      "FROM products;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT customers.customer_id, COUNT(orders.order_id) as order_count\n",
      "FROM customers\n",
      "JOIN orders ON customers.customer_id = orders.customer_id\n",
      "GROUP BY customers.customer_id\n",
      "ORDER BY order_count DESC\n",
      "LIMIT 5;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT AVG(freight) \n",
      "FROM orders\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT first_name || ' ' || last_name AS full_name, title\n",
      "FROM employees;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT categories.category_name, COUNT(products.product_id) AS product_count\n",
      "FROM categories\n",
      "LEFT JOIN products ON categories.category_id = products.category_id\n",
      "GROUP BY categories.category_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT SUM(p.units_in_stock * p.unit_price) as total_value\n",
      "FROM products p;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT company_name\n",
      "FROM suppliers\n",
      "WHERE country = 'USA';\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT product_name, unit_price\n",
      "FROM products\n",
      "ORDER BY unit_price DESC\n",
      "LIMIT 3;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT shipped_country, COUNT(*) as num_orders\n",
      "FROM orders\n",
      "GROUP BY shipped_country;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT first_name, last_name\n",
      "FROM employees\n",
      "WHERE reports_to = (\n",
      "    SELECT employee_id\n",
      "    FROM employees\n",
      "    WHERE last_name = 'Fuller' AND first_name = 'Andrew'\n",
      ");\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT product_name, units_in_stock, reorder_level\n",
      "FROM products\n",
      "WHERE units_in_stock < reorder_level;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT category_id, AVG(unit_price) as avg_price\n",
      "FROM products\n",
      "GROUP BY category_id\n",
      "ORDER BY avg_price DESC\n",
      "LIMIT 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT customers.customer_id, customers.company_name\n",
      "FROM customers\n",
      "LEFT JOIN orders ON customers.customer_id = orders.customer_id\n",
      "WHERE orders.order_id IS NULL;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT products.product_name, SUM(order_details.quantity) as total_units_sold\n",
      "FROM products\n",
      "JOIN order_details ON products.product_id = order_details.product_id\n",
      "GROUP BY products.product_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT e.last_name, e.first_name, SUM(od.unit_price * od.quantity) as total_sales\n",
      "FROM employees e\n",
      "JOIN orders o ON e.employee_id = o.employee_id\n",
      "JOIN order_details od ON o.order_id = od.order_id\n",
      "GROUP BY e.employee_id\n",
      "ORDER BY total_sales DESC\n",
      "LIMIT 3;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT country AS shipping_country, COUNT(*) AS order_count\n",
      "FROM orders\n",
      "GROUP BY country\n",
      "ORDER BY order_count DESC\n",
      "LIMIT 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT products.product_name, suppliers.company_name AS supplier, categories.category_name\n",
      "FROM products\n",
      "JOIN suppliers ON products.supplier_id = suppliers.supplier_id\n",
      "JOIN categories ON products.category_id = categories.category_id;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT AVG(julianday(shipped_date) - julianday(order_date)) AS avg_days_between_order_and_ship_date\n",
      "FROM orders;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "JOIN order_details od ON c.customer_id = od.customer_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "WHERE NOT EXISTS (\n",
      "    SELECT 1\n",
      "    FROM products p2\n",
      "    WHERE p2.product_id != p.product_id\n",
      "    AND NOT EXISTS (\n",
      "        SELECT 1\n",
      "        FROM order_details od2\n",
      "        WHERE od2.product_id = p2.product_id\n",
      "        AND od2.customer_id = c.customer_id\n",
      "    )\n",
      ")\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT employees.first_name, employees.last_name, COUNT(territories.territory_id) AS number_of_territories\n",
      "FROM employees\n",
      "LEFT JOIN employee_territories ON employees.employee_id = employee_territories.employee_id\n",
      "LEFT JOIN territories ON employee_territories.territory_id = territories.territory_id\n",
      "GROUP BY employees.employee_id, employees.first_name, employees.last_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT categories.category_name, MAX(products.unit_price) AS MaxPrice\n",
      "FROM categories\n",
      "JOIN products ON categories.category_id = products.category_id\n",
      "GROUP BY categories.category_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "LEFT JOIN order_details od ON c.customer_id = od.customer_id\n",
      "LEFT JOIN products p ON od.product_id = p.product_id\n",
      "WHERE p.discontinued = 0 OR p.product_id IS NULL;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT \n",
      "    p.product_name, \n",
      "    AVG(od.discount) AS avg_discount\n",
      "FROM \n",
      "    products p\n",
      "JOIN \n",
      "    order_details od ON p.product_id = od.product_id\n",
      "GROUP BY \n",
      "    p.product_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT e1.last_name, e1.first_name, e2.last_name, e2.first_name\n",
      "FROM employees e1\n",
      "JOIN employees e2 ON e1.title = e2.title AND e1.employee_id != e2.employee_id;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT suppliers.company_name, SUM(order_details.unit_price * order_details.quantity) AS total_sales\n",
      "FROM suppliers\n",
      "JOIN products ON suppliers.supplier_id = products.supplier_id\n",
      "JOIN order_details ON products.product_id = order_details.product_id\n",
      "GROUP BY suppliers.company_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT product_name, COUNT(*) as order_count\n",
      "FROM order_details\n",
      "JOIN products ON order_details.product_id = products.product_id\n",
      "GROUP BY product_name\n",
      "HAVING order_count > (\n",
      "    SELECT AVG(order_count)\n",
      "    FROM (\n",
      "        SELECT product_id, COUNT(*) as order_count\n",
      "        FROM order_details\n",
      "        GROUP BY product_id\n",
      "    )\n",
      ");\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "JOIN order_details od ON c.customer_id = od.customer_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "GROUP BY c.customer_id, c.company_name\n",
      "HAVING COUNT(DISTINCT p.category_id) = (SELECT COUNT(*) FROM categories)\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT o1.product_id, o2.product_id\n",
      "FROM order_details o1\n",
      "JOIN order_details o2 ON o1.order_id = o2.order_id AND o1.product_id <> o2.product_id\n",
      "GROUP BY o1.product_id, o2.product_id\n",
      "ORDER BY COUNT(*) DESC\n",
      "LIMIT 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT e.employee_id, e.last_name, e.first_name\n",
      "FROM employees e\n",
      "JOIN orders o ON e.employee_id = o.employee_id\n",
      "JOIN (\n",
      "    SELECT customer_id\n",
      "    FROM orders\n",
      "    GROUP BY customer_id\n",
      "    HAVING COUNT(DISTINCT customer_id) > 50\n",
      ") AS c ON o.customer_id = c.customer_id;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT c.customer_id, AVG(julianday(o2.order_date) - julianday(o1.order_date)) as avg_time_between_orders\n",
      "FROM customers c\n",
      "JOIN orders o1 ON c.customer_id = o1.customer_id\n",
      "JOIN orders o2 ON c.customer_id = o2.customer_id\n",
      "WHERE o1.order_date < o2.order_date\n",
      "GROUP BY c.customer_id;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT product_id, product_name\n",
      "FROM products\n",
      "WHERE product_id NOT IN (\n",
      "    SELECT product_id\n",
      "    FROM order_details\n",
      ")\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT region.region_description, SUM(order_details.unit_price * order_details.quantity) AS total_revenue\n",
      "FROM orders\n",
      "JOIN order_details ON orders.order_id = order_details.order_id\n",
      "JOIN customers ON orders.customer_id = customers.customer_id\n",
      "JOIN region ON customers.region = region.region_id\n",
      "GROUP BY region.region_description;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT COUNT(*) as total_products\n",
      "FROM products;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT customers.customer_id, COUNT(orders.order_id) AS order_count\n",
      "FROM customers\n",
      "JOIN orders ON customers.customer_id = orders.customer_id\n",
      "GROUP BY customers.customer_id\n",
      "ORDER BY order_count DESC\n",
      "LIMIT 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT categories.category_name, AVG(products.unit_price) \n",
      "FROM categories \n",
      "JOIN products ON categories.category_id = products.category_id \n",
      "GROUP BY categories.category_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT country, COUNT(DISTINCT employee_id) as employee_count\n",
      "FROM employees\n",
      "GROUP BY country;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT product_name, unit_price\n",
      "FROM products\n",
      "WHERE unit_price > (\n",
      "    SELECT AVG(unit_price)\n",
      "    FROM products\n",
      ");\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT ship_country, COUNT(*) as order_count\n",
      "FROM orders\n",
      "GROUP BY ship_country\n",
      "ORDER BY order_count DESC\n",
      "LIMIT 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT supplier_id, company_name, contact_name, contact_title, address, city, region, postal_code, country, phone, fax\n",
      "FROM suppliers;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT SUM(units_on_order) as total_units_on_order\n",
      "FROM products;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT first_name, last_name\n",
      "FROM employees\n",
      "WHERE employee_id NOT IN (\n",
      "    SELECT DISTINCT employee_id\n",
      "    FROM orders\n",
      ")\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT AVG(quantity) \n",
      "FROM order_details\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT categories.category_name, COUNT(products.product_id) AS number_of_products\n",
      "FROM categories\n",
      "LEFT JOIN products ON categories.category_id = products.category_id\n",
      "GROUP BY categories.category_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT e.employee_id, SUM(od.unit_price * od.quantity) AS total_sales\n",
      "FROM employees e\n",
      "JOIN orders o ON e.employee_id = o.employee_id\n",
      "JOIN order_details od ON o.order_id = od.order_id\n",
      "GROUP BY e.employee_id;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT customers.customer_id, SUM(order_details.quantity) as total_items\n",
      "FROM customers\n",
      "JOIN orders ON customers.customer_id = orders.customer_id\n",
      "JOIN order_details ON orders.order_id = order_details.order_id\n",
      "GROUP BY customers.customer_id\n",
      "HAVING total_items > 100;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT AVG(julianday(shipped_date) - julianday(order_date)) AS avg_shipping_delay\n",
      "FROM orders;\n",
      "</SQL>\n",
      "\n",
      "Explanation:\n",
      "The ORDERS table contains both the order_date and shipped_date columns, which are needed to calculate the shipping delay. To calculate the delay in days, we can subtract the Julian day of the order date from the Julian day of the shipped date. The AVG function is then used to calculate the average shipping delay.\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT product_name, supplier_id\n",
      "FROM products\n",
      "GROUP BY product_name, supplier_id\n",
      "HAVING COUNT(supplier_id) > 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT COUNT(territory_id) AS total_number_of_territories\n",
      "FROM territories;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT E.employee_id, E.last_name, E.first_name\n",
      "FROM employees E\n",
      "JOIN employee_territories ET ON E.employee_id = ET.employee_id\n",
      "JOIN territories T ON ET.territory_id = T.territory_id\n",
      "GROUP BY E.employee_id, E.last_name, E.first_name, T.region_id\n",
      "HAVING COUNT(DISTINCT T.region_id) > 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT product_name, unit_price\n",
      "FROM products\n",
      "ORDER BY unit_price DESC\n",
      "LIMIT 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "JOIN orders o ON c.customer_id = o.customer_id\n",
      "WHERE c.customer_id IN (\n",
      "    SELECT o2.customer_id\n",
      "    FROM orders o2\n",
      "    GROUP BY o2.customer_id\n",
      "    HAVING COUNT(DISTINCT o2.ship_country) > 1\n",
      ");\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT AVG(discount) AS avg_discount\n",
      "FROM order_details;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT product_id, product_name\n",
      "FROM products\n",
      "WHERE units_on_order = 0;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT SUM(freight) as total_freight_cost\n",
      "FROM orders\n",
      "WHERE ship_country = 'France';\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT categories.category_name, COUNT(products.product_id) AS num_discontinued\n",
      "FROM categories\n",
      "JOIN products ON categories.category_id = products.category_id\n",
      "WHERE products.discontinued = 1\n",
      "GROUP BY categories.category_name\n",
      "ORDER BY num_discontinued DESC\n",
      "LIMIT 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT supplier_id, company_name\n",
      "FROM suppliers\n",
      "WHERE supplier_id IN (\n",
      "    SELECT supplier_id\n",
      "    FROM products\n",
      "    GROUP BY supplier_id\n",
      "    HAVING COUNT(DISTINCT product_id) > 5\n",
      ")\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT s.company_name AS supplier, AVG(p.unit_price) AS avg_unit_price\n",
      "FROM suppliers s\n",
      "JOIN products p ON s.supplier_id = p.supplier_id\n",
      "GROUP BY s.company_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT categories.category_name, SUM(products.units_in_stock * products.unit_price) AS total_value\n",
      "FROM categories\n",
      "JOIN products ON categories.category_id = products.category_id\n",
      "GROUP BY categories.category_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT e.employee_id, e.last_name, e.first_name\n",
      "FROM employees e\n",
      "JOIN orders o ON e.employee_id = o.employee_id\n",
      "JOIN order_details od ON o.order_id = od.order_id\n",
      "GROUP BY e.employee_id, e.last_name, e.first_name\n",
      "HAVING COUNT(DISTINCT od.product_id) > 50;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT AVG(julian_required - julian_shipped) AS avg_days_between\n",
      "FROM (\n",
      "    SELECT \n",
      "        julianday(required_date) - julianday(shipped_date) AS julian_required,\n",
      "        julianday(shipped_date) - julianday('1970-01-01') AS julian_shipped\n",
      "    FROM orders\n",
      ") AS diff_days;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT products.product_name\n",
      "FROM products\n",
      "WHERE products.discontinued = 1\n",
      "AND products.product_id IN (\n",
      "    SELECT order_details.product_id\n",
      "    FROM order_details\n",
      ");\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT e1.last_name, e2.last_name\n",
      "FROM employees e1\n",
      "JOIN employees e2 ON e1.last_name = e2.last_name\n",
      "WHERE e1.employee_id <> e2.employee_id;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT country, COUNT(DISTINCT customer_id) as total_customers\n",
      "FROM customers\n",
      "GROUP BY country;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT product_name, reorder_level, units_in_stock\n",
      "FROM products\n",
      "WHERE reorder_level > units_in_stock;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT orders.order_id, orders.required_date, orders.shipped_date\n",
      "FROM orders\n",
      "WHERE shipped_date > required_date;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT \n",
      "    s.company_name AS shipper,\n",
      "    AVG(o.freight) AS avg_freight\n",
      "FROM \n",
      "    orders o\n",
      "JOIN \n",
      "    shippers s ON o.ship_via = s.shipper_id\n",
      "GROUP BY \n",
      "    s.company_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT employees.employee_id, employees.last_name, employees.first_name, SUM(products.unit_price * order_details.quantity) as total_value\n",
      "FROM employees\n",
      "JOIN orders ON employees.employee_id = orders.employee_id\n",
      "JOIN order_details ON orders.order_id = order_details.order_id\n",
      "JOIN products ON order_details.product_id = products.product_id\n",
      "GROUP BY employees.employee_id, employees.last_name, employees.first_name\n",
      "HAVING total_value > 100000;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT category_id, COUNT(*) as order_count\n",
      "FROM order_details\n",
      "JOIN products ON order_details.product_id = products.product_id\n",
      "GROUP BY category_id\n",
      "ORDER BY order_count DESC\n",
      "LIMIT 1;\n",
      "</SQL>\n",
      "\n",
      "Explanation:\n",
      "\n",
      "1. We start by selecting the `category_id` column from the `products` table, as well as the `order_count` which we will calculate using the `COUNT` function.\n",
      "2. We then join the `order_details` table with the `products` table on the `product_id` column, as we need to know the category of each product in the orders.\n",
      "3. We group the results by `category_id` using the `GROUP BY` clause, so that we can count the number of orders for each category.\n",
      "4. We then order the results by `order_count` in descending order using the `ORDER BY` clause, so that the category with the most orders appears first.\n",
      "5. Finally, we limit the results to only the top category using the `LIMIT` clause.\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "JOIN orders o ON c.customer_id = o.customer_id\n",
      "JOIN order_details od ON o.order_id = od.order_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "JOIN categories cat ON p.category_id = cat.category_id\n",
      "GROUP BY c.customer_id, c.company_name\n",
      "HAVING COUNT(DISTINCT cat.category_id) = (SELECT COUNT(*) FROM categories)\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT AVG(num_products) \n",
      "FROM (\n",
      "    SELECT category_id, COUNT(product_id) AS num_products \n",
      "    FROM products \n",
      "    GROUP BY category_id\n",
      ")\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT COUNT(order_id)\n",
      "FROM orders;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT product_name, unit_price \n",
      "FROM products \n",
      "ORDER BY unit_price DESC \n",
      "LIMIT 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT COUNT(*)\n",
      "FROM employees;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT country \n",
      "FROM customers;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT AVG(quantity) as avg_quantity\n",
      "FROM order_details;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT country, COUNT(DISTINCT supplier_id) as num_suppliers\n",
      "FROM suppliers\n",
      "GROUP BY country;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT SUM(order_details.unit_price * order_details.quantity) AS total_revenue\n",
      "FROM orders\n",
      "JOIN order_details ON orders.order_id = order_details.order_id;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT e.employee_id, e.last_name, e.first_name, COUNT(et.territory_id) as num_territories\n",
      "FROM employees e\n",
      "JOIN employee_territories et ON e.employee_id = et.employee_id\n",
      "GROUP BY e.employee_id, e.last_name, e.first_name\n",
      "ORDER BY num_territories DESC\n",
      "LIMIT 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT product_name\n",
      "FROM products\n",
      "WHERE product_name LIKE '%chef%';\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT COUNT(DISTINCT product_id)\n",
      "FROM order_details\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT customers.customer_id, SUM(order_details.unit_price * order_details.quantity) as total_spent\n",
      "FROM customers\n",
      "JOIN orders ON customers.customer_id = orders.customer_id\n",
      "JOIN order_details ON orders.order_id = order_details.order_id\n",
      "GROUP BY customers.customer_id\n",
      "ORDER BY total_spent DESC\n",
      "LIMIT 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT AVG(julianday(required_date) - julianday(order_date)) AS avg_days\n",
      "FROM orders;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT category_name\n",
      "FROM categories\n",
      "WHERE category_id IN (\n",
      "    SELECT category_id\n",
      "    FROM products\n",
      "    GROUP BY category_id\n",
      "    HAVING COUNT(*) > 10\n",
      ")\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT \n",
      "    employees.first_name, \n",
      "    employees.last_name,\n",
      "    COUNT(orders.order_id) as num_orders\n",
      "FROM \n",
      "    employees\n",
      "JOIN \n",
      "    orders ON employees.employee_id = orders.employee_id\n",
      "GROUP BY \n",
      "    employees.employee_id, employees.first_name, employees.last_name\n",
      "ORDER BY \n",
      "    num_orders DESC\n",
      "LIMIT 5;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT product_id, product_name\n",
      "FROM products\n",
      "WHERE product_id NOT IN (SELECT DISTINCT product_id FROM order_details);\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT AVG(discount) as avg_discount\n",
      "FROM order_details, orders\n",
      "WHERE order_details.order_id = orders.order_id AND orders.freight > 1000;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT s.supplier_id, s.company_name\n",
      "FROM suppliers s\n",
      "JOIN products p ON s.supplier_id = p.supplier_id\n",
      "GROUP BY s.supplier_id, s.company_name\n",
      "HAVING COUNT(p.category_id) > 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT s.company_name, SUM(o.freight) as total_freight_cost\n",
      "FROM orders o\n",
      "JOIN shippers s ON o.ship_via = s.shipper_id\n",
      "GROUP BY s.company_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT e.first_name, e.last_name\n",
      "FROM employees e\n",
      "JOIN orders o ON e.employee_id = o.employee_id\n",
      "JOIN customers c ON o.customer_id = c.customer_id\n",
      "WHERE e.country = c.country;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT AVG(order_details.quantity) \n",
      "FROM orders \n",
      "JOIN order_details ON orders.order_id = order_details.order_id;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "JOIN orders o ON c.customer_id = o.customer_id\n",
      "JOIN order_details od ON o.order_id = od.order_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "JOIN suppliers s ON p.supplier_id = s.supplier_id\n",
      "GROUP BY c.customer_id, c.company_name\n",
      "HAVING COUNT(DISTINCT s.supplier_id) = (SELECT COUNT(*) FROM suppliers)\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT p.product_name, p.unit_price\n",
      "FROM products p\n",
      "JOIN categories c ON p.category_id = c.category_id\n",
      "GROUP BY p.product_id, p.product_name, p.unit_price, c.category_id\n",
      "HAVING p.unit_price > AVG(p.unit_price) WITHIN GROUP (ORDER BY c.category_id);\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT e.employee_id, e.last_name, e.first_name\n",
      "FROM employees e\n",
      "LEFT JOIN orders o ON e.employee_id = o.employee_id\n",
      "LEFT JOIN customers c ON o.customer_id = c.customer_id\n",
      "WHERE c.country IS NULL OR c.country != e.country;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT p1.product_id, p2.product_id\n",
      "FROM products p1, products p2\n",
      "WHERE p1.product_id < p2.product_id\n",
      "AND NOT EXISTS (\n",
      "    SELECT *\n",
      "    FROM order_details od1\n",
      "    JOIN order_details od2 ON od1.order_id = od2.order_id\n",
      "    WHERE od1.product_id = p1.product_id AND od2.product_id = p2.product_id\n",
      ")\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT region, COUNT(DISTINCT orders.customer_id) as total_orders\n",
      "FROM customers\n",
      "JOIN orders ON customers.customer_id = orders.customer_id\n",
      "GROUP BY region;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT AVG(DISTINCT o.customer_id) \n",
      "FROM orders o\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT e.employee_id, e.last_name, e.first_name\n",
      "FROM employees e\n",
      "JOIN orders o ON e.employee_id = o.employee_id\n",
      "JOIN order_details od ON o.order_id = od.order_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "JOIN categories c ON p.category_id = c.category_id\n",
      "GROUP BY e.employee_id, e.last_name, e.first_name\n",
      "HAVING COUNT(DISTINCT c.category_id) = (SELECT COUNT(*) FROM categories)\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "JOIN order_details od ON c.customer_id = od.customer_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "WHERE c.customer_id IN (\n",
      "    SELECT od.customer_id\n",
      "    FROM order_details od\n",
      "    JOIN products p ON od.product_id = p.product_id\n",
      "    GROUP BY od.customer_id, od.product_id\n",
      "    HAVING COUNT(*) > 10\n",
      ")\n",
      "ORDER BY c.customer_id;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT \n",
      "    s.company_name AS shipper_name,\n",
      "    AVG(julianday(o.shipped_date) - julianday(o.order_date)) AS avg_days_between_dates\n",
      "FROM \n",
      "    orders o\n",
      "JOIN \n",
      "    shippers s ON o.ship_via = s.shipper_id\n",
      "GROUP BY \n",
      "    s.company_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT p.product_id, p.product_name\n",
      "FROM products p\n",
      "JOIN orders o ON p.product_id = o.order_id\n",
      "JOIN customers c ON o.customer_id = c.customer_id\n",
      "JOIN territories t ON c.country = t.territory_description\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT COUNT(DISTINCT product_id)\n",
      "FROM products;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT e.first_name, e.last_name, SUM(od.unit_price * od.quantity) as total_sales\n",
      "FROM employees e\n",
      "JOIN orders o ON e.employee_id = o.employee_id\n",
      "JOIN order_details od ON o.order_id = od.order_id\n",
      "GROUP BY e.employee_id, e.first_name, e.last_name\n",
      "ORDER BY total_sales DESC\n",
      "LIMIT 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT c.customer_id, AVG(od.unit_price * od.quantity) as avg_order_value\n",
      "FROM customers c\n",
      "JOIN orders o ON c.customer_id = o.customer_id\n",
      "JOIN order_details od ON o.order_id = od.order_id\n",
      "GROUP BY c.customer_id;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT category_id, SUM(unit_price * quantity) AS total_sales\n",
      "FROM order_details\n",
      "JOIN products ON order_details.product_id = products.product_id\n",
      "GROUP BY category_id\n",
      "ORDER BY total_sales DESC\n",
      "LIMIT 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT s.supplier_id, s.company_name\n",
      "FROM suppliers s\n",
      "JOIN products p ON s.supplier_id = p.supplier_id\n",
      "GROUP BY s.supplier_id, s.company_name\n",
      "HAVING COUNT(p.category_id) > 1;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT AVG(julianday(required_date) - julianday(order_date)) AS avg_days\n",
      "FROM orders;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT \n",
      "    p.product_name, \n",
      "    p.unit_price, \n",
      "    p.units_in_stock, \n",
      "    (p.unit_price - (SELECT AVG(unit_price) FROM products)) / (p.unit_price + 0.0) AS profit_margin\n",
      "FROM \n",
      "    products p\n",
      "ORDER BY \n",
      "    profit_margin DESC\n",
      "LIMIT 5;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT customers.customer_id, customers.company_name\n",
      "FROM customers\n",
      "LEFT JOIN orders ON customers.customer_id = orders.customer_id\n",
      "WHERE orders.order_id IS NULL;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT categories.category_name, SUM(order_details.quantity) as total_units_sold\n",
      "FROM categories\n",
      "JOIN products ON categories.category_id = products.category_id\n",
      "JOIN order_details ON products.product_id = order_details.product_id\n",
      "GROUP BY categories.category_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT e.employee_id, e.last_name, e.first_name\n",
      "FROM employees e\n",
      "JOIN orders o ON e.employee_id = o.employee_id\n",
      "JOIN order_details od ON o.order_id = od.order_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "JOIN categories c ON p.category_id = c.category_id\n",
      "GROUP BY e.employee_id, e.last_name, e.first_name\n",
      "HAVING COUNT(DISTINCT c.category_id) = (SELECT COUNT(*) FROM categories)\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT categories.category_name, AVG(order_details.discount) AS avg_discount\n",
      "FROM categories\n",
      "JOIN products ON categories.category_id = products.category_id\n",
      "JOIN order_details ON products.product_id = order_details.product_id\n",
      "GROUP BY categories.category_name;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT shipped_country, SUM(order_details.unit_price * order_details.quantity) AS total_value\n",
      "FROM orders\n",
      "JOIN order_details ON orders.order_id = order_details.order_id\n",
      "JOIN shippers ON orders.ship_via = shippers.shipper_id\n",
      "GROUP BY shipped_country;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT product_name\n",
      "FROM products\n",
      "WHERE discontinued = 0;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "JOIN orders o ON c.customer_id = o.customer_id\n",
      "JOIN order_details od ON o.order_id = od.order_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "JOIN suppliers s ON p.supplier_id = s.supplier_id\n",
      "GROUP BY c.customer_id, c.company_name\n",
      "HAVING COUNT(DISTINCT s.supplier_id) = (SELECT COUNT(*) FROM suppliers)\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT e.employee_id, SUM(p.unit_price * od.quantity) AS total_revenue\n",
      "FROM employees e\n",
      "JOIN orders o ON e.employee_id = o.employee_id\n",
      "JOIN order_details od ON o.order_id = od.order_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "GROUP BY e.employee_id;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT products.product_name\n",
      "FROM products\n",
      "WHERE products.product_id IN (\n",
      "    SELECT order_details.product_id\n",
      "    FROM order_details\n",
      "    GROUP BY order_details.product_id\n",
      "    HAVING COUNT(DISTINCT orders.region_id) = (SELECT COUNT(*) FROM region)\n",
      ");\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT c.customer_id, AVG(od.quantity) as avg_products_per_order\n",
      "FROM customers c\n",
      "JOIN orders o ON c.customer_id = o.customer_id\n",
      "JOIN order_details od ON o.order_id = od.order_id\n",
      "GROUP BY c.customer_id;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT e.employee_id, e.last_name, e.first_name\n",
      "FROM employees e\n",
      "JOIN orders o ON e.employee_id = o.employee_id\n",
      "WHERE o.customer_id IN (\n",
      "    SELECT DISTINCT customer_id\n",
      "    FROM customers\n",
      "    WHERE country IN (\n",
      "        SELECT DISTINCT country\n",
      "        FROM customers\n",
      "    )\n",
      ")\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT p.product_name\n",
      "FROM products p\n",
      "WHERE (\n",
      "    SELECT COUNT(DISTINCT o.customer_id)\n",
      "    FROM order_details od\n",
      "    JOIN orders o ON od.order_id = o.order_id\n",
      "    WHERE od.product_id = p.product_id\n",
      ") > 0.5 * (\n",
      "    SELECT COUNT(DISTINCT customer_id)\n",
      "    FROM orders\n",
      ");\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT DISTINCT s.supplier_id, s.company_name\n",
      "FROM suppliers s\n",
      "JOIN products p ON s.supplier_id = p.supplier_id\n",
      "JOIN order_details od ON p.product_id = od.product_id\n",
      "JOIN orders o ON od.order_id = o.order_id\n",
      "WHERE o.ship_country IN (\n",
      "    SELECT DISTINCT country\n",
      "    FROM customers\n",
      ")\n",
      "GROUP BY s.supplier_id, s.company_name\n",
      "HAVING COUNT(DISTINCT o.ship_country) = (\n",
      "    SELECT COUNT(DISTINCT country)\n",
      "    FROM customers\n",
      ");\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT e.employee_id, c.customer_id, SUM(od.unit_price * od.quantity) AS total_value\n",
      "FROM employees e\n",
      "JOIN orders o ON e.employee_id = o.employee_id\n",
      "JOIN order_details od ON o.order_id = od.order_id\n",
      "JOIN customers c ON o.customer_id = c.customer_id\n",
      "GROUP BY e.employee_id, c.customer_id;\n",
      "</SQL>\n",
      "generated_sql_query:  <SQL>\n",
      "SELECT c.customer_id, AVG(DATEDIFF(o.shipped_date, o.order_date)) as avg_days\n",
      "FROM customers c\n",
      "JOIN orders o ON c.customer_id = o.customer_id\n",
      "GROUP BY c.customer_id;\n",
      "</SQL>\n",
      "Error executing statement: no such column: shipped_country\n",
      "Error executing statement: no such column: country\n",
      "Error executing statement: no such column: od2.customer_id\n",
      "Error executing statement: no such column: od.customer_id\n",
      "Error executing statement: no such column: od.customer_id\n",
      "Error executing statement: near \"WITHIN\": syntax error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29464/2346174277.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['Generated_SQL_Query'] = generated_sql_queries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing statement: no such column: od.customer_id\n",
      "Error executing statement: ambiguous column name: unit_price\n",
      "Error executing statement: no such column: shipped_country\n",
      "Error executing statement: no such column: orders.region_id\n",
      "Error executing statement: no such function: DATEDIFF\n",
      "                                        Question  \\\n",
      "0         What is the total number of customers?   \n",
      "1  List all product names and their unit prices.   \n",
      "2    Who are the top 5 customers by order count?   \n",
      "\n",
      "                                               Query  \\\n",
      "0                   SELECT COUNT(*)\\nFROM customers;   \n",
      "1  SELECT products.product_name, products.unit_pr...   \n",
      "2  SELECT customers.customer_id, COUNT(orders.ord...   \n",
      "\n",
      "                                              Result Error  \\\n",
      "0                                            [(91,)]  None   \n",
      "1  [(Chai, 18.0), (Chang, 19.0), (Aniseed Syrup, ...  None   \n",
      "2  [(SAVEA, 31), (ERNSH, 30), (QUICK, 28), (HUNGO...  None   \n",
      "\n",
      "                                      ReferenceQuery  \\\n",
      "0                    SELECT COUNT(*) FROM customers;   \n",
      "1     SELECT product_name, unit_price FROM products;   \n",
      "2  SELECT c.customer_id, c.company_name, COUNT(o....   \n",
      "\n",
      "                                             Context  \n",
      "0  CREATE TABLE categories (\\n    category_id sma...  \n",
      "1  CREATE TABLE categories (\\n    category_id sma...  \n",
      "2  CREATE TABLE categories (\\n    category_id sma...  \n",
      "Number of successful queries: 112\n",
      "Number of unsuccessful queries: 11\n"
     ]
    }
   ],
   "source": [
    "# 8a. Use this golden dataset to run test with smaller LLM\n",
    "\n",
    "MODEL_ID = \"mistral.mixtral-8x7b-instruct-v0:1\" # \"anthropic.claude-3-haiku-20240307-v1:0\" # \"mistral.mixtral-8x7b-instruct-v0:1\" \"anthropic.claude-3-5-sonnet-20240620-v1:0\" \"meta.llama3-1-70b-instruct-v1:0\"\n",
    "\n",
    "# use helper class for threaded API calls\n",
    "wrapper = BedrockLLMWrapper(debug=False, model_id=MODEL_ID, max_token_count=500)\n",
    "df1 = df_good_results\n",
    "prompts_list = []\n",
    "for row in df1.itertuples():\n",
    "    prompt = build_sqlquerygen_prompt(row.Question, row.Context)\n",
    "    prompts_list.append(prompt)\n",
    "# [result_text,usage,query_time]\n",
    "results = wrapper.generate_threaded(prompts_list)\n",
    "\n",
    "# Create a list to store the generated SQL queries\n",
    "generated_sql_queries = []\n",
    "for result in results:\n",
    "    generated_sql_query = result[0].replace(\"\\\\\",\"\") # workaround, switching to ConverseAPI introduced \\ in Mistral response\n",
    "    print(f'generated_sql_query: {generated_sql_query}')\n",
    "    generated_sql_queries.append(generated_sql_query)\n",
    "\n",
    "# Add the new column 'Generated_SQL_Query' to df_results\n",
    "df1['Generated_SQL_Query'] = generated_sql_queries\n",
    "\n",
    "\n",
    "# Test generated SQL queries and verify they work\n",
    "results = []\n",
    "\n",
    "if SQL_DATABASE == 'LOCAL':\n",
    "    # Create a SQLite database connection\n",
    "    conn = sqlite3.connect('routedb.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "for row in df1.itertuples():\n",
    "    statement = extract_with_regex(row.Generated_SQL_Query, SQL_PATTERN)\n",
    "    # print(f'SQL statement: {statement}')\n",
    "    error = None\n",
    "    result = None\n",
    "    try:\n",
    "        \n",
    "        if SQL_DATABASE == 'LOCAL':\n",
    "            # Use local SQL lite\n",
    "            \n",
    "            # Replace PostgreSQL-specific syntax with SQLite equivalents\n",
    "            \n",
    "            try:\n",
    "                cursor.execute(statement)\n",
    "                # Fetch all rows from the result\n",
    "                result = cursor.fetchall()\n",
    "                # print(result)\n",
    "\n",
    "            except sqlite3.Error as e:\n",
    "                print(f\"Error executing statement: {e}\")\n",
    "                error = e\n",
    "                # print(f\"Statement: {statement}\")\n",
    "\n",
    "        else:\n",
    "            # Use Athena if AWS Glue Schema is used\n",
    "            result = execute_athena_query(DATABASE, row.Generated_SQL_Query)\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error = e\n",
    "\n",
    "    results.append({'Question': row.Question,'Query': statement, 'Result': result, 'Error': error, 'ReferenceQuery': row.Query, 'Context': row.Context})\n",
    "\n",
    "if SQL_DATABASE == 'LOCAL':\n",
    "    # close the connection\n",
    "    conn.close()\n",
    "\n",
    "# inspect first 3 results\n",
    "df1_results = pd.DataFrame(results)\n",
    "print(df1_results.head(3))\n",
    "\n",
    "# review successful/unsucessful queries\n",
    "df1_good_results = df1_results[df1_results['Error'].isnull() | (df1_results['Error'] == None)]\n",
    "print(f\"Number of successful queries: {len(df1_good_results)}\")\n",
    "\n",
    "df1_bad_results = df1_results[df1_results['Error'].notnull() | (df1_results['Error'] == 'None')]\n",
    "print(f\"Number of unsuccessful queries: {len(df1_bad_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29464/2975820441.py:755: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Generated_SQL_Query'] = generated_sql_queries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing statement: no such column: day\n",
      "Error executing statement: ambiguous column name: order_id\n",
      "                                        Question  \\\n",
      "0         What is the total number of customers?   \n",
      "1  List all product names and their unit prices.   \n",
      "2    Who are the top 5 customers by order count?   \n",
      "\n",
      "                                               Query  \\\n",
      "0  SELECT COUNT(*) AS total_customers\\nFROM custo...   \n",
      "1    SELECT product_name, unit_price\\nFROM products;   \n",
      "2  SELECT c.company_name, COUNT(o.order_id) AS or...   \n",
      "\n",
      "                                              Result Error  \\\n",
      "0                                            [(91,)]  None   \n",
      "1  [(Chai, 18.0), (Chang, 19.0), (Aniseed Syrup, ...  None   \n",
      "2  [(Save-a-lot Markets, 31), (Ernst Handel, 30),...  None   \n",
      "\n",
      "                                      ReferenceQuery  \\\n",
      "0                    SELECT COUNT(*) FROM customers;   \n",
      "1     SELECT product_name, unit_price FROM products;   \n",
      "2  SELECT c.customer_id, c.company_name, COUNT(o....   \n",
      "\n",
      "                                             Context  \n",
      "0  CREATE TABLE categories (\\n    category_id sma...  \n",
      "1  CREATE TABLE categories (\\n    category_id sma...  \n",
      "2  CREATE TABLE categories (\\n    category_id sma...  \n",
      "Number of successful queries: 121\n",
      "Number of unsuccessful queries: 2\n"
     ]
    }
   ],
   "source": [
    "# 8b. Use golden dataset to run test with larger LLM\n",
    "\n",
    "MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\" # \"anthropic.claude-3-haiku-20240307-v1:0\" # \"mistral.mixtral-8x7b-instruct-v0:1\" \"anthropic.claude-3-5-sonnet-20240620-v1:0\" \"meta.llama3-1-70b-instruct-v1:0\"\n",
    "\n",
    "df2 = runBedrockBatchJob(MODEL_ID, df_good_results)\n",
    "\n",
    "# Test generated SQL queries and verify they work\n",
    "results = []\n",
    "\n",
    "if SQL_DATABASE == 'LOCAL':\n",
    "    # Create a SQLite database connection\n",
    "    conn = sqlite3.connect('routedb.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "for row in df2.itertuples():\n",
    "    statement = extract_with_regex(row.Generated_SQL_Query, SQL_PATTERN)\n",
    "    # print(f'SQL statement: {statement}')\n",
    "    error = None\n",
    "    try:\n",
    "        \n",
    "        if SQL_DATABASE == 'LOCAL':\n",
    "            # Use local SQL lite\n",
    "            \n",
    "            # Replace PostgreSQL-specific syntax with SQLite equivalents\n",
    "            \n",
    "            try:\n",
    "                cursor.execute(statement)\n",
    "                # Fetch all rows from the result\n",
    "                result = cursor.fetchall()\n",
    "                # print(result)\n",
    "\n",
    "            except sqlite3.Error as e:\n",
    "                print(f\"Error executing statement: {e}\")\n",
    "                error = e\n",
    "                # print(f\"Statement: {statement}\")\n",
    "\n",
    "        else:\n",
    "            # Use Athena if AWS Glue Schema is used\n",
    "            result = execute_athena_query(DATABASE, row.Generated_SQL_Query)\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error = e\n",
    "\n",
    "    results.append({'Question': row.Question,'Query': statement, 'Result': result, 'Error': error, 'ReferenceQuery': row.Query, 'Context': row.Context})\n",
    "\n",
    "if SQL_DATABASE == 'LOCAL':\n",
    "    # close the connection\n",
    "    conn.close()\n",
    "\n",
    "df2_results = pd.DataFrame(results)\n",
    "\n",
    "# inspect first 3 results\n",
    "print(df2_results.head(3))\n",
    "\n",
    "# review successful/unsucessful queries\n",
    "df2_good_results = df2_results[df2_results['Error'].isnull() | (df2_results['Error'] == None)]\n",
    "print(f\"Number of successful queries: {len(df2_good_results)}\")\n",
    "\n",
    "df2_bad_results = df2_results[df2_results['Error'].notnull() | (df2_results['Error'] == 'None')]\n",
    "print(f\"Number of unsuccessful queries: {len(df2_bad_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we can observe that the larger LLM is able to produce valid SQL queries slightly more successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing SQL: Execution failed on sql 'SELECT shipped_country, COUNT(*) as num_orders\n",
      "FROM orders\n",
      "GROUP BY shipped_country;': no such column: shipped_country\n",
      "Error executing SQL: Execution failed on sql 'SELECT shipped_country, COUNT(*) as num_orders\n",
      "FROM orders\n",
      "GROUP BY shipped_country;': no such column: shipped_country\n",
      "Error executing SQL: Execution failed on sql 'SELECT country AS shipping_country, COUNT(*) AS order_count\n",
      "FROM orders\n",
      "GROUP BY country\n",
      "ORDER BY order_count DESC\n",
      "LIMIT 1;': no such column: country\n",
      "Error executing SQL: Execution failed on sql 'SELECT country AS shipping_country, COUNT(*) AS order_count\n",
      "FROM orders\n",
      "GROUP BY country\n",
      "ORDER BY order_count DESC\n",
      "LIMIT 1;': no such column: country\n",
      "Error executing SQL: Execution failed on sql 'SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "JOIN order_details od ON c.customer_id = od.customer_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "WHERE NOT EXISTS (\n",
      "    SELECT 1\n",
      "    FROM products p2\n",
      "    WHERE p2.product_id != p.product_id\n",
      "    AND NOT EXISTS (\n",
      "        SELECT 1\n",
      "        FROM order_details od2\n",
      "        WHERE od2.product_id = p2.product_id\n",
      "        AND od2.customer_id = c.customer_id\n",
      "    )\n",
      ")': no such column: od2.customer_id\n",
      "Error executing SQL: Execution failed on sql 'SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "JOIN order_details od ON c.customer_id = od.customer_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "WHERE NOT EXISTS (\n",
      "    SELECT 1\n",
      "    FROM products p2\n",
      "    WHERE p2.product_id != p.product_id\n",
      "    AND NOT EXISTS (\n",
      "        SELECT 1\n",
      "        FROM order_details od2\n",
      "        WHERE od2.product_id = p2.product_id\n",
      "        AND od2.customer_id = c.customer_id\n",
      "    )\n",
      ")': no such column: od2.customer_id\n",
      "Error executing SQL: Execution failed on sql 'SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "LEFT JOIN order_details od ON c.customer_id = od.customer_id\n",
      "LEFT JOIN products p ON od.product_id = p.product_id\n",
      "WHERE p.discontinued = 0 OR p.product_id IS NULL;': no such column: od.customer_id\n",
      "Error executing SQL: Execution failed on sql 'SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "LEFT JOIN order_details od ON c.customer_id = od.customer_id\n",
      "LEFT JOIN products p ON od.product_id = p.product_id\n",
      "WHERE p.discontinued = 0 OR p.product_id IS NULL;': no such column: od.customer_id\n",
      "Error executing SQL: Execution failed on sql 'SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "JOIN order_details od ON c.customer_id = od.customer_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "GROUP BY c.customer_id, c.company_name\n",
      "HAVING COUNT(DISTINCT p.category_id) = (SELECT COUNT(*) FROM categories)': no such column: od.customer_id\n",
      "Error executing SQL: Execution failed on sql 'SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "JOIN order_details od ON c.customer_id = od.customer_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "GROUP BY c.customer_id, c.company_name\n",
      "HAVING COUNT(DISTINCT p.category_id) = (SELECT COUNT(*) FROM categories)': no such column: od.customer_id\n",
      "Error executing SQL: Execution failed on sql 'SELECT p.product_name, p.unit_price\n",
      "FROM products p\n",
      "JOIN categories c ON p.category_id = c.category_id\n",
      "GROUP BY p.product_id, p.product_name, p.unit_price, c.category_id\n",
      "HAVING p.unit_price > AVG(p.unit_price) WITHIN GROUP (ORDER BY c.category_id);': near \"WITHIN\": syntax error\n",
      "Error executing SQL: Execution failed on sql 'SELECT p.product_name, p.unit_price\n",
      "FROM products p\n",
      "JOIN categories c ON p.category_id = c.category_id\n",
      "GROUP BY p.product_id, p.product_name, p.unit_price, c.category_id\n",
      "HAVING p.unit_price > AVG(p.unit_price) WITHIN GROUP (ORDER BY c.category_id);': near \"WITHIN\": syntax error\n",
      "Error executing SQL: Execution failed on sql 'SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "JOIN order_details od ON c.customer_id = od.customer_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "WHERE c.customer_id IN (\n",
      "    SELECT od.customer_id\n",
      "    FROM order_details od\n",
      "    JOIN products p ON od.product_id = p.product_id\n",
      "    GROUP BY od.customer_id, od.product_id\n",
      "    HAVING COUNT(*) > 10\n",
      ")\n",
      "ORDER BY c.customer_id;': no such column: od.customer_id\n",
      "Error executing SQL: Execution failed on sql 'SELECT DISTINCT c.customer_id, c.company_name\n",
      "FROM customers c\n",
      "JOIN order_details od ON c.customer_id = od.customer_id\n",
      "JOIN products p ON od.product_id = p.product_id\n",
      "WHERE c.customer_id IN (\n",
      "    SELECT od.customer_id\n",
      "    FROM order_details od\n",
      "    JOIN products p ON od.product_id = p.product_id\n",
      "    GROUP BY od.customer_id, od.product_id\n",
      "    HAVING COUNT(*) > 10\n",
      ")\n",
      "ORDER BY c.customer_id;': no such column: od.customer_id\n",
      "Error executing SQL: Execution failed on sql 'SELECT category_id, SUM(unit_price * quantity) AS total_sales\n",
      "FROM order_details\n",
      "JOIN products ON order_details.product_id = products.product_id\n",
      "GROUP BY category_id\n",
      "ORDER BY total_sales DESC\n",
      "LIMIT 1;': ambiguous column name: unit_price\n",
      "Error executing SQL: Execution failed on sql 'SELECT category_id, SUM(unit_price * quantity) AS total_sales\n",
      "FROM order_details\n",
      "JOIN products ON order_details.product_id = products.product_id\n",
      "GROUP BY category_id\n",
      "ORDER BY total_sales DESC\n",
      "LIMIT 1;': ambiguous column name: unit_price\n",
      "Error executing SQL: Execution failed on sql 'SELECT shipped_country, SUM(order_details.unit_price * order_details.quantity) AS total_value\n",
      "FROM orders\n",
      "JOIN order_details ON orders.order_id = order_details.order_id\n",
      "JOIN shippers ON orders.ship_via = shippers.shipper_id\n",
      "GROUP BY shipped_country;': no such column: shipped_country\n",
      "Error executing SQL: Execution failed on sql 'SELECT shipped_country, SUM(order_details.unit_price * order_details.quantity) AS total_value\n",
      "FROM orders\n",
      "JOIN order_details ON orders.order_id = order_details.order_id\n",
      "JOIN shippers ON orders.ship_via = shippers.shipper_id\n",
      "GROUP BY shipped_country;': no such column: shipped_country\n",
      "Error executing SQL: Execution failed on sql 'SELECT products.product_name\n",
      "FROM products\n",
      "WHERE products.product_id IN (\n",
      "    SELECT order_details.product_id\n",
      "    FROM order_details\n",
      "    GROUP BY order_details.product_id\n",
      "    HAVING COUNT(DISTINCT orders.region_id) = (SELECT COUNT(*) FROM region)\n",
      ");': no such column: orders.region_id\n",
      "Error executing SQL: Execution failed on sql 'SELECT products.product_name\n",
      "FROM products\n",
      "WHERE products.product_id IN (\n",
      "    SELECT order_details.product_id\n",
      "    FROM order_details\n",
      "    GROUP BY order_details.product_id\n",
      "    HAVING COUNT(DISTINCT orders.region_id) = (SELECT COUNT(*) FROM region)\n",
      ");': no such column: orders.region_id\n",
      "Error executing SQL: Execution failed on sql 'SELECT c.customer_id, AVG(DATEDIFF(o.shipped_date, o.order_date)) as avg_days\n",
      "FROM customers c\n",
      "JOIN orders o ON c.customer_id = o.customer_id\n",
      "GROUP BY c.customer_id;': no such function: DATEDIFF\n",
      "Error executing SQL: Execution failed on sql 'SELECT c.customer_id, AVG(DATEDIFF(o.shipped_date, o.order_date)) as avg_days\n",
      "FROM customers c\n",
      "JOIN orders o ON c.customer_id = o.customer_id\n",
      "GROUP BY c.customer_id;': no such function: DATEDIFF\n",
      "                                 Question                            Query   Result Error                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Context                               Generated_SQL_Query                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               reasoning score\n",
      "0  What is the total number of customers?  SELECT COUNT(*) FROM customers;  [(91,)]  None  CREATE TABLE categories (\\n    category_id smallint NOT NULL PRIMARY KEY,\\n    category_name character varying(15) NOT NULL,\\n    description text,\\n    picture bytea\\n);\\n\\nCREATE TABLE customer_demographics (\\n    customer_type_id bpchar NOT NULL PRIMARY KEY,\\n    customer_desc text\\n);\\n\\nCREATE TABLE customers (\\n    customer_id bpchar NOT NULL PRIMARY KEY,\\n    company_name character varying(40) NOT NULL,\\n    contact_name character varying(30),\\n    contact_title character varying(30),\\n    address character varying(60),\\n    city character varying(15),\\n    region character varying(15),\\n    postal_code character varying(10),\\n    country character varying(15),\\n    phone character varying(24),\\n    fax character varying(24)\\n);\\n\\nCREATE TABLE customer_customer_demo (\\n    customer_id bpchar NOT NULL,\\n    customer_type_id bpchar NOT NULL,\\n    PRIMARY KEY (customer_id, customer_type_id),\\n    FOREIGN KEY (customer_type_id) REFERENCES customer_demographics,\\n    FOREIGN KEY (customer_id) REFERENCES customers\\n);\\n\\nCREATE TABLE employees (\\n    employee_id smallint NOT NULL PRIMARY KEY,\\n    last_name character varying(20) NOT NULL,\\n    first_name character varying(10) NOT NULL,\\n    title character varying(30),\\n    title_of_courtesy character varying(25),\\n    birth_date date,\\n    hire_date date,\\n    address character varying(60),\\n    city character varying(15),\\n    region character varying(15),\\n    postal_code character varying(10),\\n    country character varying(15),\\n    home_phone character varying(24),\\n    extension character varying(4),\\n    photo bytea,\\n    notes text,\\n    reports_to smallint,\\n    photo_path character varying(255),\\n\\tFOREIGN KEY (reports_to) REFERENCES employees\\n);\\n\\nCREATE TABLE suppliers (\\n    supplier_id smallint NOT NULL PRIMARY KEY,\\n    company_name character varying(40) NOT NULL,\\n    contact_name character varying(30),\\n    contact_title character varying(30),\\n    address character varying(60),\\n    city character varying(15),\\n    region character varying(15),\\n    postal_code character varying(10),\\n    country character varying(15),\\n    phone character varying(24),\\n    fax character varying(24),\\n    homepage text\\n);\\n\\nCREATE TABLE products (\\n    product_id smallint NOT NULL PRIMARY KEY,\\n    product_name character varying(40) NOT NULL,\\n    supplier_id smallint,\\n    category_id smallint,\\n    quantity_per_unit character varying(20),\\n    unit_price real,\\n    units_in_stock smallint,\\n    units_on_order smallint,\\n    reorder_level smallint,\\n    discontinued integer NOT NULL,\\n\\tFOREIGN KEY (category_id) REFERENCES categories,\\n\\tFOREIGN KEY (supplier_id) REFERENCES suppliers\\n);\\n\\nCREATE TABLE region (\\n    region_id smallint NOT NULL PRIMARY KEY,\\n    region_description bpchar NOT NULL\\n);\\n\\nCREATE TABLE shippers (\\n    shipper_id smallint NOT NULL PRIMARY KEY,\\n    company_name character varying(40) NOT NULL,\\n    phone character varying(24)\\n);\\n\\nCREATE TABLE orders (\\n    order_id smallint NOT NULL PRIMARY KEY,\\n    customer_id bpchar,\\n    employee_id smallint,\\n    order_date date,\\n    required_date date,\\n    shipped_date date,\\n    ship_via smallint,\\n    freight real,\\n    ship_name character varying(40),\\n    ship_address character varying(60),\\n    ship_city character varying(15),\\n    ship_region character varying(15),\\n    ship_postal_code character varying(10),\\n    ship_country character varying(15),\\n    FOREIGN KEY (customer_id) REFERENCES customers,\\n    FOREIGN KEY (employee_id) REFERENCES employees,\\n    FOREIGN KEY (ship_via) REFERENCES shippers\\n);\\n\\nCREATE TABLE territories (\\n    territory_id character varying(20) NOT NULL PRIMARY KEY,\\n    territory_description bpchar NOT NULL,\\n    region_id smallint NOT NULL,\\n\\tFOREIGN KEY (region_id) REFERENCES region\\n);\\n\\nCREATE TABLE employee_territories (\\n    employee_id smallint NOT NULL,\\n    territory_id character varying(20) NOT NULL,\\n    PRIMARY KEY (employee_id, territory_id),\\n    FOREIGN KEY (territory_id) REFERENCES territories,\\n    FOREIGN KEY (employee_id) REFERENCES employees\\n);\\n\\nCREATE TABLE order_details (\\n    order_id smallint NOT NULL,\\n    product_id smallint NOT NULL,\\n    unit_price real NOT NULL,\\n    quantity smallint NOT NULL,\\n    discount real NOT NULL,\\n    PRIMARY KEY (order_id, product_id),\\n    FOREIGN KEY (product_id) REFERENCES products,\\n    FOREIGN KEY (order_id) REFERENCES orders\\n);\\n\\nCREATE TABLE us_states (\\n    state_id smallint NOT NULL PRIMARY KEY,\\n    state_name character varying(100),\\n    state_abbr character varying(2),\\n    state_region character varying(50)\\n);\\n\\n   <SQL>\\nSELECT COUNT(*)\\nFROM customers;\\n</SQL>  The generated SQL query is identical to the groundtruth SQL query, and it correctly returns the total number of customers from the customers table. The execution accuracy and exact set match accuracy metrics are both 1.0, indicating a perfect match with the expected result.\\n\\nThe valid efficiency score of 0.846 suggests that the generated query is slightly less efficient than the groundtruth query, but this difference is likely negligible in most cases.\\n\\nOverall, the generated SQL query is of high quality and meets the requirements of the original question.     5\n"
     ]
    }
   ],
   "source": [
    "# 9. Use LLM as a Judge to and grade generated SQL from smaller LLM \n",
    "MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "# use helper class for threaded API calls, reduce max_tokens to avoid throttling with Sonnet\n",
    "wrapper = BedrockLLMWrapper(debug=False, model_id=MODEL_ID, max_token_count=512)\n",
    "\n",
    "db_path = 'routedb.db'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "prompts_list = []\n",
    "for row in df1.itertuples():\n",
    "\n",
    "    # Calculate Text-to-SQL metrics:\n",
    "    # 1) Execution Accuracy (EX):  compares the generated SQL query to the labeled SQL query to determine if its a match or not. \n",
    "    # 2) Exact Set Match Accuracy (EM) – did the returned result set actually answer the question, regardless of how the query was written\n",
    "    # 3) Valid Efficiency Score (VES)\n",
    "    generated_sql = extract_with_regex(str(row.Generated_SQL_Query).replace(\"\\\\\",\"\"), SQL_PATTERN)\n",
    "    labeled_sql = str(row.Query)\n",
    "\n",
    "    ex_score = execution_accuracy(generated_sql, labeled_sql)\n",
    "    em_score = exact_set_match_accuracy(generated_sql, labeled_sql, conn)\n",
    "    ves_score = valid_efficiency_score(generated_sql, labeled_sql, conn)\n",
    "    # print(f\"Execution Accuracy: {ex_score}\")\n",
    "    # print(f\"Exact Set Match Accuracy: {em_score}\")\n",
    "    # print(f\"Valid Efficiency Score: {ves_score}\")\n",
    "\n",
    "    prompt = build_grader_prompt(\n",
    "        original_instruction=build_sqlquerygen_prompt(str(row.Question), str(row.Context)),\n",
    "        sql_query=generated_sql,\n",
    "        sql_query_run_result=str(row.Result),\n",
    "        sql_query_run_error=str(row.Error),\n",
    "        groundtruth_sql_query=labeled_sql,\n",
    "        ex_score=ex_score,\n",
    "        em_score=em_score,\n",
    "        ves_score=ves_score,\n",
    "    )\n",
    "    prompts_list.append(prompt)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "results = wrapper.generate_threaded(prompts_list)\n",
    "\n",
    "formatted_results = []\n",
    "for i, result in enumerate(results):\n",
    "    reasoning = extract_with_regex(str(result[0]), REASONING_PATTERN)\n",
    "    score = int(extract_with_regex(str(result[0]), SCORE_PATTERN))\n",
    "    \n",
    "    formatted_results.append({\n",
    "        \"reasoning\": str(reasoning),\n",
    "        \"score\": str(score),\n",
    "        \"original_index\": i\n",
    "    })\n",
    "\n",
    "evaluated_df1 = pd.DataFrame(formatted_results)\n",
    "\n",
    "# Sort evaluated_df by the original index to ensure alignment with df1\n",
    "evaluated_df1 = evaluated_df1.sort_values(\"original_index\").reset_index(drop=True)\n",
    "\n",
    "# Drop the temporary 'original_index' column\n",
    "evaluated_df1 = evaluated_df1.drop(columns=[\"original_index\"])\n",
    "\n",
    "# Merge df1 dataframe with columns from evaluated_df\n",
    "df1_graded = pd.concat([df1.reset_index(drop=True), evaluated_df1.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(df1_graded.head(1).to_string())\n",
    "df1_graded.to_json('question_query_small_llm_grades.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1_graded already exists in memory.\n",
      "Data type of 'score' column in df1_graded: int64\n",
      "Percentage correct for smaller LLM: 17.07%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-striped\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Question</th>\n",
       "      <th>Query</th>\n",
       "      <th>Result</th>\n",
       "      <th>Error</th>\n",
       "      <th>Generated_SQL_Query</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Which customers have ordered products from all suppliers?</td>\n",
       "      <td>SELECT c.customer_id, c.company_name FROM customers c WHERE (SELECT COUNT(DISTINCT p.supplier_id) FROM orders o JOIN order_details od ON o.order_id = od.order_id JOIN products p ON od.product_id = p.product_id WHERE o.customer_id = c.customer_id) = (SELECT COUNT(*) FROM suppliers);</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;SQL&gt;\\nSELECT DISTINCT c.customer_id, c.company_name\\nFROM customers c\\nJOIN orders o ON c.customer_id = o.customer_id\\nJOIN order_details od ON o.order_id = od.order_id\\nJOIN products p ON od.product_id = p.product_id\\nJOIN suppliers s ON p.supplier_id = s.supplier_id\\nGROUP BY c.customer_id, c.company_name\\nHAVING COUNT(DISTINCT s.supplier_id) = (SELECT COUNT(*) FROM suppliers)\\n&lt;/SQL&gt;</td>\n",
       "      <td>The generated SQL query is able to correctly identify customers who have ordered products from all suppliers, by joining the customers, orders, order_details, products, and suppliers tables. It uses a HAVING clause to filter for customers who have distinct supplier IDs equal to the total number of suppliers.\\n\\nHowever, the query is not as efficient as the groundtruth query, which uses a correlated subquery to check the condition for each customer directly, rather than joining all the tables first. The groundtruth query is likely to perform better, especially for larger datasets.\\n\\nAdditionally, while the generated query returns the correct result set (EM = 1.0), it does not match the groundtruth query exactly (Execution Accuracy = 0.0). The Valid Efficiency Score of 0.636986301369863 indicates that the generated query is less efficient than the groundtruth query.\\n\\nOverall, the generated query is a reasonable attempt to solve the problem, but it can be improved in terms of efficiency and exactness compared to the groundtruth query.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Which customer has spent the most money on orders?</td>\n",
       "      <td>SELECT c.customer_id, c.company_name, SUM(od.unit_price * od.quantity * (1 - od.discount)) as total_spent FROM customers c JOIN orders o ON c.customer_id = o.customer_id JOIN order_details od ON o.order_id = od.order_id GROUP BY c.customer_id, c.company_name ORDER BY total_spent DESC LIMIT 1;</td>\n",
       "      <td>[(QUICK, QUICK-Stop, 110277.305)]</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;SQL&gt;\\nSELECT customers.customer_id, SUM(order_details.unit_price * order_details.quantity) as total_spent\\nFROM customers\\nJOIN orders ON customers.customer_id = orders.customer_id\\nJOIN order_details ON orders.order_id = order_details.order_id\\nGROUP BY customers.customer_id\\nORDER BY total_spent DESC\\nLIMIT 1;\\n&lt;/SQL&gt;</td>\n",
       "      <td>The generated SQL query is able to find the customer who has spent the most money on orders, which is the main goal of the original question. However, there are a few issues:\\n\\n1. It does not include the company name in the output, which is useful information to identify the customer.\\n2. It does not account for discounts on order details, so the total spent may be slightly overestimated.\\n3. The query structure is a bit more complex than necessary, with multiple joins that could potentially impact performance.\\n\\nThe groundtruth SQL query addresses these issues by including the company name, accounting for discounts, and using a more concise query structure with a single join across all three tables.\\n\\nWhile the generated query produces the correct result, it is not as efficient or complete as the groundtruth query. Therefore, I would rate it as a medium quality query compared to the groundtruth.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What is the average time between orders for each customer?</td>\n",
       "      <td>SELECT customer_id, AVG(next_order_date - order_date) as avg_days_between_orders FROM (SELECT customer_id, order_date, LEAD(order_date) OVER (PARTITION BY customer_id ORDER BY order_date) as next_order_date FROM orders) as subquery WHERE next_order_date IS NOT NULL GROUP BY customer_id ORDER BY avg_days_between_orders;</td>\n",
       "      <td>[(FOLIG, 0.0), (LACOR, 0.0), (LAZYK, 0.0), (SAVEA, 0.06666666666666667), (ERNSH, 0.06896551724137931), (QUICK, 0.07407407407407407), (MEREP, 0.08333333333333333), (LINOD, 0.09090909090909091), (GREAL, 0.1), (FOLKO, 0.1111111111111111), (HUNGO, 0.1111111111111111), (BERGS, 0.11764705882352941), (HILAA, 0.11764705882352941), (RATTC, 0.11764705882352941), (BONAP, 0.125), (GOURL, 0.125), (FRANK, 0.14285714285714285), (LEHMS, 0.14285714285714285), (WARTH, 0.14285714285714285), (BOTTM, 0.15384615384615385), (HANAR, 0.15384615384615385), (KOENE, 0.15384615384615385), (LAMAI, 0.15384615384615385), (LILAS, 0.15384615384615385), (WHITC, 0.15384615384615385), (AROUT, 0.16666666666666666), (BLAUS, 0.16666666666666666), (FAMIA, 0.16666666666666666), (MAISD, 0.16666666666666666), (QUEEN, 0.16666666666666666), (WILMK, 0.16666666666666666), (REGGC, 0.18181818181818182), (SUPRD, 0.18181818181818182), (ALFKI, 0.2), (BLONP, 0.2), (CACTU, 0.2), (FRANS, 0.2), (RICAR, 0.2), (VAFFE, 0.2), (BSBEV, 0.2222222222222222), (GODOS, 0.2222222222222222), (ISLAT, 0.2222222222222222), (MAGAA, 0.2222222222222222), (OLDWO, 0.2222222222222222), (OTTIK, 0.2222222222222222), (PICCO, 0.2222222222222222), (RICSU, 0.2222222222222222), (TORTU, 0.2222222222222222), (VICTE, 0.2222222222222222), (WANDK, 0.2222222222222222), (HUNGC, 0.25), (OCEAN, 0.25), (QUEDE, 0.25), (RANCH, 0.25), (SEVES, 0.25), (SPLIR, 0.25), (VINET, 0.25), (WELLI, 0.25), (CHOPS, 0.2857142857142857), (EASTC, 0.2857142857142857), (FURIB, 0.2857142857142857), (LONEP, 0.2857142857142857), (ANTON, 0.3333333333333333), (LETSS, 0.3333333333333333), (SIMOB, 0.3333333333333333), (SPECD, 0.3333333333333333), (WOLZA, 0.3333333333333333), (DRACD, 0.4), (PERIC, 0.4), (SANTG, 0.4), (TOMSP, 0.4), (TRADH, 0.4), (COMMI, 0.5), (CONSH, 0.5), (FRANR, 0.5), (GALED, 0.5), (LAUGB, 0.5), (MORGK, 0.5), (NORTS, 0.5), (PRINI, 0.5), (ROMEY, 0.5), (THECR, 0.5), (TRAIH, 0.5), (ANATR, 0.6666666666666666), (DUMON, 0.6666666666666666), (THEBI, 0.6666666666666666), (BOLID, 1.0), (GROSR, 1.0)]</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;SQL&gt;\\nSELECT c.customer_id, AVG(julianday(o2.order_date) - julianday(o1.order_date)) as avg_time_between_orders\\nFROM customers c\\nJOIN orders o1 ON c.customer_id = o1.customer_id\\nJOIN orders o2 ON c.customer_id = o2.customer_id\\nWHERE o1.order_date &lt; o2.order_date\\nGROUP BY c.customer_id;\\n&lt;/SQL&gt;</td>\n",
       "      <td>The generated SQL query attempts to calculate the average time between orders for each customer by joining the orders table with itself and calculating the difference in days between consecutive orders for each customer. However, there are a few issues with this approach:\\n\\n1. It does not handle cases where a customer has only one order, as there would be no previous order to calculate the time difference from.\\n2. It calculates the time difference between all pairs of orders for a customer, rather than just consecutive orders. This means the average will be skewed by including non-consecutive order pairs.\\n3. The use of the julianday function is not necessary and makes the query more complex than it needs to be.\\n\\nThe groundtruth query uses a more efficient and correct approach by using the LEAD window function to get the next order date for each order, and then calculating the average difference between consecutive order dates.\\n\\nOverall, while the generated query shows an attempt to solve the problem, it has some significant flaws compared to the groundtruth query. Therefore, I would give it a low rating.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe table table-striped\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Question</th>\n",
       "      <th>Query</th>\n",
       "      <th>Result</th>\n",
       "      <th>Error</th>\n",
       "      <th>Generated_SQL_Query</th>\n",
       "      <th>reasoning</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Which products have a higher than average unit price?</td>\n",
       "      <td>SELECT product_name, unit_price FROM products WHERE unit_price &gt; (SELECT AVG(unit_price) FROM products);</td>\n",
       "      <td>[(Uncle Bob's Organic Dried Pears, 30.0), (Northwoods Cranberry Sauce, 40.0), (Mishi Kobe Niku, 97.0), (Ikura, 31.0), (Queso Manchego La Pastora, 38.0), (Alice Mutton, 39.0), (Carnarvon Tigers, 62.5), (Sir Rodney's Marmalade, 81.0), (Gumbär Gummibärchen, 31.23), (Schoggi Schokolade, 43.9), (Rössle Sauerkraut, 45.6), (Thüringer Rostbratwurst, 123.79), (Mascarpone Fabioli, 32.0), (Côte de Blaye, 263.5), (Ipoh Coffee, 46.0), (Manjimup Dried Apples, 53.0), (Perth Pasties, 32.8), (Gnocchi di nonna Alice, 38.0), (Raclette Courdavault, 55.0), (Camembert Pierrot, 34.0), (Tarte au sucre, 49.3), (Vegie-spread, 43.9), (Wimmers gute Semmelknödel, 33.25), (Gudbrandsdalsost, 36.0), (Mozzarella di Giovanni, 34.8)]</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;SQL&gt;\\nSELECT product_name, unit_price\\nFROM products\\nWHERE unit_price &gt; (\\n    SELECT AVG(unit_price)\\n    FROM products\\n);\\n&lt;/SQL&gt;</td>\n",
       "      <td>The generated SQL query is identical to the groundtruth SQL query, which means it is a perfect match in terms of syntax and semantics. It correctly retrieves the product names and unit prices for products that have a higher than average unit price.\\n\\nThe execution accuracy and exact set match accuracy metrics both have a score of 1.0, indicating that the generated query produces the correct result set. The valid efficiency score of 0.9143 suggests that the generated query is slightly less efficient than the groundtruth query, but still reasonably performant.\\n\\nGiven that the generated query is functionally equivalent to the groundtruth and produces the correct result set, it deserves a high rating according to the provided rubric.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What is the most common ship country for orders?</td>\n",
       "      <td>SELECT ship_country, COUNT(*) as shipment_count FROM orders GROUP BY ship_country ORDER BY shipment_count DESC LIMIT 1;</td>\n",
       "      <td>[(USA, 122)]</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;SQL&gt;\\nSELECT ship_country, COUNT(*) as order_count\\nFROM orders\\nGROUP BY ship_country\\nORDER BY order_count DESC\\nLIMIT 1;\\n&lt;/SQL&gt;</td>\n",
       "      <td>The generated SQL query is very similar to the groundtruth query, with only minor differences in the column alias name and the order of the clauses. Both queries achieve the same goal of finding the most common ship country for orders by grouping the orders by ship_country and counting the number of orders for each country, ordering by the count in descending order, and taking the first result.\\n\\nThe execution accuracy and exact set match accuracy metrics are both 0, which seems incorrect since the generated query produces the same result as the groundtruth. The valid efficiency score of 0 is also questionable, as the queries appear to have similar efficiency.\\n\\nOverall, the generated query is of high quality and should be able to answer the original question effectively. The only potential improvement could be to use a more descriptive column alias like 'shipment_count' instead of 'order_count'.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>What is the average unit price of products by category?</td>\n",
       "      <td>SELECT c.category_name, AVG(p.unit_price) as avg_unit_price FROM categories c JOIN products p ON c.category_id = p.category_id GROUP BY c.category_name ORDER BY avg_unit_price DESC;</td>\n",
       "      <td>[(Meat/Poultry, 54.00666666666667), (Beverages, 37.979166666666664), (Produce, 32.37), (Dairy Products, 28.73), (Confections, 25.16), (Condiments, 22.854166666666668), (Seafood, 20.6825), (Grains/Cereals, 20.25)]</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;SQL&gt;\\nSELECT categories.category_name, AVG(products.unit_price) \\nFROM categories \\nJOIN products ON categories.category_id = products.category_id \\nGROUP BY categories.category_name;\\n&lt;/SQL&gt;</td>\n",
       "      <td>The generated SQL query is able to retrieve the average unit price of products grouped by category, which correctly answers the original user question. However, there are a few differences compared to the groundtruth SQL query:\\n\\n1. The generated query does not include an alias for the AVG(products.unit_price) column, while the groundtruth query uses \"avg_unit_price\" as the alias.\\n2. The generated query does not include an ORDER BY clause to sort the results, while the groundtruth query orders the results by avg_unit_price in descending order.\\n\\nWhile the generated query is functionally correct and produces the expected result, it lacks some minor optimizations and readability improvements present in the groundtruth query. Additionally, the execution accuracy and exact set match accuracy metrics are both 0.0, indicating that the generated query does not exactly match the groundtruth query or its result set.\\n\\nConsidering these factors, I would rate the generated SQL query as:</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10. Review LLM grades\n",
    "\n",
    "# Check if df1_graded exists in the current namespace\n",
    "if 'df1_graded' not in globals():\n",
    "    # If it doesn't exist, try to load it from a JSONL file\n",
    "    file_path = 'question_query_small_llm_grades.jsonl'\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # Load the dataframe from the JSONL file\n",
    "        df1_graded = pd.read_json(file_path, lines=True)\n",
    "        print(\"df1_graded loaded from JSONL file.\")\n",
    "    else:\n",
    "        print(f\"Error: JSONL file not found at {file_path}\")\n",
    "else:\n",
    "    print(\"df1_graded already exists in memory.\")\n",
    "\n",
    "\n",
    "df1_graded['score'] = df1_graded['score'].astype('int64')\n",
    "\n",
    "# Print the data type of the 'score' column\n",
    "print(f\"Data type of 'score' column in df1_graded: {df1_graded['score'].dtype}\")\n",
    "\n",
    "percentage_correct = df1_graded['score'].value_counts(normalize=True)[5] * 100\n",
    "print(f\"Percentage correct for smaller LLM: {percentage_correct:.2f}%\")\n",
    "\n",
    "\n",
    "# sample a subsection of 3 incorrect responses of small LLM\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "incorrect_rows = df1_graded[(df1_graded['score'] == 1) | (df1_graded['score'] == 2)| (df1_graded['score'] == 3)].sample(n=3)\n",
    "incorrect_rows_no_context = incorrect_rows.drop(columns=['Context'])\n",
    "# Convert the dataframe to an HTML table\n",
    "table_html = incorrect_rows_no_context.to_html(index=False, classes='table table-striped')\n",
    "display(HTML(table_html))\n",
    "\n",
    "# sample a subsection of 3 correct responses of small LLM\n",
    "correct_rows = df1_graded[(df1_graded['score'] == 4) | (df1_graded['score'] == 5)].sample(n=3)\n",
    "correct_rows_no_context = correct_rows.drop(columns=['Context'])\n",
    "# Convert the dataframe to an HTML table\n",
    "table_html = correct_rows_no_context.to_html(index=False, classes='table table-striped')\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 11. Visualize score and label distribution\n",
    "\n",
    "visualize_distribution(df1_graded,key='score')\n",
    "\n",
    "# Let us assume that if the score is >= 4, we will route to the small LLM model (indicating the response quality is good enough); \n",
    "# otherwise, we will route to the large LLM model. Under this assumption, the data distribution looks like this\n",
    "\n",
    "df1_graded[\"routing_label\"] = df1_graded[\"score\"].apply(\n",
    "    lambda x: 1 if (x >=4) else 0\n",
    ")\n",
    "\n",
    "visualize_distribution(df1_graded, key=\"routing_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 74\n"
     ]
    }
   ],
   "source": [
    "# balance the dataset for our classification task\n",
    "balanced_train_df = balance_dataset(df1_graded, key=\"routing_label\")\n",
    "visualize_distribution(balanced_train_df, key=\"routing_label\")\n",
    "print(f\"Train size: {len(balanced_train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prompt', 'completion'], dtype='object')\n",
      "Index(['prompt', 'completion'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# sample training data and reformat to format for finetuning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_total_samples = 37\n",
    "train_ratio = 0.75  # 75% for training, 25% for validation\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "n_train = int(n_total_samples * train_ratio)\n",
    "n_val = n_total_samples - n_train\n",
    "\n",
    "# Sample the data\n",
    "sampled_df = balanced_train_df.sample(n=n_total_samples, random_state=42)\n",
    "\n",
    "# Split the sampled data into training and validation sets\n",
    "train_df, val_df = train_test_split(sampled_df, train_size=n_train, random_state=42)\n",
    "\n",
    "# Define output file names\n",
    "output_file = \"sampled_train_data.jsonl\"\n",
    "val_output_file = \"sampled_val_data.jsonl\"\n",
    "\n",
    "\n",
    "# reformat to format for finetuning\n",
    "training_data = []\n",
    "for index, row in train_df.iterrows():\n",
    "    prompt = str(build_prediction_prompt(user_question=row['Question'], sql_database_schema=row['Context']))\n",
    "    completion = str(row['routing_label'])\n",
    "    training_data.append({'prompt': prompt, 'completion': completion})\n",
    "training_df = pd.DataFrame(training_data)\n",
    "\n",
    "# Explicitly set the data types to string\n",
    "training_df['prompt'] = training_df['prompt'].astype(str)\n",
    "training_df['completion'] = training_df['completion'].astype('int64')\n",
    "\n",
    "print(training_df.columns)\n",
    "training_df.head(1)\n",
    "training_df.to_json(output_file, orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "\n",
    "val_data = []\n",
    "for index, row in val_df.iterrows():\n",
    "    prompt = str(build_prediction_prompt(user_question=row['Question'], sql_database_schema=row['Context']))\n",
    "    completion = str(row['routing_label'])\n",
    "    val_data.append({'prompt': prompt, 'completion': completion})\n",
    "validation_df = pd.DataFrame(val_data)\n",
    "\n",
    "# Explicitly set the data types to string\n",
    "validation_df['prompt'] = validation_df['prompt'].astype(str)\n",
    "validation_df['completion'] = validation_df['completion'].astype('int64')\n",
    "\n",
    "print(validation_df.columns)\n",
    "validation_df.head(1)\n",
    "validation_df.to_json(val_output_file, orient=\"records\", lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb Cell 20\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m eval_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1939\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[1;32m   1940\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1941\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[1;32m   1942\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1943\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages/transformers/trainer.py:2236\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2233\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 2236\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2237\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   2239\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m~/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages/accelerate/data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    455\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m completion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompletions[idx]\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Tokenize the prompt and completion\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     prompt,\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     completion,\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Remove the batch dimension\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/routerEval.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m item \u001b[39m=\u001b[39m {key: val\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m encoding\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3055\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3053\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3054\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3055\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[1;32m   3056\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3057\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.conda/envs/bedrock-router-eval/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3114\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3115\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3116\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3117\u001b[0m     )\n\u001b[1;32m   3119\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m-> 3120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3122\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3123\u001b[0m     )\n\u001b[1;32m   3125\u001b[0m \u001b[39mif\u001b[39;00m is_split_into_words:\n\u001b[1;32m   3126\u001b[0m     is_batched \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(text, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m text \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(text[\u001b[39m0\u001b[39m], (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# Finetune a small local LLM instruct model (DistilBERT) as binary classifier model\n",
    "# %pip install transformers torch scikit-learn transformers[torch] accelerate -U\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.prompts = dataframe['prompt'].tolist()\n",
    "        self.completions = dataframe['completion'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prompt = self.prompts[idx]\n",
    "        completion = self.completions[idx]\n",
    "        \n",
    "        # Tokenize the prompt and completion\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            completion,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Remove the batch dimension\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        \n",
    "        # item['labels'] = torch.tensor(self.labels[idx])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "# Create datasets with a DataFrame called training_df with 'prompt' and 'completion' columns\n",
    "train_dataset = CustomDataset(training_df, tokenizer)\n",
    "val_dataset = CustomDataset(validation_df, tokenizer)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Define metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")\n",
    "\n",
    "# Inference example\n",
    "def predict(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs.logits.softmax(dim=-1)\n",
    "    return probs.argmax().item()\n",
    "\n",
    "# Test the model\n",
    "test_text = \"List all suppliers with their contact information.\"\n",
    "prediction = predict(test_text)\n",
    "print(f\"Prediction for '{test_text}': {'Small_LLM' if prediction == 1 else 'Large_LLM'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINE-TUNING JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEDROCK_FINE_TUNING = False\n",
    "if BEDROCK_FINE_TUNING == True:\n",
    "    # upload to S3\n",
    "    bucket_name = 'felixh-demo'\n",
    "    prefix = 'finetuning'\n",
    "    filename = output_file\n",
    "    s3_uri = dataframe_to_s3_jsonl(training_df, bucket_name, prefix, filename)\n",
    "    print(f's3_uri: {s3_uri}')\n",
    "\n",
    "    # Set parameters\n",
    "    customizationType = \"FINE_TUNING\"\n",
    "    baseModelIdentifier = \"arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1\"\n",
    "    roleArn = \"arn:aws:iam::026459568683:role/admin\"\n",
    "    jobName = \"Text-to-SQL-Routing-Classifier-Job-V2\"\n",
    "    customModelName = \"LLM-Routing-Classifier\"\n",
    "    hyperParameters = {\n",
    "            \"epochCount\": \"1\", # The maximum number of iterations through the entire training dataset\n",
    "            \"batchSize\": \"1\", # The number of samples processed before updating model parameters\n",
    "            \"learningRate\": \".0005\", # Multiplier that influences the learning rate at which model parameters are updated after each batch\n",
    "            \"learningRateWarmupSteps\": \"0\"\n",
    "        }\n",
    "    trainingDataConfig = {\"s3Uri\": s3_uri}\n",
    "    outputDataConfig = {\"s3Uri\": f\"s3://{bucket_name}/{prefix}/output\"}\n",
    "\n",
    "    # Create job\n",
    "    response_ft = bedrock_client.create_model_customization_job(\n",
    "        jobName=jobName, \n",
    "        customModelName=customModelName,\n",
    "        roleArn=roleArn,\n",
    "        baseModelIdentifier=baseModelIdentifier,\n",
    "        hyperParameters=hyperParameters,\n",
    "        trainingDataConfig=trainingDataConfig,\n",
    "        outputDataConfig=outputDataConfig\n",
    "    )\n",
    "\n",
    "    jobArn = response_ft.get('jobArn')\n",
    "    print(f'jobArn: {jobArn}')\n",
    "\n",
    "    response = bedrock_client.get_model_customization_job(jobIdentifier=jobArn)\n",
    "    status = response.get('status')\n",
    "    if status == 'Completed':\n",
    "        outputModelArn = response.get(\"outputModelArn\")\n",
    "        print(f'outputModelArn: {outputModelArn}')\n",
    "        customModelName = \"LLM-Routing-Classifier\"\n",
    "        response_pt = bedrock_client.create_provisioned_model_throughput(\n",
    "            modelId= outputModelArn,\n",
    "            provisionedModelName= customModelName,\n",
    "            modelUnits=1\n",
    "        )\n",
    "\n",
    "        provisionedModelArn = response_pt.get('provisionedModelArn')\n",
    "        print(f'provisionedModelArn: {provisionedModelArn}')\n",
    "    else:\n",
    "        print(f'finetuning job status: {status}')\n",
    "        print(f'finetuning job response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://felixh-demo/finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'huggingface-llm-mistral-7b' with wildcard version identifier '*'. You can pin to version '2.9.1' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'peft_type': 'None', 'instruction_tuned': 'True', 'chat_dataset': 'False', 'epoch': '1', 'learning_rate': '6e-06', 'lora_r': '64', 'lora_alpha': '16', 'lora_dropout': '0', 'bits': '16', 'double_quant': 'True', 'quant_type': 'nf4', 'per_device_train_batch_size': '2', 'per_device_eval_batch_size': '8', 'add_input_output_demarcation_key': 'True', 'warmup_ratio': '0.1', 'train_from_scratch': 'False', 'fp16': 'False', 'bf16': 'True', 'evaluation_strategy': 'steps', 'eval_steps': '20', 'gradient_accumulation_steps': '8', 'logging_steps': '8', 'weight_decay': '0.2', 'load_best_model_at_end': 'True', 'max_train_samples': '-1', 'max_val_samples': '-1', 'seed': '10', 'max_input_length': '-1', 'validation_split_ratio': '0.2', 'train_data_split_seed': '0', 'preprocessing_num_workers': 'None', 'max_steps': '-1', 'gradient_checkpointing': 'True', 'early_stopping_patience': '3', 'early_stopping_threshold': '0.0', 'adam_beta1': '0.9', 'adam_beta2': '0.999', 'adam_epsilon': '1e-08', 'max_grad_norm': '1.0', 'label_smoothing_factor': '0', 'logging_first_step': 'False', 'logging_nan_inf_filter': 'True', 'save_strategy': 'steps', 'save_steps': '500', 'save_total_limit': '1', 'dataloader_drop_last': 'False', 'dataloader_num_workers': '0', 'eval_accumulation_steps': 'None', 'auto_find_batch_size': 'False', 'lr_scheduler_type': 'constant_with_warmup', 'warmup_steps': '0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: hf-llm-mistral-7b-2024-08-23-00-58-36-841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-23 00:58:38 Starting - Starting the training job\n",
      "2024-08-23 00:58:38 Pending - Training job waiting for capacity........."
     ]
    }
   ],
   "source": [
    "if BEDROCK_FINE_TUNING == False: # use SageMaker\n",
    "    from sagemaker import hyperparameters\n",
    "    from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "    # reformat to format for finetuning SageMaker\n",
    "    from sagemaker.s3 import S3Uploader\n",
    "    import sagemaker\n",
    "    import random\n",
    "    import json\n",
    "    template = {\n",
    "        \"prompt\": \"{prompt}\\n\\n\",\n",
    "        \"completion\": \" {completion}\",\n",
    "    }\n",
    "    with open(\"template.json\", \"w\") as f:\n",
    "        json.dump(template, f)\n",
    "\n",
    "    bucket_name = 'felixh-demo'\n",
    "    prefix = 'finetuning'\n",
    "    filename = output_file\n",
    "\n",
    "    train_data_location = f\"s3://{bucket_name}/{prefix}\"\n",
    "    S3Uploader.upload(output_file, train_data_location)\n",
    "    S3Uploader.upload(\"template.json\", train_data_location)\n",
    "    print(f\"Training data: {train_data_location}\")\n",
    "\n",
    "\n",
    "    model_id = 'huggingface-llm-mistral-7b' #check https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html\n",
    "    model_version = '*'\n",
    "    \n",
    "    finetuning_hyperparameters = hyperparameters.retrieve_default(\n",
    "        model_id=model_id, model_version=model_version\n",
    "    )\n",
    "    print(finetuning_hyperparameters)\n",
    "\n",
    "    finetuning_hyperparameters[\"epoch\"] = \"1\"\n",
    "    finetuning_hyperparameters[\"per_device_train_batch_size\"] = \"2\"\n",
    "    finetuning_hyperparameters[\"gradient_accumulation_steps\"] = \"2\"\n",
    "    finetuning_hyperparameters[\"instruction_tuned\"] = \"True\"\n",
    "    finetuning_hyperparameters[\"max_input_length\"] = \"32000\"\n",
    "\n",
    "    #validate parameters\n",
    "    hyperparameters.validate(\n",
    "        model_id=model_id, model_version=model_version, hyperparameters=finetuning_hyperparameters\n",
    "    )\n",
    "    \n",
    "    # start training\n",
    "    instruction_tuned_estimator = JumpStartEstimator(\n",
    "        model_id=model_id,\n",
    "        hyperparameters=finetuning_hyperparameters,\n",
    "        instance_type=\"ml.g5.12xlarge\",\n",
    "    )\n",
    "    instruction_tuned_estimator.fit({\"train\": train_data_location}, logs=True)\n",
    "\n",
    "    # get metrics\n",
    "    from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "    training_job_name = instruction_tuned_estimator.latest_training_job.job_name\n",
    "\n",
    "    df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "    df.head(10)\n",
    "\n",
    "    # deploy fine-tuned model\n",
    "    instruction_tuned_predictor = instruction_tuned_estimator.deploy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df already exists in memory.\n",
      "classification: 5\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "Error executing SQL: Execution failed on sql 'SELECT EXTRACT(YEAR FROM order_date) AS year, SUM(order_details.unit_price * order_details.quantity * (1 - order_details.discount)) AS total_sales\n",
      "FROM orders\n",
      "JOIN order_details ON orders.order_id = order_details.order_id\n",
      "GROUP BY EXTRACT(YEAR FROM order_date)\n",
      "ORDER BY year;': near \"FROM\": syntax error\n",
      "Error executing SQL: Execution failed on sql 'SELECT EXTRACT(YEAR FROM order_date) AS year, SUM(order_details.unit_price * order_details.quantity * (1 - order_details.discount)) AS total_sales\n",
      "FROM orders\n",
      "JOIN order_details ON orders.order_id = order_details.order_id\n",
      "GROUP BY EXTRACT(YEAR FROM order_date)\n",
      "ORDER BY year;': near \"FROM\": syntax error\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "Error executing SQL: Execution failed on sql 'SELECT\n",
      "    STRFTIME('%Y-%m', order_date) AS order_month,\n",
      "    SUM(order_details.unit_price * order_details.quantity * (1 - order_details.discount)) AS total_revenue\n",
      "FROM\n",
      "    orders\n",
      "    JOIN order_details ON orders.order_id = order_details.order_id\n",
      "WHERE\n",
      "    order_date >= DATE(MAX(order_date), '-1 year')\n",
      "GROUP BY\n",
      "    order_month\n",
      "ORDER BY\n",
      "    order_month;': misuse of aggregate: MAX()\n",
      "Error executing SQL: Execution failed on sql 'SELECT\n",
      "    STRFTIME('%Y-%m', order_date) AS order_month,\n",
      "    SUM(order_details.unit_price * order_details.quantity * (1 - order_details.discount)) AS total_revenue\n",
      "FROM\n",
      "    orders\n",
      "    JOIN order_details ON orders.order_id = order_details.order_id\n",
      "WHERE\n",
      "    order_date >= DATE(MAX(order_date), '-1 year')\n",
      "GROUP BY\n",
      "    order_month\n",
      "ORDER BY\n",
      "    order_month;': misuse of aggregate: MAX()\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 3\n",
      "Error executing SQL: Execution failed on sql 'SELECT c.customer_id, c.company_name, AVG(DATEDIFF(day, LAG(o.order_date, 1) OVER (PARTITION BY c.customer_id ORDER BY o.order_date), o.order_date)) AS avg_days_between_orders\n",
      "FROM customers c\n",
      "JOIN orders o ON c.customer_id = o.customer_id\n",
      "GROUP BY c.customer_id, c.company_name;': no such column: day\n",
      "Error executing SQL: Execution failed on sql 'SELECT c.customer_id, c.company_name, AVG(DATEDIFF(day, LAG(o.order_date, 1) OVER (PARTITION BY c.customer_id ORDER BY o.order_date), o.order_date)) AS avg_days_between_orders\n",
      "FROM customers c\n",
      "JOIN orders o ON c.customer_id = o.customer_id\n",
      "GROUP BY c.customer_id, c.company_name;': no such column: day\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 5\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "Error executing SQL: Execution failed on sql 'SELECT EXTRACT(YEAR FROM order_date) AS year, SUM(freight) AS total_freight_cost\n",
      "FROM orders\n",
      "GROUP BY EXTRACT(YEAR FROM order_date)\n",
      "ORDER BY year;': near \"FROM\": syntax error\n",
      "Error executing SQL: Execution failed on sql 'SELECT EXTRACT(YEAR FROM order_date) AS year, SUM(freight) AS total_freight_cost\n",
      "FROM orders\n",
      "GROUP BY EXTRACT(YEAR FROM order_date)\n",
      "ORDER BY year;': near \"FROM\": syntax error\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 4\n",
      "Error executing SQL: Execution failed on sql 'SELECT ROUND(SUM(quantity) * 1.0 / COUNT(DISTINCT order_id), 2) AS avg_items_per_order\n",
      "FROM order_details\n",
      "JOIN orders ON order_details.order_id = orders.order_id;': ambiguous column name: order_id\n",
      "Error executing SQL: Execution failed on sql 'SELECT ROUND(SUM(quantity) * 1.0 / COUNT(DISTINCT order_id), 2) AS avg_items_per_order\n",
      "FROM order_details\n",
      "JOIN orders ON order_details.order_id = orders.order_id;': ambiguous column name: order_id\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "Error executing SQL: Execution failed on sql 'SELECT DATE_PART('month', order_date) AS month, COUNT(*) AS order_count\n",
      "FROM orders\n",
      "WHERE DATE_PART('year', order_date) = 1997\n",
      "GROUP BY DATE_PART('month', order_date)\n",
      "ORDER BY month;': no such function: DATE_PART\n",
      "Error executing SQL: Execution failed on sql 'SELECT DATE_PART('month', order_date) AS month, COUNT(*) AS order_count\n",
      "FROM orders\n",
      "WHERE DATE_PART('year', order_date) = 1997\n",
      "GROUP BY DATE_PART('month', order_date)\n",
      "ORDER BY month;': no such function: DATE_PART\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 5\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "Error executing SQL: Execution failed on sql 'SELECT AVG(CAST(AGE(hire_date, birth_date) AS DECIMAL(10,2))) AS average_age_at_hiring\n",
      "FROM employees;': no such function: AGE\n",
      "Error executing SQL: Execution failed on sql 'SELECT AVG(CAST(AGE(hire_date, birth_date) AS DECIMAL(10,2))) AS average_age_at_hiring\n",
      "FROM employees;': no such function: AGE\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "Error executing SQL: Execution failed on sql 'SELECT \n",
      "    EXTRACT(YEAR FROM order_date) AS year,\n",
      "    ship_country,\n",
      "    COUNT(*) AS order_count,\n",
      "    RANK() OVER (PARTITION BY EXTRACT(YEAR FROM order_date) ORDER BY COUNT(*) DESC) AS rank\n",
      "FROM orders\n",
      "WHERE ship_country IS NOT NULL\n",
      "GROUP BY EXTRACT(YEAR FROM order_date), ship_country\n",
      "ORDER BY year, rank;': near \"FROM\": syntax error\n",
      "Error executing SQL: Execution failed on sql 'SELECT \n",
      "    EXTRACT(YEAR FROM order_date) AS year,\n",
      "    ship_country,\n",
      "    COUNT(*) AS order_count,\n",
      "    RANK() OVER (PARTITION BY EXTRACT(YEAR FROM order_date) ORDER BY COUNT(*) DESC) AS rank\n",
      "FROM orders\n",
      "WHERE ship_country IS NOT NULL\n",
      "GROUP BY EXTRACT(YEAR FROM order_date), ship_country\n",
      "ORDER BY year, rank;': near \"FROM\": syntax error\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 4\n",
      "Error executing SQL: Execution failed on sql 'SELECT EXTRACT(MONTH FROM order_date) AS month, SUM(freight) AS total_freight\n",
      "FROM orders\n",
      "WHERE EXTRACT(YEAR FROM order_date) = 1997\n",
      "GROUP BY EXTRACT(MONTH FROM order_date)\n",
      "ORDER BY month;': near \"FROM\": syntax error\n",
      "Error executing SQL: Execution failed on sql 'SELECT EXTRACT(MONTH FROM order_date) AS month, SUM(freight) AS total_freight\n",
      "FROM orders\n",
      "WHERE EXTRACT(YEAR FROM order_date) = 1997\n",
      "GROUP BY EXTRACT(MONTH FROM order_date)\n",
      "ORDER BY month;': near \"FROM\": syntax error\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "Error executing SQL: Execution failed on sql 'WITH order_dates AS (\n",
      "  SELECT customer_id, order_date, \n",
      "    LEAD(order_date) OVER (PARTITION BY customer_id ORDER BY order_date) AS next_order_date\n",
      "  FROM orders\n",
      ")\n",
      "SELECT customer_id, AVG(DATEDIFF(next_order_date, order_date)) AS avg_days_between_orders\n",
      "FROM order_dates\n",
      "WHERE next_order_date IS NOT NULL\n",
      "GROUP BY customer_id;': no such function: DATEDIFF\n",
      "Error executing SQL: Execution failed on sql 'WITH order_dates AS (\n",
      "  SELECT customer_id, order_date, \n",
      "    LEAD(order_date) OVER (PARTITION BY customer_id ORDER BY order_date) AS next_order_date\n",
      "  FROM orders\n",
      ")\n",
      "SELECT customer_id, AVG(DATEDIFF(next_order_date, order_date)) AS avg_days_between_orders\n",
      "FROM order_dates\n",
      "WHERE next_order_date IS NOT NULL\n",
      "GROUP BY customer_id;': no such function: DATEDIFF\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 3\n",
      "Error executing SQL: Execution failed on sql 'SELECT p.product_name\n",
      "FROM products p\n",
      "INNER JOIN order_details od ON p.product_id = od.product_id\n",
      "INNER JOIN orders o ON od.order_id = o.order_id\n",
      "WHERE o.order_date BETWEEN '2022-01-01' AND '2022-12-31'\n",
      "GROUP BY p.product_name\n",
      "HAVING COUNT(DISTINCT EXTRACT(MONTH FROM o.order_date)) = 12;': near \"FROM\": syntax error\n",
      "Error executing SQL: Execution failed on sql 'SELECT p.product_name\n",
      "FROM products p\n",
      "INNER JOIN order_details od ON p.product_id = od.product_id\n",
      "INNER JOIN orders o ON od.order_id = o.order_id\n",
      "WHERE o.order_date BETWEEN '2022-01-01' AND '2022-12-31'\n",
      "GROUP BY p.product_name\n",
      "HAVING COUNT(DISTINCT EXTRACT(MONTH FROM o.order_date)) = 12;': near \"FROM\": syntax error\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "Error executing SQL: Execution failed on sql 'SELECT c.customer_id\n",
      "FROM customers c\n",
      "INNER JOIN orders o1 ON c.customer_id = o1.customer_id\n",
      "INNER JOIN orders o2 ON c.customer_id = o2.customer_id\n",
      "WHERE EXTRACT(MONTH FROM o1.order_date) = EXTRACT(MONTH FROM o2.order_date) - 1\n",
      "  AND EXTRACT(YEAR FROM o1.order_date) = EXTRACT(YEAR FROM o2.order_date)\n",
      "GROUP BY c.customer_id;': near \"FROM\": syntax error\n",
      "Error executing SQL: Execution failed on sql 'SELECT c.customer_id\n",
      "FROM customers c\n",
      "INNER JOIN orders o1 ON c.customer_id = o1.customer_id\n",
      "INNER JOIN orders o2 ON c.customer_id = o2.customer_id\n",
      "WHERE EXTRACT(MONTH FROM o1.order_date) = EXTRACT(MONTH FROM o2.order_date) - 1\n",
      "  AND EXTRACT(YEAR FROM o1.order_date) = EXTRACT(YEAR FROM o2.order_date)\n",
      "GROUP BY c.customer_id;': near \"FROM\": syntax error\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 4\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 4\n",
      "Error executing SQL: Execution failed on sql 'SELECT DATE_FORMAT(order_date, '%Y-%m') AS month_year, SUM(orders.freight) AS total_order_value\n",
      "FROM orders\n",
      "GROUP BY month_year\n",
      "ORDER BY month_year;': no such function: DATE_FORMAT\n",
      "Error executing SQL: Execution failed on sql 'SELECT DATE_FORMAT(order_date, '%Y-%m') AS month_year, SUM(orders.freight) AS total_order_value\n",
      "FROM orders\n",
      "GROUP BY month_year\n",
      "ORDER BY month_year;': no such function: DATE_FORMAT\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 3\n",
      "classification: 3\n",
      "Error executing SQL: Execution failed on sql 'SELECT p.product_name\n",
      "FROM products p\n",
      "INNER JOIN order_details od ON p.product_id = od.product_id\n",
      "INNER JOIN orders o ON od.order_id = o.order_id\n",
      "WHERE o.order_date BETWEEN '2022-01-01' AND '2022-12-31'\n",
      "GROUP BY p.product_name\n",
      "HAVING COUNT(DISTINCT EXTRACT(QUARTER FROM o.order_date)) = 4;': near \"FROM\": syntax error\n",
      "Error executing SQL: Execution failed on sql 'SELECT p.product_name\n",
      "FROM products p\n",
      "INNER JOIN order_details od ON p.product_id = od.product_id\n",
      "INNER JOIN orders o ON od.order_id = o.order_id\n",
      "WHERE o.order_date BETWEEN '2022-01-01' AND '2022-12-31'\n",
      "GROUP BY p.product_name\n",
      "HAVING COUNT(DISTINCT EXTRACT(QUARTER FROM o.order_date)) = 4;': near \"FROM\": syntax error\n",
      "classification: 4\n"
     ]
    }
   ],
   "source": [
    "# Evaluate routing from accuracy,latency, and cost perspective\n",
    "\n",
    "# Check if df exists in the current namespace\n",
    "if 'df' not in globals():\n",
    "    # If it doesn't exist, try to load it from a JSONL file\n",
    "    file_path = 'question_query_good_results.jsonl'\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        # Load the dataframe from the JSONL file\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        print(\"df loaded from JSONL file.\")\n",
    "    else:\n",
    "        print(f\"Error: JSONL file not found at {file_path}\")\n",
    "else:\n",
    "    print(\"df already exists in memory.\")\n",
    "\n",
    "if SQL_DATABASE == 'LOCAL':\n",
    "    # Create a SQLite database connection\n",
    "    conn = sqlite3.connect('routedb.db')\n",
    "\n",
    "\n",
    "# for each row in ground truth data\n",
    "results = []\n",
    "df.columns = df.columns.str.capitalize()\n",
    "\n",
    "LARGE_LLM_MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "wrapper = BedrockLLMWrapper(debug=False, model_id=LARGE_LLM_MODEL_ID, max_token_count=512)\n",
    "routeLLM = RouteLLMWrapper()\n",
    "util = Util()\n",
    "for row in df.itertuples():\n",
    "\n",
    "    # invoke LLM pipeline with classifier\n",
    "    classification_result, routeLLMresult = routeLLM.generate(user_question= row.Question, sql_database_schema= schema)\n",
    "    small_generated_sql = extract_with_regex(routeLLMresult[0], SQL_PATTERN).replace(\"\\\\\",\"\") # converse API workaround\n",
    "    small_query_time = classification_result[2] + routeLLMresult[2]\n",
    "    small_cost = util.calculate_cost(classification_result[1], routeLLM.small_llm_model_id) + (util.calculate_cost(routeLLMresult[1], routeLLM.small_llm_model_id) if classification_result[0] == '1' else util.calculate_cost(routeLLMresult[1], routeLLM.large_llm_model_id))\n",
    "\n",
    "    \n",
    "    #invoke large LLM directly\n",
    "    prompt = build_sqlquerygen_prompt(user_question= row.Question, sql_database_schema= schema)\n",
    "    large_llm_result = wrapper.generate(prompt) # invoke large LLM\n",
    "    #large_generated_sql, usage, query_time\n",
    "    large_generated_sql = extract_with_regex(large_llm_result[0], SQL_PATTERN).replace(\"\\\\\",\"\")\n",
    "    large_query_time = large_llm_result[2]\n",
    "    large_cost = util.calculate_cost(large_llm_result[1], wrapper.model_id)\n",
    "    \n",
    "    # Calculate Text-to-SQL metrics\n",
    "    ex_score = execution_accuracy(small_generated_sql, large_generated_sql)\n",
    "    em_score = exact_set_match_accuracy(small_generated_sql, large_generated_sql, conn)\n",
    "    ves_score = valid_efficiency_score(small_generated_sql, large_generated_sql, conn)\n",
    "\n",
    "    result = {\"small_generated_sql\": small_generated_sql, \n",
    "              \"small_cost\": small_cost, \n",
    "              \"small_query_time\": small_query_time, \n",
    "              \"large_generated_sql\": large_generated_sql, \n",
    "              \"large_cost\": large_cost , \n",
    "              \"large_query_time\": large_query_time,\n",
    "              \"ex_score\": ex_score,\n",
    "              \"em_score\": em_score,\n",
    "              \"ves_score\": ves_score}\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "evaluation_results = pd.DataFrame(results)\n",
    "\n",
    "print(f'sum of small_cost: {evaluation_results[small_cost].sum()}')\n",
    "print(f'sum of large_cost: {evaluation_results[large_cost].sum()}')\n",
    "\n",
    "print(f'avg of small_query_time: {evaluation_results[small_query_time].avg()}')\n",
    "print(f'avg of large_query_time: {evaluation_results[large_query_time].avg()}')\n",
    "\n",
    "print(f'avg of ex_score: {evaluation_results[ex_score].avg()}')\n",
    "print(f'avg of em_score: {evaluation_results[em_score].avg()}')\n",
    "print(f'avg of ves_score: {evaluation_results[ves_score].avg()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this tutorial, we have successfully built and evaluated a finetuned-LLM router. \n",
    "We generated synthetic labeled data using the LLM-as-a-judge method to train the model, finetuned an LLM classifier using Amazon Bedrock's API, \n",
    "and conducted offline evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "https://github.com/lm-sys/RouteLLM\n",
    "\n",
    "https://medium.com/@learngrowthrive.fast/routellm-achieves-90-gpt-4-quality-at-80-lower-cost-6686e5f46e2a\n",
    "\n",
    "https://medium.com/ai-insights-cobet/beyond-basic-chatbots-how-semantic-router-is-changing-the-game-783dd959a32d\n",
    "\n",
    "https://medium.com/@bhawana.prs/semantic-routes-in-llms-to-make-chatbots-more-accurate-d99c17e30487\n",
    "\n",
    "\n",
    "popular benchmarks: MT Bench, MMLU, and GSM8K.\n",
    "\n",
    "* Semantic routing: Using a vector analysis to route the query to the closest “cluster”\n",
    "https://github.com/aurelio-labs/semantic-router\n",
    "\n",
    "* Prompt Chaining: Similar to what has been implemented inside Bedrock agents, and LangChain’s Custom function, these use an small LLM to analyze the question and route it to the next part of the chain. https://aws.amazon.com/blogs/machine-learning/enhance-conversational-ai-with-advanced-routing-techniques-with-amazon-bedrock/\n",
    "You can optimize this by having the “router” model answer directly simple questions instead of routing them to another model.\n",
    "\n",
    "* Intent Classification: Creating a custom model, similar to ROHF or Rerankers to classify the query and route it to the right LLM.  \n",
    "https://medium.com/aimonks/intent-classification-generative-ai-based-application-architecture-3-79d2927537b4\n",
    "\n",
    "https://www.anyscale.com/blog/building-an-llm-router-for-high-quality-and-cost-effective-responses\n",
    "\n",
    "https://github.com/aws-samples/amazon-bedrock-samples/blob/main/function-calling/function_calling_text2SQL_converse_bedrock_streamlit.py\n",
    "\n",
    "https://github.com/aws-samples/amazon-bedrock-samples/tree/main/rag-solutions/sql-query-generator\n",
    "\n",
    "### Next steps\n",
    "\n",
    "Explore other data sources:  https://bird-bench.github.io/ , latest spyder dataset, any SQL dataset from HF\n",
    "\n",
    "<!-- # https://huggingface.co/datasets/b-mc2/sql-create-context\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# # %sql sqlite:///routedb.db\n",
    "# # to load SQL dataset from starcoder\n",
    "## ds = load_dataset(\"bigcode/starcoderdata\", data_dir=\"sql\", split=\"train\", token=True)\n",
    "# ds = load_dataset(\"b-mc2/sql-create-context\", split=\"train\", token=True)\n",
    "# from datasets import load_dataset_builder\n",
    "# ds_builder = load_dataset_builder(\"b-mc2/sql-create-context\")\n",
    "# ds_builder.info.description\n",
    "# ds_builder.info.features -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRATCHPAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  A \"Hello World\" program is a simple program that outputs the text \"Hello, World!\" and its purpose is to introduce and familiarize beginners with the basic syntax and structure of a programming language.\n",
      "token_usage: {'inputTokens': 22, 'outputTokens': 42, 'totalTokens': 64}\n",
      "latency: 1328\n"
     ]
    }
   ],
   "source": [
    "# mistral.mixtral-8x7b-instruct-v0:1\n",
    "\n",
    "# Use the native inference API to send a text message to Anthropic Claude.\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region of your choice.\n",
    "bedrock_runtime_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# Set the model ID, e.g., Claude 3 Haiku.\n",
    "model_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "\n",
    "# Define the prompt for the model.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Setup the system prompts and messages to send to the model.\n",
    "system_prompts = [] # [{\"text\": \"You are a helpful AI Assistant.\"}]\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": prompt}]\n",
    "}\n",
    "\n",
    "messages = []\n",
    "messages.append(message)\n",
    "\n",
    "try:\n",
    "    # Base inference parameters to use.\n",
    "    inference_config = {\"temperature\": 0.0}\n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {\"top_k\": 5}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_runtime_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    # Log token usage.\n",
    "    text = response['output'].get('message').get('content')[0].get('text')\n",
    "    print(f'text: {text}')\n",
    "    token_usage = response['usage']\n",
    "    print(f'token_usage: {token_usage}')\n",
    "    latency = response['metrics'].get('latencyMs')\n",
    "    print(f'latency: {latency}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error with calling Bedrock: \"+str(e))\n",
    "    attempt+=1\n",
    "    if attempt>3:\n",
    "        print(\"Max attempts reached!\")\n",
    "        result_text = str(e)\n",
    "        \n",
    "    else:#retry in 10 seconds\n",
    "        print(\"retry\")\n",
    "        time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock-router-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
