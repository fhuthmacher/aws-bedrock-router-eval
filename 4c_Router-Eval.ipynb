{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Bedrock LLM Router Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro and Goal\n",
    "This Jupyter Notebook is designed to fine-tune an LLM (Large Language Model) routing system on a Text-to-SQL use case.\n",
    "\n",
    "The goal is to take a prompt, determine the level of complexity and then route the prompt either to a small or large LLM to generate the corresponding SQL query.\n",
    "\n",
    "*WIP*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a conda environment\n",
    "\n",
    "# !conda create -y --name bedrock-router-eval python=3.11.8\n",
    "# !conda init && activate bedrock-router-eval\n",
    "# !conda install -n bedrock-router-eval ipykernel --update-deps --force-reinstall -y\n",
    "# !conda install -c conda-forge ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install dependencies\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Import necessary libraries and load environment variables\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import json\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import boto3\n",
    "import sqlite3\n",
    "from pandas.io import sql\n",
    "from botocore.config import Config\n",
    "import pandas as pd\n",
    "import io\n",
    "import json\n",
    "from io import StringIO\n",
    "import sqlparse\n",
    "import sqlite3\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import typing as t\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'bedrock-router-eval.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "os.environ['SQL_DATABASE'] = os.getenv('SQL_DATABASE') # LOCAL or GLUE\n",
    "os.environ['SQL_DIALECT'] = os.getenv('SQL_DIALECT') # SQlite or awsathena\n",
    "os.environ['SQL_DATABASE_NAME'] = os.getenv('SQL_DATABASE_NAME')\n",
    "# os.environ['AWS_ACCESS_KEY'] = os.getenv('AWS_ACCESS_KEY')\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY'] = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "REGION = os.environ['REGION']\n",
    "HF_TOKEN = os.environ['HF_TOKEN']\n",
    "SQL_DATABASE = os.environ['SQL_DATABASE']\n",
    "SQL_DIALECT = os.environ['SQL_DIALECT']\n",
    "SQL_DATABASE_NAME = os.environ['SQL_DATABASE_NAME']\n",
    "\n",
    "MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\" # anthropic.claude-3-haiku-20240307-v1:0 \"anthropic.claude-3-5-sonnet-20240620-v1:0\" \"meta.llama3-1-70b-instruct-v1:0\"\n",
    "\n",
    "# get ground truth data\n",
    "file_path = 'question_query_good_results.jsonl'\n",
    "groundtruth_df = pd.read_json(file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4. Define Helper functions\n",
    "\n",
    "# def balance_dataset(\n",
    "#     dataset_df: pd.DataFrame, key: str, random_state: int = 42\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Balance the dataset by oversampling the minority class.\n",
    "#     \"\"\"\n",
    "#     # Determine the minority class\n",
    "#     min_count = dataset_df[key].value_counts().min()\n",
    "\n",
    "#     # Create a balanced DataFrame\n",
    "#     sampled_dfs = []\n",
    "#     for label in dataset_df[key].unique():\n",
    "#         sampled = dataset_df[dataset_df[key] == label].sample(\n",
    "#             n=min_count, random_state=random_state\n",
    "#         )\n",
    "#         sampled_dfs.append(sampled)\n",
    "\n",
    "#     balanced_df = pd.concat(sampled_dfs).sample(frac=1, random_state=random_state)\n",
    "#     return balanced_df\n",
    "    \n",
    "# def visualize_distribution(df, key):\n",
    "#     # Check if 'score' column exists in the DataFrame\n",
    "#     if key not in df.columns:\n",
    "#         raise ValueError(f\"The DataFrame does not contain a '{key}' column.\")\n",
    "    \n",
    "#     # Count the frequency of each score\n",
    "#     score_counts = df[key].value_counts().sort_index()\n",
    "    \n",
    "#     # Create a bar chart\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.bar(score_counts.index, score_counts.values)\n",
    "    \n",
    "#     # Customize the chart\n",
    "#     plt.title(f'Distribution of {key}')\n",
    "#     plt.xlabel(f'{key}')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.xticks(range(int(score_counts.index.min()), int(score_counts.index.max()) + 1))\n",
    "    \n",
    "#     # Add value labels on top of each bar\n",
    "#     for i, v in enumerate(score_counts.values):\n",
    "#         plt.text(score_counts.index[i], v, str(v), ha='center', va='bottom')\n",
    "    \n",
    "#     # Display the chart\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# def execution_accuracy(generated_sql, labeled_sql):\n",
    "#     \"\"\"\n",
    "#     Calculate Execution Accuracy (EX)\n",
    "    \n",
    "#     Args:\n",
    "#     generated_sql (str): The SQL query generated by the model\n",
    "#     labeled_sql (str): The labeled (ground truth) SQL query\n",
    "    \n",
    "#     Returns:\n",
    "#     float: 1.0 if the queries match, 0.0 otherwise\n",
    "#     \"\"\"\n",
    "#     # Normalize and compare the SQL queries\n",
    "#     gen_normalized = sqlparse.format(generated_sql, strip_comments=True, reindent=True)\n",
    "#     lab_normalized = sqlparse.format(labeled_sql, strip_comments=True, reindent=True)\n",
    "    \n",
    "#     return 1.0 if gen_normalized == lab_normalized else 0.0\n",
    "\n",
    "# def exact_set_match_accuracy(generated_sql, labeled_sql, db_connection):\n",
    "#     \"\"\"\n",
    "#     Calculate Exact Set Match Accuracy (EM)\n",
    "    \n",
    "#     Args:\n",
    "#     generated_sql (str): The SQL query generated by the model\n",
    "#     labeled_sql (str): The labeled (ground truth) SQL query\n",
    "#     db_connection: A database connection object\n",
    "    \n",
    "#     Returns:\n",
    "#     float: 1.0 if the result sets match, 0.0 otherwise\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Execute both queries\n",
    "#         gen_result = pd.read_sql_query(generated_sql, db_connection)\n",
    "#         lab_result = pd.read_sql_query(labeled_sql, db_connection)\n",
    "        \n",
    "#         # Compare the result sets\n",
    "#         return 1.0 if gen_result.equals(lab_result) else 0.0\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing SQL: {e}\")\n",
    "#         return 0.0\n",
    "\n",
    "# def valid_efficiency_score(generated_sql, labeled_sql, db_connection):\n",
    "#     \"\"\"\n",
    "#     Calculate Valid Efficiency Score (VES)\n",
    "    \n",
    "#     Args:\n",
    "#     generated_sql (str): The SQL query generated by the model\n",
    "#     labeled_sql (str): The labeled (ground truth) SQL query\n",
    "#     db_connection: A database connection object\n",
    "    \n",
    "#     Returns:\n",
    "#     float: The VES score\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Execute both queries and measure execution time\n",
    "#         gen_start = time.time()\n",
    "#         gen_result = pd.read_sql_query(generated_sql, db_connection)\n",
    "#         gen_time = time.time() - gen_start\n",
    "#         # print(f'generated_sql_execution_time: {gen_time}')\n",
    "#         lab_start = time.time()\n",
    "#         lab_result = pd.read_sql_query(labeled_sql, db_connection)\n",
    "#         lab_time = time.time() - lab_start\n",
    "#         # print(f'labeled_sql_execution_time: {lab_time}')\n",
    "        \n",
    "#         # Check if results match\n",
    "#         if not gen_result.equals(lab_result):\n",
    "#             return 0.0\n",
    "        \n",
    "#         # Calculate VES\n",
    "#         ves = min(lab_time / gen_time, 1.0)\n",
    "#         return ves\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing SQL: {e}\")\n",
    "#         return 0.0\n",
    "\n",
    "\n",
    "# def dataframe_to_s3_jsonl(df, bucket_name, prefix, filename):\n",
    "#     \"\"\"\n",
    "#     Convert a pandas DataFrame to JSONL format and upload it to S3.\n",
    "\n",
    "#     Parameters:\n",
    "#     df (pandas.DataFrame): The DataFrame to be converted and uploaded.\n",
    "#     bucket_name (str): The name of the S3 bucket.\n",
    "#     prefix (str): The S3 prefix (folder path) where the file will be uploaded.\n",
    "#     filename (str): The name of the file to be created in S3.\n",
    "\n",
    "#     Returns:\n",
    "#     str: The S3 URI of the uploaded file.\n",
    "#     \"\"\"\n",
    "#     # Convert DataFrame to JSONL\n",
    "#     jsonl_buffer = StringIO()\n",
    "#     for _, row in df.iterrows():\n",
    "#         json.dump(row.to_dict(), jsonl_buffer)\n",
    "#         jsonl_buffer.write('\\n')\n",
    "#     jsonl_buffer.seek(0)\n",
    "#     s3_client = boto3.client('s3')\n",
    "#     # Upload the JSONL data to S3\n",
    "#     s3_key = f\"{prefix.rstrip('/')}/{filename}\"\n",
    "#     s3_client.put_object(\n",
    "#         Bucket=bucket_name,\n",
    "#         Key=s3_key,\n",
    "#         Body=jsonl_buffer.getvalue(),\n",
    "#         ContentType='application/json'\n",
    "#     )\n",
    "\n",
    "#     # Return the S3 URI of the uploaded file\n",
    "#     return f\"s3://{bucket_name}/{s3_key}\"\n",
    "\n",
    "\n",
    "# def download_and_parse_jsonl(bucket_name, object_key):\n",
    "#     \"\"\"\n",
    "#     Downloads a JSONL file from an Amazon S3 bucket and parses it into a pandas DataFrame.\n",
    "\n",
    "#     Args:\n",
    "#         bucket_name (str): The name of the S3 bucket where the JSONL file is stored.\n",
    "#         object_key (str): The key (path) of the JSONL file in the S3 bucket.\n",
    "\n",
    "#     Returns:\n",
    "#         pandas.DataFrame: A DataFrame containing the data from the JSONL file.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     s3_client = boto3.client('s3')\n",
    "#     # Download the JSONL file from S3\n",
    "#     response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "#     jsonl_data = response['Body'].read().decode('utf-8')\n",
    "\n",
    "#     # Parse the JSONL data into a list of dictionaries\n",
    "#     data = [json.loads(line) for line in jsonl_data.strip().split('\\n')]\n",
    "\n",
    "#     # Create a DataFrame from the list of dictionaries\n",
    "#     df = pd.DataFrame(data)\n",
    "\n",
    "#     return df\n",
    "\n",
    "# def check_job_status_and_wait(job_arn):\n",
    "#     # # check status\n",
    "#     # bedrock.get_model_invocation_job(jobIdentifier=jobArn)['status']\n",
    "\n",
    "#     # # list batch jobs\n",
    "#     # bedrock.list_model_invocation_jobs(\n",
    "#     #     maxResults=10,\n",
    "#     #     statusEquals=\"Failed\",\n",
    "#     #     sortOrder=\"Descending\"\n",
    "#     # )\n",
    "\n",
    "#     while True:\n",
    "#         job_status = bedrock_client.get_model_invocation_job(jobIdentifier=job_arn)['status']\n",
    "#         print(f\"Job status: {job_status}\")\n",
    "\n",
    "#         if job_status == 'COMPLETED':\n",
    "#             output_s3_uri = bedrock_client.get_model_invocation_job(jobIdentifier=job_arn)['outputDataConfig']['s3OutputDataConfig']['s3Uri']\n",
    "#             output_file_key = output_s3_uri.replace(f\"s3://{output_bucket}/{output_prefix}\", \"\")\n",
    "#             output_file_name = output_file_key.split(\"/\")[-1]\n",
    "#             break\n",
    "#         elif job_status == 'FAILED':\n",
    "#             print(\"Job failed.\")\n",
    "#             break\n",
    "#         else:\n",
    "#             time.sleep(60)  # Wait for 1 minute before checking again\n",
    "    \n",
    "#     return output_s3_uri\n",
    "\n",
    "# def get_schema(database_name, table_names=None):\n",
    "#     try:\n",
    "#         glue_client = boto3.client('glue', region_name=REGION)\n",
    "#         table_schema_list = []\n",
    "#         response = glue_client.get_tables(DatabaseName=database_name)\n",
    "\n",
    "#         all_table_names = [table['Name'] for table in response['TableList']]\n",
    "\n",
    "#         if table_names:\n",
    "#             table_names = [name for name in table_names if name in all_table_names]\n",
    "#         else:\n",
    "#             table_names = all_table_names\n",
    "\n",
    "#         for table_name in table_names:\n",
    "#             response = glue_client.get_table(DatabaseName=database_name, Name=table_name)\n",
    "#             columns = response['Table']['StorageDescriptor']['Columns']\n",
    "#             schema = {column['Name']: column['Type'] for column in columns}\n",
    "#             table_schema_list.append({\"Table: {}\".format(table_name): 'Schema: {}'.format(schema)})\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {str(e)}\")\n",
    "#     return table_schema_list\n",
    "\n",
    "# def execute_athena_query(database, query):\n",
    "#     athena_client = boto3.client('athena', region_name=REGION)\n",
    "#     # Start query execution\n",
    "#     response = athena_client.start_query_execution(\n",
    "#         QueryString=query,\n",
    "#         QueryExecutionContext={\n",
    "#             'Database': database\n",
    "#         },\n",
    "#         ResultConfiguration={\n",
    "#             'OutputLocation': outputLocation\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     # Get query execution ID\n",
    "#     query_execution_id = response['QueryExecutionId']\n",
    "#     print(f\"Query Execution ID: {query_execution_id}\")\n",
    "\n",
    "#     # Wait for the query to complete\n",
    "#     response_wait = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "\n",
    "#     while response_wait['QueryExecution']['Status']['State'] in ['QUEUED', 'RUNNING']:\n",
    "#         print(\"Query is still running...\")\n",
    "#         response_wait = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "\n",
    "#     print(f'response_wait {response_wait}')\n",
    "\n",
    "#     # Check if the query completed successfully\n",
    "#     if response_wait['QueryExecution']['Status']['State'] == 'SUCCEEDED':\n",
    "#         print(\"Query succeeded!\")\n",
    "\n",
    "#         # Get query results\n",
    "#         query_results = athena_client.get_query_results(QueryExecutionId=query_execution_id)\n",
    "\n",
    "#         # Extract and return the result data\n",
    "#         code = 'SUCCEEDED'\n",
    "#         return code, extract_result_data(query_results)\n",
    "\n",
    "#     else:\n",
    "#         print(\"Query failed!\")\n",
    "#         code = response_wait['QueryExecution']['Status']['State']\n",
    "#         message = response_wait['QueryExecution']['Status']['StateChangeReason']\n",
    "    \n",
    "#         return code, message\n",
    "\n",
    "# def extract_result_data(query_results):\n",
    "#     #Return a cleaned response to the agent\n",
    "#     result_data = []\n",
    "\n",
    "#     # Extract column names\n",
    "#     column_info = query_results['ResultSet']['ResultSetMetadata']['ColumnInfo']\n",
    "#     column_names = [column['Name'] for column in column_info]\n",
    "\n",
    "#     # Extract data rows\n",
    "#     for row in query_results['ResultSet']['Rows']:\n",
    "#         data = [item['VarCharValue'] for item in row['Data']]\n",
    "#         result_data.append(dict(zip(column_names, data)))\n",
    "\n",
    "#     return result_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL generation prompt\n",
    "\n",
    "sql_template = \"\"\"You are a SQL expert. You will be provided with the original user question and a SQL database schema. \n",
    "                Only return the SQL query and nothing else.\n",
    "                Here is the original user question.\n",
    "                <user_question>\n",
    "                {user_question}\n",
    "                </user_question>\n",
    "\n",
    "                Here is the SQL database schema.\n",
    "                <sql_database_schema>\n",
    "                {sql_database_schema}\n",
    "                </sql_database_schema>\n",
    "                \n",
    "                Instructions:\n",
    "                Generate a SQL query that answers the original user question.\n",
    "                Use the schema, first create a syntactically correct {sql_dialect} query to answer the question. \n",
    "                Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\n",
    "                Pay attention to use only the column names that you can see in the schema description. \n",
    "                Be careful to not query for columns that do not exist. \n",
    "                Pay attention to which column is in which table. \n",
    "                Also, qualify column names with the table name when needed.\n",
    "                If you cannot answer the user question with the help of the provided SQL database schema, \n",
    "                then output that this question question cannot be answered based of the information stored in the database.\n",
    "                You are required to use the following format, each taking one line:\n",
    "                Return the sql query inside the <SQL></SQL> tab.\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# router template\n",
    "route_prompt_template=\"\"\"                       \n",
    "                        Instructions:\n",
    "                        1. Give this question a difficulty rating from 1 to 3, where 3 is the most difficult and 1 is the easiest.\n",
    "                        2. Return the difficulty inside <difficulty></difficulty> tags. \n",
    "                        3. If the score is 1, then generate the SQL query to answer the question.\n",
    "                        4. Return the generated SQL qery inside <SQL></SQL> tags.\n",
    "                        5. Review your formatted response. It needs to be valid XML.\n",
    "                        \n",
    "                        Question:\n",
    "                        <user_question>\n",
    "                        {user_question}\n",
    "                        </user_question>\n",
    "\n",
    "                        SQL database schema for the SQL dialect {sql_dialect}:\n",
    "                        <sql_database_schema>\n",
    "                        {sql_database_schema}\n",
    "                        </sql_database_schema>\n",
    "                        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grading prompt\n",
    "\n",
    "evaluation_template = \"\"\"You are a SQL expert. \n",
    "                Your task is to evaluate a given SQL query based on a provided SQL schema and question using the criteria provided below.\n",
    " \n",
    "                Evaluation Criteria (Additive Score, 0-5):\n",
    "                1. Context: Award 1 point if the generated SQL query uses only information provided in the SQL schema, without introducing external or fabricated details.\n",
    "                2. Completeness: Add 1 point if the generated SQL query addresses all key elements of the question based on the available SQL schema and Exact Set Match Accuracy (EM) score.\n",
    "                3. ExecutionAccuracy: Add 1 point if the generated SQL query is very close to the groundtruth answer based on Execution Accuracy score.\n",
    "                4. Faultless: Add 1 point if the generated SQL query ran without any errors.\n",
    "                5. ValidEfficiencyScore:  Add 1 point if the runtime of the generated SQL query is similar or better than the the groundtruth qery as measured by the Valid Efficiency Score (VES).\n",
    "                \n",
    "                Evaluation Steps:\n",
    "                1. Read provided context, question and answer carefully.\n",
    "                2. Go through each evaluation criterion one by one and assess whether the answer meets the criteria.\n",
    "                3. Compose your reasoning for each critera, explaining why you did or did not award a point. You can only award full points. \n",
    "                4. Calculate the total score by summing the points awarded.\n",
    "                5. Think through the evaluation criteria inside <thinking></thinking> tags. \n",
    "                Then, output the total score inside <score></score> tags.\n",
    "                Review your formatted response. It needs to be valid XML.\n",
    "    \n",
    "                Original question:\n",
    "                <question>\n",
    "                {question}\n",
    "                </question>\n",
    "\n",
    "                SQL schema:\n",
    "                <sql_schema>\n",
    "                {sql_schema}\n",
    "                </sql_schema>\n",
    "\n",
    "                Generated SQL query based on these instructions:\n",
    "                <sql_query>\n",
    "                {sql_query}\n",
    "                </sql_query>\n",
    "\n",
    "                SQL result based on the generated SQL query:\n",
    "                <sql_query_run_result>\n",
    "                {sql_query_run_result}\n",
    "                </sql_query_run_result>\n",
    "\n",
    "                Any SQL errors that might have occured based on the generated SQL query:\n",
    "                <sql_query_run_error>\n",
    "                {sql_query_run_error}\n",
    "                </sql_query_run_error>\n",
    "\n",
    "                Groundtruth SQL query for comparison with the generated SQL query:\n",
    "                <groundtruth_sql_query>\n",
    "                {groundtruth_sql_query}\n",
    "                </groundtruth_sql_query>\n",
    "                \n",
    "                Execution Accuracy, which compares the generated SQL query to the labeled SQL query to determine if its a match or not: \n",
    "                <ex_score>\n",
    "                {ex_score}\n",
    "                </ex_score>\n",
    "                \n",
    "                Exact Set Match Accuracy (EM), which evaluates if the returned result set actually answer the question, regardless of how the query was written: \n",
    "                <em_score>\n",
    "                {em_score}\n",
    "                </em_score>\n",
    "\n",
    "                Valid Efficiency Score (VES), which compares the runtime of the SQL provided as groundtruth to the generated SQL query:\n",
    "                <ves_score>\n",
    "                {ves_score}\n",
    "                </ves_score>                \n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Custom Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get graded data\n",
    "file_path = 'question_query_small_llm_grades.jsonl'\n",
    "df1_graded = pd.read_json(file_path, lines=True)\n",
    "\n",
    "\n",
    "df1_graded[\"routing_label\"] = df1_graded[\"score\"].apply(\n",
    "    lambda x: 1 if (x >=4) else 0\n",
    ")\n",
    "\n",
    "visualize_distribution(df1_graded, key=\"routing_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance the dataset for our classification task\n",
    "balanced_train_df = balance_dataset(df1_graded, key=\"routing_label\")\n",
    "visualize_distribution(balanced_train_df, key=\"routing_label\")\n",
    "print(f\"Train size: {len(balanced_train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prompt', 'completion'], dtype='object')\n",
      "Index(['prompt', 'completion'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# sample training data and reformat to format for finetuning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n_total_samples = 90\n",
    "train_ratio = 0.75  # 75% for training, 25% for validation\n",
    "\n",
    "# Calculate the number of samples for each set\n",
    "n_train = int(n_total_samples * train_ratio)\n",
    "n_val = n_total_samples - n_train\n",
    "\n",
    "# Sample the data\n",
    "sampled_df = balanced_train_df.sample(n=n_total_samples, random_state=42)\n",
    "\n",
    "# Split the sampled data into training and validation sets\n",
    "train_df, val_df = train_test_split(sampled_df, train_size=n_train, random_state=42)\n",
    "\n",
    "# Define output file names\n",
    "output_file = \"sampled_train_data.jsonl\"\n",
    "val_output_file = \"sampled_val_data.jsonl\"\n",
    "\n",
    "\n",
    "# reformat to format for finetuning\n",
    "training_data = []\n",
    "for index, row in train_df.iterrows():\n",
    "    prompt = str(build_prediction_prompt(user_question=row['Question'], sql_database_schema=row['Context']))\n",
    "    completion = str(row['routing_label'])\n",
    "    training_data.append({'prompt': prompt, 'completion': completion})\n",
    "training_df = pd.DataFrame(training_data)\n",
    "\n",
    "# Explicitly set the data types to string\n",
    "training_df['prompt'] = training_df['prompt'].astype(str)\n",
    "training_df['completion'] = training_df['completion'].astype('int64')\n",
    "\n",
    "print(training_df.columns)\n",
    "training_df.head(1)\n",
    "training_df.to_json(output_file, orient=\"records\", lines=True)\n",
    "\n",
    "\n",
    "\n",
    "val_data = []\n",
    "for index, row in val_df.iterrows():\n",
    "    prompt = str(build_prediction_prompt(user_question=row['Question'], sql_database_schema=row['Context']))\n",
    "    completion = str(row['routing_label'])\n",
    "    val_data.append({'prompt': prompt, 'completion': completion})\n",
    "validation_df = pd.DataFrame(val_data)\n",
    "\n",
    "# Explicitly set the data types to string\n",
    "validation_df['prompt'] = validation_df['prompt'].astype(str)\n",
    "validation_df['completion'] = validation_df['completion'].astype('int64')\n",
    "\n",
    "print(validation_df.columns)\n",
    "validation_df.head(1)\n",
    "validation_df.to_json(val_output_file, orient=\"records\", lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TBD: Finetune a small local LLM instruct model (DistilBERT) as binary classifier model\n",
    "# # %pip install transformers torch scikit-learn transformers[torch] accelerate -U\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# model_name = \"distilbert-base-uncased\"\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "#         self.prompts = dataframe['prompt'].tolist()\n",
    "#         self.completions = dataframe['completion'].tolist()\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         prompt = self.prompts[idx]\n",
    "#         completion = self.completions[idx]\n",
    "        \n",
    "#         # Tokenize the prompt and completion\n",
    "#         encoding = self.tokenizer(\n",
    "#             prompt,\n",
    "#             completion,\n",
    "#             truncation=True,\n",
    "#             padding='max_length',\n",
    "#             max_length=self.max_length,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "        \n",
    "#         # Remove the batch dimension\n",
    "#         item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        \n",
    "#         # item['labels'] = torch.tensor(self.labels[idx])\n",
    "        \n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.prompts)\n",
    "\n",
    "# # Create datasets with a DataFrame called training_df with 'prompt' and 'completion' columns\n",
    "# train_dataset = CustomDataset(training_df, tokenizer)\n",
    "# val_dataset = CustomDataset(validation_df, tokenizer)\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     warmup_steps=500,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=10,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "# )\n",
    "\n",
    "# # Define metrics function\n",
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     return {\n",
    "#         'accuracy': acc,\n",
    "#         'f1': f1,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall\n",
    "#     }\n",
    "\n",
    "# # Create Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Evaluate the model\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(eval_results)\n",
    "\n",
    "# # Save the model\n",
    "# model.save_pretrained(\"./finetuned_model\")\n",
    "# tokenizer.save_pretrained(\"./finetuned_model\")\n",
    "\n",
    "# # Inference example\n",
    "# def predict(text):\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "#     outputs = model(**inputs)\n",
    "#     probs = outputs.logits.softmax(dim=-1)\n",
    "#     return probs.argmax().item()\n",
    "\n",
    "# # Test the model\n",
    "# test_text = \"List all suppliers with their contact information.\"\n",
    "# prediction = predict(test_text)\n",
    "# print(f\"Prediction for '{test_text}': {'Small_LLM' if prediction == 1 else 'Large_LLM'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINE-TUNING JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEDROCK_FINE_TUNING = False\n",
    "if BEDROCK_FINE_TUNING == True: # required Bedrock Provisioned Throughput to deploy\n",
    "    # upload to S3\n",
    "    bucket_name = 'felixh-demo'\n",
    "    prefix = 'finetuning'\n",
    "    filename = output_file\n",
    "    s3_uri = dataframe_to_s3_jsonl(training_df, bucket_name, prefix, filename)\n",
    "    print(f's3_uri: {s3_uri}')\n",
    "\n",
    "    # Set parameters\n",
    "    customizationType = \"FINE_TUNING\"\n",
    "    baseModelIdentifier = \"arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1\"\n",
    "    roleArn = \"arn:aws:iam::026459568683:role/admin\"\n",
    "    jobName = \"Text-to-SQL-Routing-Classifier-Job-V2\"\n",
    "    customModelName = \"LLM-Routing-Classifier\"\n",
    "    hyperParameters = {\n",
    "            \"epochCount\": \"1\", # The maximum number of iterations through the entire training dataset\n",
    "            \"batchSize\": \"1\", # The number of samples processed before updating model parameters\n",
    "            \"learningRate\": \".0005\", # Multiplier that influences the learning rate at which model parameters are updated after each batch\n",
    "            \"learningRateWarmupSteps\": \"0\"\n",
    "        }\n",
    "    trainingDataConfig = {\"s3Uri\": s3_uri}\n",
    "    outputDataConfig = {\"s3Uri\": f\"s3://{bucket_name}/{prefix}/output\"}\n",
    "\n",
    "    # Create job\n",
    "    response_ft = bedrock_client.create_model_customization_job(\n",
    "        jobName=jobName, \n",
    "        customModelName=customModelName,\n",
    "        roleArn=roleArn,\n",
    "        baseModelIdentifier=baseModelIdentifier,\n",
    "        hyperParameters=hyperParameters,\n",
    "        trainingDataConfig=trainingDataConfig,\n",
    "        outputDataConfig=outputDataConfig\n",
    "    )\n",
    "\n",
    "    jobArn = response_ft.get('jobArn')\n",
    "    print(f'jobArn: {jobArn}')\n",
    "\n",
    "    response = bedrock_client.get_model_customization_job(jobIdentifier=jobArn)\n",
    "    status = response.get('status')\n",
    "    if status == 'Completed':\n",
    "        outputModelArn = response.get(\"outputModelArn\")\n",
    "        print(f'outputModelArn: {outputModelArn}')\n",
    "        customModelName = \"LLM-Routing-Classifier\"\n",
    "        response_pt = bedrock_client.create_provisioned_model_throughput(\n",
    "            modelId= outputModelArn,\n",
    "            provisionedModelName= customModelName,\n",
    "            modelUnits=1\n",
    "        )\n",
    "\n",
    "        provisionedModelArn = response_pt.get('provisionedModelArn')\n",
    "        print(f'provisionedModelArn: {provisionedModelArn}')\n",
    "    else:\n",
    "        print(f'finetuning job status: {status}')\n",
    "        print(f'finetuning job response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/aws-bedrock-router-eval/4b_Router-Eval.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/4b_Router-Eval.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m bucket_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfelixh-demo\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/4b_Router-Eval.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m prefix \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfinetuning\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/4b_Router-Eval.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m filename \u001b[39m=\u001b[39m output_file\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/4b_Router-Eval.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m train_data_location \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ms3://\u001b[39m\u001b[39m{\u001b[39;00mbucket_name\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mprefix\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/4b_Router-Eval.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m S3Uploader\u001b[39m.\u001b[39mupload(output_file, train_data_location)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_file' is not defined"
     ]
    }
   ],
   "source": [
    "if BEDROCK_FINE_TUNING == False: # use SageMaker endpoint\n",
    "    from sagemaker import hyperparameters\n",
    "    from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "    # reformat to format for finetuning SageMaker\n",
    "    from sagemaker.s3 import S3Uploader\n",
    "    import sagemaker\n",
    "    import random\n",
    "    import json\n",
    "    template = {\n",
    "        \"prompt\": \"{prompt}\\n\\n\",\n",
    "        \"completion\": \" {completion}\",\n",
    "    }\n",
    "    with open(\"template.json\", \"w\") as f:\n",
    "        json.dump(template, f)\n",
    "\n",
    "    bucket_name = 'felixh-demo'\n",
    "    prefix = 'finetuning'\n",
    "    filename = output_file\n",
    "\n",
    "    train_data_location = f\"s3://{bucket_name}/{prefix}\"\n",
    "    S3Uploader.upload(output_file, train_data_location)\n",
    "    S3Uploader.upload(\"template.json\", train_data_location)\n",
    "    print(f\"Training data: {train_data_location}\")\n",
    "\n",
    "\n",
    "    model_id = 'huggingface-llm-mistral-7b' #check https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html\n",
    "    model_version = '*'\n",
    "    \n",
    "    finetuning_hyperparameters = hyperparameters.retrieve_default(\n",
    "        model_id=model_id, model_version=model_version\n",
    "    )\n",
    "    print(finetuning_hyperparameters)\n",
    "\n",
    "    finetuning_hyperparameters[\"epoch\"] = \"1\"\n",
    "    finetuning_hyperparameters[\"per_device_train_batch_size\"] = \"2\"\n",
    "    finetuning_hyperparameters[\"gradient_accumulation_steps\"] = \"2\"\n",
    "    finetuning_hyperparameters[\"instruction_tuned\"] = \"True\"\n",
    "    finetuning_hyperparameters[\"max_input_length\"] = \"32000\"\n",
    "\n",
    "    #validate parameters\n",
    "    hyperparameters.validate(\n",
    "        model_id=model_id, model_version=model_version, hyperparameters=finetuning_hyperparameters\n",
    "    )\n",
    "    \n",
    "    # start training\n",
    "    instruction_tuned_estimator = JumpStartEstimator(\n",
    "        model_id=model_id,\n",
    "        hyperparameters=finetuning_hyperparameters,\n",
    "        instance_type=\"ml.g5.12xlarge\",\n",
    "    )\n",
    "    instruction_tuned_estimator.fit({\"train\": train_data_location}, logs=True)\n",
    "\n",
    "    # get metrics\n",
    "    from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "    training_job_name = instruction_tuned_estimator.latest_training_job.job_name\n",
    "\n",
    "    df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "    df.head(10)\n",
    "\n",
    "    # deploy fine-tuned model\n",
    "    instruction_tuned_predictor = instruction_tuned_estimator.deploy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BEDROCK_FINE_TUNING' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/aws-bedrock-router-eval/4b_Router-Eval.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/4b_Router-Eval.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m BEDROCK_FINE_TUNING \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m: \u001b[39m# use SageMaker endpoint\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/4b_Router-Eval.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39msagemaker\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://hp8z681i3gpzvpo.studio.us-east-1.sagemaker.aws/home/sagemaker-user/aws-bedrock-router-eval/4b_Router-Eval.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39msagemaker\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpredictor\u001b[39;00m \u001b[39mimport\u001b[39;00m Predictor\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BEDROCK_FINE_TUNING' is not defined"
     ]
    }
   ],
   "source": [
    "if BEDROCK_FINE_TUNING == False: # use SageMaker endpoint\n",
    "    import sagemaker\n",
    "    from sagemaker.predictor import Predictor\n",
    "    import json\n",
    "\n",
    "    # Initialize the SageMaker session\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "\n",
    "    # Specify the endpoint name\n",
    "    endpoint_name = \"hf-llm-mistral-7b-2024-08-23-01-17-36-474\"\n",
    "\n",
    "    # Create the predictor\n",
    "    predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer()\n",
    "    )\n",
    "\n",
    "    prompt = build_sqlquerygen_prompt(user_question='What is the total number of customers?', sql_database_schema=schema)\n",
    "    # Wrap the prompt in the instruction format if needed\n",
    "    prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "    # prompt = f\"<s>[INST] What is the total number of customers? [/INST]\"\n",
    "    # print(f'prompt: {prompt}')\n",
    "    # Create the payload\n",
    "    payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"top_k\": 5,\n",
    "        \"top_p\": 0.75,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    }\n",
    "\n",
    "    # Make the prediction\n",
    "    result = predictor.predict(payload)\n",
    "    print(f'result: {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this tutorial, we have successfully built and evaluated a finetuned-LLM router. \n",
    "We generated synthetic labeled data using the LLM-as-a-judge method to train the model, finetuned an LLM classifier using Amazon Bedrock's API, \n",
    "and conducted offline evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "https://github.com/lm-sys/RouteLLM\n",
    "\n",
    "https://medium.com/@learngrowthrive.fast/routellm-achieves-90-gpt-4-quality-at-80-lower-cost-6686e5f46e2a\n",
    "\n",
    "https://medium.com/ai-insights-cobet/beyond-basic-chatbots-how-semantic-router-is-changing-the-game-783dd959a32d\n",
    "\n",
    "https://medium.com/@bhawana.prs/semantic-routes-in-llms-to-make-chatbots-more-accurate-d99c17e30487\n",
    "\n",
    "\n",
    "popular benchmarks: MT Bench, MMLU, and GSM8K.\n",
    "\n",
    "* Semantic routing: Using a vector analysis to route the query to the closest “cluster”\n",
    "https://github.com/aurelio-labs/semantic-router\n",
    "\n",
    "* Prompt Chaining: Similar to what has been implemented inside Bedrock agents, and LangChain’s Custom function, these use an small LLM to analyze the question and route it to the next part of the chain. https://aws.amazon.com/blogs/machine-learning/enhance-conversational-ai-with-advanced-routing-techniques-with-amazon-bedrock/\n",
    "You can optimize this by having the “router” model answer directly simple questions instead of routing them to another model.\n",
    "\n",
    "* Intent Classification: Creating a custom model, similar to ROHF or Rerankers to classify the query and route it to the right LLM.  \n",
    "https://medium.com/aimonks/intent-classification-generative-ai-based-application-architecture-3-79d2927537b4\n",
    "\n",
    "https://www.anyscale.com/blog/building-an-llm-router-for-high-quality-and-cost-effective-responses\n",
    "\n",
    "https://github.com/aws-samples/amazon-bedrock-samples/blob/main/function-calling/function_calling_text2SQL_converse_bedrock_streamlit.py\n",
    "\n",
    "https://github.com/aws-samples/amazon-bedrock-samples/tree/main/rag-solutions/sql-query-generator\n",
    "\n",
    "### Next steps\n",
    "\n",
    "Explore other data sources:  https://bird-bench.github.io/ , latest spyder dataset, any SQL dataset from HF\n",
    "\n",
    "<!-- # https://huggingface.co/datasets/b-mc2/sql-create-context\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# # %sql sqlite:///routedb.db\n",
    "# # to load SQL dataset from starcoder\n",
    "## ds = load_dataset(\"bigcode/starcoderdata\", data_dir=\"sql\", split=\"train\", token=True)\n",
    "# ds = load_dataset(\"b-mc2/sql-create-context\", split=\"train\", token=True)\n",
    "# from datasets import load_dataset_builder\n",
    "# ds_builder = load_dataset_builder(\"b-mc2/sql-create-context\")\n",
    "# ds_builder.info.description\n",
    "# ds_builder.info.features -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRATCHPAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral.mixtral-8x7b-instruct-v0:1\n",
    "\n",
    "# Use the native inference API to send a text message to Anthropic Claude.\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region of your choice.\n",
    "bedrock_runtime_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# Set the model ID, e.g., Claude 3 Haiku.\n",
    "model_id = \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "\n",
    "# Define the prompt for the model.\n",
    "prompt = build_sqlquerygen_prompt(user_question= 'What is the total number of customers?', sql_database_schema= schema)\n",
    "\n",
    "# Setup the system prompts and messages to send to the model.\n",
    "system_prompts = [] # [{\"text\": \"You are a helpful AI Assistant.\"}]\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": prompt}]\n",
    "}\n",
    "\n",
    "messages = []\n",
    "messages.append(message)\n",
    "\n",
    "try:\n",
    "    # Base inference parameters to use.\n",
    "    inference_config = {\"temperature\": 0.0}\n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {\"top_k\": 5}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_runtime_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    # Log token usage.\n",
    "    text = response['output'].get('message').get('content')[0].get('text')\n",
    "    print(f'text: {text}')\n",
    "    token_usage = response['usage']\n",
    "    print(f'token_usage: {token_usage}')\n",
    "    latency = response['metrics'].get('latencyMs')\n",
    "    print(f'latency: {latency}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error with calling Bedrock: \"+str(e))\n",
    "    attempt+=1\n",
    "    if attempt>3:\n",
    "        print(\"Max attempts reached!\")\n",
    "        result_text = str(e)\n",
    "        \n",
    "    else:#retry in 10 seconds\n",
    "        print(\"retry\")\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize SageMaker predictor with endpoint name e.g. hf-llm-mistral-7b-2024-08-23-01-17-36-474\n",
    "import sagemaker\n",
    "from sagemaker.predictor import Predictor\n",
    "import json\n",
    "\n",
    "# Initialize the SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Specify the endpoint name\n",
    "endpoint_name = \"hf-llm-mistral-7b-2024-08-23-01-17-36-474\"\n",
    "\n",
    "# Create the predictor\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer()\n",
    ")\n",
    "prompt = build_sqlquerygen_prompt(user_question= 'What is the total number of customers?', sql_database_schema= schema)\n",
    "# print(f'prompt: {prompt}')\n",
    "# prompt = f\"[INST] {prompt} [/INST]\"\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 500,\n",
    "        \"top_k\": 5,\n",
    "        \"top_p\": 0.75,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "result = predictor.predict(payload)\n",
    "print(f'result: {result}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock-router-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
