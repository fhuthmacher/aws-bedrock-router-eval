{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497a56a1-197f-4da6-b4ec-32648f97ddeb",
   "metadata": {},
   "source": [
    "# Bedrock Model Routing - LLM routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a99e95-88b0-4f4e-84d8-a3d301a967e5",
   "metadata": {},
   "source": [
    "## Intro and Goal\n",
    "This Jupyter Notebook is designed to test an LLM (Large Language Model) routing system. The goal is to take a prompt, embed it using a vector embedding in Bedrock, and then measure the distance with two specific vectors that represent the domain for two specific LLMs. Based on the distance, the prompt will be routed to the appropriate LLM.\n",
    "\n",
    "The notebook is structured as follows:\n",
    "1. Create the samples for the 2 domains that we'll route to (e.g., code generation and summarization).\n",
    "2. Generate the embeddings for the 2 domain prompts.\n",
    "3. Create a 3rd prompt, generate its embedding, and measure the distance to select which domain it relates to.\n",
    "4. Construct the router that will take the prompt and automatically generate the answer from the LLM the prompt is routed to based on the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d6bd69d-d755-4694-b042-ea18f37cc660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "# loading environment variables that are stored in local file dev.env\n",
    "local_env_filename = 'bedrock-router-eval.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "REGION = os.environ['REGION']\n",
    "\n",
    "client = boto3.client(service_name='bedrock-runtime', region_name=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8d79909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bedrock_client = boto3.client(service_name='bedrock', region_name=REGION)\n",
    "# bedrock_client.list_foundation_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c7cfe67-dc9e-4d6c-9ba6-2779c127db12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define your LLM router\n",
    "router_prompt = \"Give this question a difficulty rating, from 1 to 3, simply provide the number without anything else in your answer:\\n\"\n",
    "router_model = \"anthropic.claude-3-haiku-20240307-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "719dbdc2-51fe-465a-9415-670c6548b35f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Evaluate the prompt\n",
    "prompt = \"Implement a function that sorts a list of integers in descending order using the insertion sort algorithm.\"\n",
    "client = boto3.client(service_name='bedrock-runtime', region_name=REGION)\n",
    "from botocore.exceptions import ClientError\n",
    "# Format the request payload using the model's native structure.\n",
    "def eval(prompt):\n",
    "    native_request = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.5,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": router_prompt + prompt}],\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    # Convert the native request to JSON.\n",
    "    request = json.dumps(native_request)\n",
    "    \n",
    "    try:\n",
    "        # Invoke the model with the request.\n",
    "        response = client.invoke_model(modelId=router_model, body=request)\n",
    "    \n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Decode the response body.\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "    \n",
    "    # Extract and print the response text.\n",
    "    response_text = model_response[\"content\"][0][\"text\"]\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c52de057-089a-4857-88a5-2a0d0d1457e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implement a function that sorts a list of integers in descending order using the insertion sort algorithm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(prompt)\n",
    "eval(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b764ef9c-3be2-45e6-92b0-8430bb2b886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define your model selection\n",
    "model_1 = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "native_request_1 = {\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "model_2 = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "native_request_2 = {\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "model_3 = \"mistral.mixtral-8x7b-instruct-v0:1\" #\"meta.llama3-1-70b-instruct-v1:0\" mistral.mixtral-8x7b-instruct-v0:1\n",
    "native_request_3 = {\n",
    "    \"prompt\": '<s>[INST] ' + prompt + '[/INST]',\n",
    "    # \"max_gen_len\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "# native_request_3 = {\n",
    "#     \"max_tokens\": 512,\n",
    "#     \"temperature\": 0.5,\n",
    "#     \"messages\": [\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
    "#         }\n",
    "#     ],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc40eaf2-09b2-4af5-92eb-8a8c0b086daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Construct the router\n",
    "def route_prompt(prompt):\n",
    "    if eval(prompt)==1:\n",
    "        request = json.dumps(native_request_1)\n",
    "        try:\n",
    "            # Invoke the model with the request.\n",
    "            response = client.invoke_model(modelId=model_1, body=request)\n",
    "        \n",
    "        except (ClientError, Exception) as e:\n",
    "            print(f\"ERROR: Can't invoke '{model_1}'. Reason: {e}\")\n",
    "            exit(1)\n",
    "        \n",
    "        # Decode the response body.\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "        \n",
    "        # Extract and print the response text.\n",
    "        response_text = model_response[\"content\"][0][\"text\"]\n",
    "        return response_text\n",
    "    if eval(prompt)==2:\n",
    "        request = json.dumps(native_request_2)\n",
    "        try:\n",
    "            # Invoke the model with the request.\n",
    "            response = client.invoke_model(modelId=model_2, body=request)\n",
    "        \n",
    "        except (ClientError, Exception) as e:\n",
    "            print(f\"ERROR: Can't invoke '{model_2}'. Reason: {e}\")\n",
    "            exit(1)\n",
    "        \n",
    "        # Decode the response body.\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "        \n",
    "        # Extract and print the response text.\n",
    "        response_text = model_response[\"content\"][0][\"text\"]\n",
    "        return response_text\n",
    "    else:\n",
    "        request = json.dumps(native_request_3)\n",
    "        try:\n",
    "            # Invoke the model with the request.\n",
    "            response = client.invoke_model(modelId=model_3, body=request)\n",
    "        \n",
    "        except (ClientError, Exception) as e:\n",
    "            print(f\"ERROR: Can't invoke '{model_3}'. Reason: {e}\")\n",
    "            exit(1)\n",
    "        \n",
    "        # Decode the response body.\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "        \n",
    "        # Extract and print the response text.\n",
    "        # print(model_response)\n",
    "        response_text = model_response.get('outputs')[0].get('text')\n",
    "        return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "430e64e6-da45-4a2f-a901-78b2a72d3858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def insertion_sort_descending(arr):\\n    for i in range(1, len(arr)):\\n        key = arr[i]\\n        j = i - 1\\n        while j >= 0 and key > arr[j]:\\n            arr[j + 1] = arr[j]\\n            j -= 1\\n        arr[j + 1] = key\\n\\n# Example usage:\\narr = [12, 11, 13, 5, 6]\\ninsertion_sort_descending(arr)\\nprint(\"Sorted array in descending order:\")\\nprint(arr)'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_prompt(\"Implement a function that sorts a list of integers in descending order using the insertion sort algorithm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7911331-a486-4974-b8a1-b906051f7c50",
   "metadata": {},
   "source": [
    "# other\n",
    "Dataset of questions on specific difficulty and topics\n",
    "zero shot vs few shot vs fine-tuned\n",
    "create a synthetic dataset (text to sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa86480-1265-4264-af0c-2767ebec9b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
